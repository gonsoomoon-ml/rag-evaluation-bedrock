{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Generated Synthetic Datasets\n",
    "\n",
    "In this section, we load synthetic datasets that have been generated for testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>question_type</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do temperature, Top K, and Top P parameter...</td>\n",
       "      <td>Temperature, Top K, and Top P are parameters t...</td>\n",
       "      <td>complex</td>\n",
       "      <td>• If you set a high temperature, the probabili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long will Amazon Bedrock support base mode...</td>\n",
       "      <td>Amazon Bedrock will support base models for a ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>• EOL: This version is no longer available for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the system handle a scenario where a ...</td>\n",
       "      <td>The system doesn't explicitly show a function ...</td>\n",
       "      <td>complex</td>\n",
       "      <td>'payment_date': ['2021-10-05', '2021-10-06', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of an S3 retrieval node in...</td>\n",
       "      <td>An S3 retrieval node lets you retrieve data fr...</td>\n",
       "      <td>simple</td>\n",
       "      <td>An S3 retrieval node lets you retrieve data fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can a developer create a new prompt versio...</td>\n",
       "      <td>To create a new prompt version, retrieve its i...</td>\n",
       "      <td>complex</td>\n",
       "      <td>make a CreatePromptVersion Agents for Amazon B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How do temperature, Top K, and Top P parameter...   \n",
       "1  How long will Amazon Bedrock support base mode...   \n",
       "2  How does the system handle a scenario where a ...   \n",
       "3  What is the purpose of an S3 retrieval node in...   \n",
       "4  How can a developer create a new prompt versio...   \n",
       "\n",
       "                                        ground_truth question_type  \\\n",
       "0  Temperature, Top K, and Top P are parameters t...       complex   \n",
       "1  Amazon Bedrock will support base models for a ...        simple   \n",
       "2  The system doesn't explicitly show a function ...       complex   \n",
       "3  An S3 retrieval node lets you retrieve data fr...        simple   \n",
       "4  To create a new prompt version, retrieve its i...       complex   \n",
       "\n",
       "                                            contexts  \n",
       "0  • If you set a high temperature, the probabili...  \n",
       "1  • EOL: This version is no longer available for...  \n",
       "2  'payment_date': ['2021-10-05', '2021-10-06', '...  \n",
       "3  An S3 retrieval node lets you retrieve data fr...  \n",
       "4  make a CreatePromptVersion Agents for Amazon B...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"data/sample_qa_dataset.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884294bc92444834bcd0cb13776a3499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'ground_truth', 'question_type', 'contexts'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+', '', s)\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    return s\n",
    "\n",
    "def convert_to_list(example):\n",
    "    cleaned_context = clean_string(example[\"contexts\"])\n",
    "    try:\n",
    "        contexts = ast.literal_eval(cleaned_context)\n",
    "    except:\n",
    "        contexts = cleaned_context\n",
    "    return {\"contexts\": contexts}\n",
    "\n",
    "\n",
    "subset_length = 10  # Change \n",
    "test_dataset = Dataset.from_pandas(df.head(subset_length))\n",
    "\n",
    "test_dataset = test_dataset.map(convert_to_list)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline Setting\n",
    "\n",
    "The test dataset is used to simulate real-world queries in a RAG pipeline, which combines document retrieval with response generation. \n",
    "\n",
    "Here, we are using the default settings for the KnowledgeBase in Amazon Bedrock as part of the RAG configuration.\n",
    "\n",
    "_1. To utilize the code below for testing, the KnowledgeBase must be pre-configured in the Amazon Bedrock console_\n",
    "\n",
    "_2. If you have a specific RAG pipeline you want to evaluate, please modify the cells below accordingly_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Retrieval\n",
    "\n",
    "In this section, we will test the system’s ability to retrieve relevant context from the KnowledgeBase using the provided queries. \n",
    "\n",
    "This is a critical step in the RAG pipeline, as the accuracy of the context retrieved has a direct impact on the quality of the generated responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\n",
      "search_result[0]: {'index': 1, 'content': 'If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that lies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers \"horses\" and \"zebras\" as they are in the top 90% of probability distribution.     Randomness and diversity 325Amazon Bedrock User Guide     Length     Foundation models typically support parameters that limit the length of the response. Examples of these parameters are provided below.     ? Response length ? An exact value to specify the minimum or maximum number of tokens to return in the generated response.     ? Penalties ? Specify the degree to which to penalize outputs in a response. Examples include the following.     ? The length of the response.     ? Repeated tokens in a response.     ? Frequency of tokens in a response.     ? Types of tokens in a response.     ? Stop sequences ? Specify sequences of characters that stop the model from generating further tokens. If the model generates a stop sequence that you specify, it will stop generating after that sequence.     Supported Regions and models for running model inference     Model inference using foundation models is supported in all Regions and with all models supported by Amazon Bedrock. To see the Regions and models supported by Amazon Bedrock, refer to Supported foundation models in Amazon Bedrock.     You can also run model inference with Amazon Bedrock resources other than foundation models.', 'source': {'s3Location': {'uri': 's3://knowledgebase-bedrock-agent-gsmoon/bedrock-developer-guide/bedrock-ug.pdf'}, 'type': 'S3'}, 'score': 0.54313713}\n",
      "context: If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that lies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers \"horses\" and \"zebras\" as they are in the top 90% of probability distribution.     Randomness and diversity 325Amazon Bedrock User Guide     Length     Foundation models typically support parameters that limit the length of the response. Examples of these parameters are provided below.     ? Response length ? An exact value to specify the minimum or maximum number of tokens to return in the generated response.     ? Penalties ? Specify the degree to which to penalize outputs in a response. Examples include the following.     ? The length of the response.     ? Repeated tokens in a response.     ? Frequency of tokens in a response.     ? Types of tokens in a response.     ? Stop sequences ? Specify sequences of characters that stop the model from generating further tokens. If the model generates a stop sequence that you specify, it will stop generating after that sequence.     Supported Regions and models for running model inference     Model inference using foundation models is supported in all Regions and with all models supported by Amazon Bedrock. To see the Regions and models supported by Amazon Bedrock, refer to Supported foundation models in Amazon Bedrock.     You can also run model inference with Amazon Bedrock resources other than foundation models.\n",
      "--\n",
      "For example, if you choose a value of 50 for Top K, the model selects from 50 of the most probable tokens that could be next in the sequence.     ? Top P ? The percentage of most-likely candidates that the model considers for the next token.     ? Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.     ? Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs.     In technical terms, the model computes the cumulative probability distribution for the set of responses and considers only the top P% of the distribution.     For example, if you choose a value of 0.8 for Top P, the model selects from the top 80% of the probability distribution of tokens that could be next in the sequence.     Randomness and diversity 324Amazon Bedrock User Guide     The following table summarizes the effects of these parameters.     Parameter Effect of lower value Effect of higher value     Temperature Increase likelihood of higher- probability tokens     Decrease likelihood of lower- probability tokens     Increase likelihood of lower- probability tokens     Decrease likelihood of higher- probability tokens     Top K Remove lower-probability tokens     Allow lower-probability tokens     Top P Remove lower-probability tokens     Allow lower-probability tokens     As an example to understand these parameters, consider the example prompt I hear the hoof beats of \".\n",
      "--\n",
      "The model or inference profile that you choose also specifies a level of throughput, which defines the number and rate of input and output tokens that you can process. For more information about the foundation models that are available in Amazon Bedrock, see Amazon Bedrock foundation model information. For more information about inference profiles, see Set up a model invocation resource using inference profiles. For more information about increasing throughput, see Increase throughput with cross-region inference and Increase model invocation capacity with Provisioned Throughput in Amazon Bedrock.     ? Inference parameters ? A set of values that can be adjusted to limit or influence the model response. For information about inference parameters, see Influence response generation with inference parameters and Inference request parameters and response fields for foundation models.     Amazon Bedrock offers a suite of foundation models that you can use to generate outputs of the following modalities. To see modality support by foundation model, refer to Supported foundation models in Amazon Bedrock.     Output modality Description Example use cases     Text Provide text input and generate various types of text     Chat, question-and- answering, brainstorming, summarization, code     321Amazon Bedrock User Guide     Output modality Description Example use cases     generation, table creation, data formatting, rewriting     Image Provide text or input images and generate or modify images     Image generation, image editing, image variation     Embeddings Provide text, images, or both text and images and generate a vector of numeric values that represent the input.\n"
     ]
    }
   ],
   "source": [
    "# RAG implementation sample 1 (Replace with RAG pipeline for evaluation)\n",
    "from libs.bedrock_kb_util import context_retrieval_from_kb\n",
    "\n",
    "amazon_kb_id = 'HNCKVA5XST'\n",
    "\n",
    "question = test_dataset[0]['question']\n",
    "print(\"question:\\n\", question)\n",
    "search_result = context_retrieval_from_kb(question, 3, 'us-west-2', amazon_kb_id, 'SEMANTIC')\n",
    "print(\"search_result[0]:\", search_result[0])\n",
    "\n",
    "contexts = \"\\n--\\n\".join([result['content'] for result in search_result])\n",
    "print(\"context:\", contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = 'us-west-2'\n",
    "\n",
    "retry_config = Config(\n",
    "    region_name=region,\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "boto3_client = boto3.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation\n",
    "\n",
    "Here, we are generating answers based on the retrieved context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, I can explain how temperature, Top K, and Top P parameters interact in Amazon Bedrock\\'s foundation models and how adjusting them might affect the output when generating text about different types of equines:\\n\\n1. Temperature: \\n- Lower values increase the likelihood of higher-probability tokens and decrease the likelihood of lower-probability tokens.\\n- Higher values increase the likelihood of lower-probability tokens and decrease the likelihood of higher-probability tokens.\\n\\n2. Top K:\\n- Lower values remove lower-probability tokens from consideration.\\n- Higher values allow more lower-probability tokens to be considered.\\n\\n3. Top P:\\n- Lower values remove lower-probability tokens by considering only the top percentage of the probability distribution.\\n- Higher values allow more lower-probability tokens by considering a larger percentage of the probability distribution.\\n\\nIn the context of generating text about different types of equines:\\n\\n- If you set Top K to 50, the model would select from the 50 most probable tokens that could come next in the sequence. This might include various equine-related words.\\n\\n- If you set Top P to 0.7, the model would only consider \"horses\" as it\\'s the only candidate in the top 70% of the probability distribution.\\n\\n- If you increase Top P to 0.9, the model would consider both \"horses\" and \"zebras\" as they fall within the top 90% of the probability distribution.\\n\\nAdjusting these parameters in combination can affect the diversity and specificity of the generated text about equines. For example, lowering temperature while keeping a high Top K or Top P might result in more common equine terms, while increasing temperature with higher Top K or Top P values could lead to more diverse or unusual equine-related outputs.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer(question, contexts):\n",
    "    system_prompt = \"\"\"You are an AI assistant that uses retrieved context to answer questions accurately. \n",
    "    Follow these guidelines:\n",
    "    1. Use the provided context to inform your answers.\n",
    "    2. If the context doesn't contain relevant information, say \"I don't have enough information to answer that.\"\n",
    "    3. Be concise and to the point in your responses.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {contexts}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Please answer the question based on the given context.\"\"\"\n",
    "\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{'role': 'user', 'content': [{'text': user_prompt}]}],\n",
    "        system=[{'text': system_prompt}]\n",
    "    )\n",
    "\n",
    "    answer = response['output']['message']['content'][0]['text']\n",
    "    return answer\n",
    "\n",
    "generate_answer(question, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Process for All Sample Questions\n",
    "\n",
    "This section runs the entire pipeline, from context retrieval to answer generation, across a set of sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1893935789e4fc997566c6084d1c60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "kb_region = 'us-west-2'\n",
    "kb_id = amazon_kb_id\n",
    "top_k = 3\n",
    "\n",
    "def process_item(item):\n",
    "    sleep(5)  # Prevent throttling\n",
    "    question = item['question']\n",
    "    search_result = context_retrieval_from_kb(question, top_k, kb_region, kb_id, 'SEMANTIC')\n",
    "\n",
    "    contexts = [result['content'] for result in search_result]\n",
    "    answer = generate_answer(question, \"\\n--\\n\".join(contexts))\n",
    "\n",
    "    return {\n",
    "        'question': item['question'],\n",
    "        'ground_truth': item['ground_truth'],\n",
    "        'original_contexts': item['contexts'],\n",
    "        'retrieved_contexts': contexts,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "updated_dataset = test_dataset.map(process_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Intermediate Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/sample_processed_qa_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_file = \"data/sample_processed_qa_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in updated_dataset:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Format Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\n",
      "\n",
      "\n",
      "Answer: Based on the provided context, here's how temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models and how adjusting them might affect output about different types of equines:\n",
      "\n",
      "1. Temperature: \n",
      "   - Lower values increase the likelihood of higher-probability tokens and decrease the likelihood of lower-probability tokens.\n",
      "   - Higher values increase the likelihood of lower-probability tokens and decrease the likelihood of higher-probability tokens.\n",
      "   - For equine-related text, lower temperature might favor more common horse terms, while higher temperature could introduce more diverse or unusual equine references.\n",
      "\n",
      "2. Top K:\n",
      "   - Lower values remove lower-probability tokens from consideration.\n",
      "   - Higher values allow more lower-probability tokens to be considered.\n",
      "   - For equine text, a lower Top K might limit responses to the most common horse types, while a higher value could include rarer equine species.\n",
      "\n",
      "3. Top P:\n",
      "   - Lower values remove lower-probability tokens from the distribution.\n",
      "   - Higher values allow more lower-probability tokens to be considered.\n",
      "   - In the equine example given, with Top P at 0.7, only \"horses\" would be considered. At 0.9, both \"horses\" and \"zebras\" would be in the pool of possible responses.\n",
      "\n",
      "These parameters interact to control the balance between focused, high-probability outputs and more diverse, potentially creative responses. Adjusting them in combination allows fine-tuning of the model's output to achieve the desired level of predictability versus creativity in generating text about different types of equines.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/sample_processed_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\\n\\n\")\n",
    "print(f\"Answer: {item['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Each Metric\n",
    "\n",
    "We now evaluate the system based on various metrics. \n",
    "\n",
    "For detailed implementations, refer to the `libs/custom_ragas.py` file. \n",
    "\n",
    "This script contains the specific evaluation criteria that we use to assess the performance of the RAG pipeline across different dimensions, such as accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cc30bb68bf4b7688e1e974dca71783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnswerRelevancy - Row 1: Score = 0.9479375190746863\n",
      "Faithfulness - Row 1: Score = 0.8\n",
      "ContextRecall - Row 1: Score = 0.0\n",
      "ContextPrecision - Row 1: Score = 0.99999999995\n",
      "AnswerRelevancy - Row 2: Score = 0.0\n",
      "Faithfulness - Row 2: Score = 0.0\n",
      "ContextRecall - Row 2: Score = 0.0\n",
      "ContextPrecision - Row 2: Score = 0.0\n",
      "AnswerRelevancy - Row 3: Score = 0.0\n",
      "Faithfulness - Row 3: Score = 0.8\n",
      "ContextRecall - Row 3: Score = 1.0\n",
      "ContextPrecision - Row 3: Score = 0.99999999995\n",
      "AnswerRelevancy - Row 4: Score = 0.9774043577012451\n",
      "Faithfulness - Row 4: Score = 0.6666666666666666\n",
      "ContextRecall - Row 4: Score = 1.0\n",
      "ContextPrecision - Row 4: Score = 0.8333333332916666\n",
      "AnswerRelevancy - Row 5: Score = 0.0\n",
      "Faithfulness - Row 5: Score = 0.8333333333333334\n",
      "ContextRecall - Row 5: Score = 0.0\n",
      "ContextPrecision - Row 5: Score = 0.0\n",
      "AnswerRelevancy - Row 6: Score = 0.9393254721491061\n",
      "Faithfulness - Row 6: Score = 1.0\n",
      "ContextRecall - Row 6: Score = 1.0\n",
      "ContextPrecision - Row 6: Score = 0.5833333333041666\n",
      "AnswerRelevancy - Row 7: Score = 0.9358363611152251\n",
      "Faithfulness - Row 7: Score = 1.0\n",
      "ContextRecall - Row 7: Score = 0.0\n",
      "ContextPrecision - Row 7: Score = 0.0\n",
      "AnswerRelevancy - Row 8: Score = 0.9303811737004329\n",
      "Faithfulness - Row 8: Score = 1.0\n",
      "ContextRecall - Row 8: Score = 1.0\n",
      "ContextPrecision - Row 8: Score = 0.3333333333\n",
      "AnswerRelevancy - Row 9: Score = 0.8931086750058114\n",
      "Faithfulness - Row 9: Score = 0.5\n",
      "ContextRecall - Row 9: Score = 0.0\n",
      "ContextPrecision - Row 9: Score = 0.0\n",
      "AnswerRelevancy - Row 10: Score = 0.87556912102331\n",
      "Faithfulness - Row 10: Score = 1.0\n",
      "ContextRecall - Row 10: Score = 1.0\n",
      "ContextPrecision - Row 10: Score = 0.49999999995\n",
      "Average Scores:\n",
      "{'AnswerRelevancy': 0.6499562679769817, 'Faithfulness': 0.76, 'ContextRecall': 0.5, 'ContextPrecision': 0.4249999999745834}\n",
      "\n",
      "Detailed Results:\n",
      "{'row': 1, 'AnswerRelevancy': 0.9479375190746863, 'Faithfulness': 0.8, 'ContextRecall': 0.0, 'ContextPrecision': 0.99999999995}\n",
      "{'row': 2, 'AnswerRelevancy': 0.0, 'Faithfulness': 0.0, 'ContextRecall': 0.0, 'ContextPrecision': 0.0}\n",
      "{'row': 3, 'AnswerRelevancy': 0.0, 'Faithfulness': 0.8, 'ContextRecall': 1.0, 'ContextPrecision': 0.99999999995}\n",
      "{'row': 4, 'AnswerRelevancy': 0.9774043577012451, 'Faithfulness': 0.6666666666666666, 'ContextRecall': 1.0, 'ContextPrecision': 0.8333333332916666}\n",
      "{'row': 5, 'AnswerRelevancy': 0.0, 'Faithfulness': 0.8333333333333334, 'ContextRecall': 0.0, 'ContextPrecision': 0.0}\n",
      "{'row': 6, 'AnswerRelevancy': 0.9393254721491061, 'Faithfulness': 1.0, 'ContextRecall': 1.0, 'ContextPrecision': 0.5833333333041666}\n",
      "{'row': 7, 'AnswerRelevancy': 0.9358363611152251, 'Faithfulness': 1.0, 'ContextRecall': 0.0, 'ContextPrecision': 0.0}\n",
      "{'row': 8, 'AnswerRelevancy': 0.9303811737004329, 'Faithfulness': 1.0, 'ContextRecall': 1.0, 'ContextPrecision': 0.3333333333}\n",
      "{'row': 9, 'AnswerRelevancy': 0.8931086750058114, 'Faithfulness': 0.5, 'ContextRecall': 0.0, 'ContextPrecision': 0.0}\n",
      "{'row': 10, 'AnswerRelevancy': 0.87556912102331, 'Faithfulness': 1.0, 'ContextRecall': 1.0, 'ContextPrecision': 0.49999999995}\n"
     ]
    }
   ],
   "source": [
    "from libs.custom_ragas import (\n",
    "    evaluate,\n",
    "    AnswerRelevancy, \n",
    "    Faithfulness, \n",
    "    ContextRecall,\n",
    "    ContextPrecision\n",
    ")\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "emb_id = \"amazon.titan-embed-text-v2:0\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "metrics = [AnswerRelevancy, Faithfulness, ContextRecall, ContextPrecision]\n",
    "\n",
    "def map_dataset(example):\n",
    "    return {\n",
    "        \"user_input\": example[\"question\"],\n",
    "        \"retrieved_contexts\": example[\"retrieved_contexts\"],\n",
    "        \"referenced_contexts\": example[\"original_contexts\"],\n",
    "        \"response\": example[\"answer\"],\n",
    "        \"reference\": example[\"ground_truth\"]\n",
    "    }\n",
    "\n",
    "dataset = updated_dataset.map(map_dataset)\n",
    "results = evaluate(dataset, metrics, llm_id, emb_id, region)\n",
    "\n",
    "print(\"Average Scores:\")\n",
    "print(results['average_scores'])\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "for row in results['detailed_results']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to data/sample_ragas_result.json\n",
      "{'average_scores': {'AnswerRelevancy': 0.6499562679769817, 'Faithfulness': 0.76, 'ContextRecall': 0.5, 'ContextPrecision': 0.4249999999745834}, 'detailed_results': [{'row': 1, 'AnswerRelevancy': 0.9479375190746863, 'Faithfulness': 0.8, 'ContextRecall': 0.0, 'ContextPrecision': 0.99999999995}, {'row': 2, 'AnswerRelevancy': 0.0, 'Faithfulness': 0.0, 'ContextRecall': 0.0, 'ContextPrecision': 0.0}, {'row': 3, 'AnswerRelevancy': 0.0, 'Faithfulness': 0.8, 'ContextRecall': 1.0, 'ContextPrecision': 0.99999999995}, {'row': 4, 'AnswerRelevancy': 0.9774043577012451, 'Faithfulness': 0.6666666666666666, 'ContextRecall': 1.0, 'ContextPrecision': 0.8333333332916666}, {'row': 5, 'AnswerRelevancy': 0.0, 'Faithfulness': 0.8333333333333334, 'ContextRecall': 0.0, 'ContextPrecision': 0.0}, {'row': 6, 'AnswerRelevancy': 0.9393254721491061, 'Faithfulness': 1.0, 'ContextRecall': 1.0, 'ContextPrecision': 0.5833333333041666}, {'row': 7, 'AnswerRelevancy': 0.9358363611152251, 'Faithfulness': 1.0, 'ContextRecall': 0.0, 'ContextPrecision': 0.0}, {'row': 8, 'AnswerRelevancy': 0.9303811737004329, 'Faithfulness': 1.0, 'ContextRecall': 1.0, 'ContextPrecision': 0.3333333333}, {'row': 9, 'AnswerRelevancy': 0.8931086750058114, 'Faithfulness': 0.5, 'ContextRecall': 0.0, 'ContextPrecision': 0.0}, {'row': 10, 'AnswerRelevancy': 0.87556912102331, 'Faithfulness': 1.0, 'ContextRecall': 1.0, 'ContextPrecision': 0.49999999995}]}\n"
     ]
    }
   ],
   "source": [
    "json_results = {\n",
    "    'average_scores': results['average_scores'],\n",
    "    'detailed_results': results['detailed_results']\n",
    "}\n",
    "\n",
    "json_filename = \"data/sample_ragas_result.json\"\n",
    "\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to {json_filename}\")\n",
    "print(json_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
