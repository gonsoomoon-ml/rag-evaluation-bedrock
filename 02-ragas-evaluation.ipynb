{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Generated Synthetic Datasets\n",
    "\n",
    "In this section, we load synthetic datasets that have been generated for testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"data/sample_qa_dataset.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+', '', s)\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    return s\n",
    "\n",
    "def convert_to_list(example):\n",
    "    cleaned_context = clean_string(example[\"contexts\"])\n",
    "    try:\n",
    "        contexts = ast.literal_eval(cleaned_context)\n",
    "    except:\n",
    "        contexts = cleaned_context\n",
    "    return {\"contexts\": contexts}\n",
    "\n",
    "\n",
    "subset_length = 10  # Change \n",
    "test_dataset = Dataset.from_pandas(df.head(subset_length))\n",
    "\n",
    "test_dataset = test_dataset.map(convert_to_list)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline Setting\n",
    "\n",
    "The test dataset is used to simulate real-world queries in a RAG pipeline, which combines document retrieval with response generation. \n",
    "\n",
    "Here, we are using the default settings for the KnowledgeBase in Amazon Bedrock as part of the RAG configuration.\n",
    "\n",
    "_1. To utilize the code below for testing, the KnowledgeBase must be pre-configured in the Amazon Bedrock console_\n",
    "\n",
    "_2. If you have a specific RAG pipeline you want to evaluate, please modify the cells below accordingly_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Retrieval\n",
    "\n",
    "In this section, we will test the systemâ€™s ability to retrieve relevant context from the KnowledgeBase using the provided queries. \n",
    "\n",
    "This is a critical step in the RAG pipeline, as the accuracy of the context retrieved has a direct impact on the quality of the generated responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG implementation sample 1 (Replace with RAG pipeline for evaluation)\n",
    "from libs.bedrock_kb_util import context_retrieval_from_kb\n",
    "\n",
    "question = test_dataset[0]['question']\n",
    "search_result = context_retrieval_from_kb(question, 3, 'us-west-2', 'CNDSUOPKAS', 'SEMANTIC')\n",
    "print(\"search_result[0]:\", search_result[0])\n",
    "\n",
    "contexts = \"\\n--\\n\".join([result['content'] for result in search_result])\n",
    "print(\"context:\", contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = 'us-west-2'\n",
    "\n",
    "retry_config = Config(\n",
    "    region_name=region,\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "boto3_client = boto3.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation\n",
    "\n",
    "Here, we are generating answers based on the retrieved context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, contexts):\n",
    "    system_prompt = \"\"\"You are an AI assistant that uses retrieved context to answer questions accurately. \n",
    "    Follow these guidelines:\n",
    "    1. Use the provided context to inform your answers.\n",
    "    2. If the context doesn't contain relevant information, say \"I don't have enough information to answer that.\"\n",
    "    3. Be concise and to the point in your responses.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {contexts}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Please answer the question based on the given context.\"\"\"\n",
    "\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{'role': 'user', 'content': [{'text': user_prompt}]}],\n",
    "        system=[{'text': system_prompt}]\n",
    "    )\n",
    "\n",
    "    answer = response['output']['message']['content'][0]['text']\n",
    "    return answer\n",
    "\n",
    "generate_answer(question, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Process for All Sample Questions\n",
    "\n",
    "This section runs the entire pipeline, from context retrieval to answer generation, across a set of sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "kb_region = 'us-west-2'\n",
    "kb_id = 'CNDSUOPKAS'\n",
    "top_k = 3\n",
    "\n",
    "def process_item(item):\n",
    "    sleep(5)  # Prevent throttling\n",
    "    question = item['question']\n",
    "    search_result = context_retrieval_from_kb(question, top_k, kb_region, kb_id, 'SEMANTIC')\n",
    "\n",
    "    contexts = [result['content'] for result in search_result]\n",
    "    answer = generate_answer(question, \"\\n--\\n\".join(contexts))\n",
    "\n",
    "    return {\n",
    "        'question': item['question'],\n",
    "        'ground_truth': item['ground_truth'],\n",
    "        'original_contexts': item['contexts'],\n",
    "        'retrieved_contexts': contexts,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "updated_dataset = test_dataset.map(process_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Intermediate Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_file = \"data/sample_processed_qa_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in updated_dataset:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Format Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/sample_processed_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\\n\\n\")\n",
    "print(f\"Answer: {item['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Each Metric\n",
    "\n",
    "We now evaluate the system based on various metrics. \n",
    "\n",
    "For detailed implementations, refer to the `libs/custom_ragas.py` file. \n",
    "\n",
    "This script contains the specific evaluation criteria that we use to assess the performance of the RAG pipeline across different dimensions, such as accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.custom_ragas import (\n",
    "    evaluate,\n",
    "    AnswerRelevancy, \n",
    "    Faithfulness, \n",
    "    ContextRecall,\n",
    "    ContextPrecision\n",
    ")\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "emb_id = \"amazon.titan-embed-text-v2:0\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "metrics = [AnswerRelevancy, Faithfulness, ContextRecall, ContextPrecision]\n",
    "\n",
    "def map_dataset(example):\n",
    "    return {\n",
    "        \"user_input\": example[\"question\"],\n",
    "        \"retrieved_contexts\": example[\"retrieved_contexts\"],\n",
    "        \"referenced_contexts\": example[\"original_contexts\"],\n",
    "        \"response\": example[\"answer\"],\n",
    "        \"reference\": example[\"ground_truth\"]\n",
    "    }\n",
    "\n",
    "dataset = updated_dataset.map(map_dataset)\n",
    "results = evaluate(dataset, metrics, llm_id, emb_id, region)\n",
    "\n",
    "print(\"Average Scores:\")\n",
    "print(results['average_scores'])\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "for row in results['detailed_results']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_results = {\n",
    "    'average_scores': results['average_scores'],\n",
    "    'detailed_results': results['detailed_results']\n",
    "}\n",
    "\n",
    "json_filename = \"data/sample_ragas_result.json\"\n",
    "\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to {json_filename}\")\n",
    "print(json_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
