{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Generated Synthetic Datasets\n",
    "\n",
    "In this section, we load synthetic datasets that have been generated for testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>question_type</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do temperature, Top K, and Top P parameter...</td>\n",
       "      <td>Temperature, Top K, and Top P are parameters t...</td>\n",
       "      <td>complex</td>\n",
       "      <td>• If you set a high temperature, the probabili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long will Amazon Bedrock support base mode...</td>\n",
       "      <td>Amazon Bedrock will support base models for a ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>• EOL: This version is no longer available for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the system handle a scenario where a ...</td>\n",
       "      <td>The system doesn't explicitly show a function ...</td>\n",
       "      <td>complex</td>\n",
       "      <td>'payment_date': ['2021-10-05', '2021-10-06', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of an S3 retrieval node in...</td>\n",
       "      <td>An S3 retrieval node lets you retrieve data fr...</td>\n",
       "      <td>simple</td>\n",
       "      <td>An S3 retrieval node lets you retrieve data fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can a developer create a new prompt versio...</td>\n",
       "      <td>To create a new prompt version, retrieve its i...</td>\n",
       "      <td>complex</td>\n",
       "      <td>make a CreatePromptVersion Agents for Amazon B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How do temperature, Top K, and Top P parameter...   \n",
       "1  How long will Amazon Bedrock support base mode...   \n",
       "2  How does the system handle a scenario where a ...   \n",
       "3  What is the purpose of an S3 retrieval node in...   \n",
       "4  How can a developer create a new prompt versio...   \n",
       "\n",
       "                                        ground_truth question_type  \\\n",
       "0  Temperature, Top K, and Top P are parameters t...       complex   \n",
       "1  Amazon Bedrock will support base models for a ...        simple   \n",
       "2  The system doesn't explicitly show a function ...       complex   \n",
       "3  An S3 retrieval node lets you retrieve data fr...        simple   \n",
       "4  To create a new prompt version, retrieve its i...       complex   \n",
       "\n",
       "                                            contexts  \n",
       "0  • If you set a high temperature, the probabili...  \n",
       "1  • EOL: This version is no longer available for...  \n",
       "2  'payment_date': ['2021-10-05', '2021-10-06', '...  \n",
       "3  An S3 retrieval node lets you retrieve data fr...  \n",
       "4  make a CreatePromptVersion Agents for Amazon B...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"data/sample_qa_dataset.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function convert_to_list at 0x7fcc0378e3e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7624840636c2457dbdfae82c6e457a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'ground_truth', 'question_type', 'contexts'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+', '', s)\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    return s\n",
    "\n",
    "def convert_to_list(example):\n",
    "    cleaned_context = clean_string(example[\"contexts\"])\n",
    "    try:\n",
    "        contexts = ast.literal_eval(cleaned_context)\n",
    "    except:\n",
    "        contexts = cleaned_context\n",
    "    return {\"contexts\": contexts}\n",
    "\n",
    "\n",
    "subset_length = 10  # Change \n",
    "test_dataset = Dataset.from_pandas(df.head(subset_length))\n",
    "\n",
    "test_dataset = test_dataset.map(convert_to_list)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline Setting\n",
    "\n",
    "The test dataset is used to simulate real-world queries in a RAG pipeline, which combines document retrieval with response generation. \n",
    "\n",
    "Here, we are using the default settings for the KnowledgeBase in Amazon Bedrock as part of the RAG configuration.\n",
    "\n",
    "_1. To utilize the code below for testing, the KnowledgeBase must be pre-configured in the Amazon Bedrock console_\n",
    "\n",
    "_2. If you have a specific RAG pipeline you want to evaluate, please modify the cells below accordingly_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Retrieval\n",
    "\n",
    "In this section, we will test the system’s ability to retrieve relevant context from the KnowledgeBase using the provided queries. \n",
    "\n",
    "This is a critical step in the RAG pipeline, as the accuracy of the context retrieved has a direct impact on the quality of the generated responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceNotFoundException",
     "evalue": "An error occurred (ResourceNotFoundException) when calling the Retrieve operation: Knowledge Base with id CNDSUOPKAS does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceNotFoundException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/rag-evaluation-bedrock/02-ragas-evaluation.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://5rpekozdhu6pwqt.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag-evaluation-bedrock/02-ragas-evaluation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbedrock_kb_util\u001b[39;00m \u001b[39mimport\u001b[39;00m context_retrieval_from_kb\n\u001b[1;32m      <a href='vscode-notebook-cell://5rpekozdhu6pwqt.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag-evaluation-bedrock/02-ragas-evaluation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m question \u001b[39m=\u001b[39m test_dataset[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://5rpekozdhu6pwqt.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag-evaluation-bedrock/02-ragas-evaluation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m search_result \u001b[39m=\u001b[39m context_retrieval_from_kb(question, \u001b[39m3\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mus-west-2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCNDSUOPKAS\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mSEMANTIC\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://5rpekozdhu6pwqt.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag-evaluation-bedrock/02-ragas-evaluation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msearch_result[0]:\u001b[39m\u001b[39m\"\u001b[39m, search_result[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://5rpekozdhu6pwqt.studio.us-east-1.sagemaker.aws/home/sagemaker-user/rag-evaluation-bedrock/02-ragas-evaluation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m contexts \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m--\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([result[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m search_result])\n",
      "File \u001b[0;32m~/rag-evaluation-bedrock/libs/bedrock_kb_util.py:6\u001b[0m, in \u001b[0;36mcontext_retrieval_from_kb\u001b[0;34m(prompt, top_k, region, kb_id, search_type)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcontext_retrieval_from_kb\u001b[39m(prompt, top_k, region, kb_id, search_type):\n\u001b[1;32m      5\u001b[0m     bedrock_agent_client \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mclient(\u001b[39m'\u001b[39m\u001b[39mbedrock-agent-runtime\u001b[39m\u001b[39m'\u001b[39m, region_name\u001b[39m=\u001b[39mregion)\n\u001b[0;32m----> 6\u001b[0m     response \u001b[39m=\u001b[39m bedrock_agent_client\u001b[39m.\u001b[39;49mretrieve(\n\u001b[1;32m      7\u001b[0m         knowledgeBaseId\u001b[39m=\u001b[39;49mkb_id,\n\u001b[1;32m      8\u001b[0m         retrievalConfiguration\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      9\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mvectorSearchConfiguration\u001b[39;49m\u001b[39m'\u001b[39;49m: {\n\u001b[1;32m     10\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39mnumberOfResults\u001b[39;49m\u001b[39m'\u001b[39;49m: top_k,\n\u001b[1;32m     11\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39moverrideSearchType\u001b[39;49m\u001b[39m'\u001b[39;49m: search_type\n\u001b[1;32m     12\u001b[0m             }\n\u001b[1;32m     13\u001b[0m         },\n\u001b[1;32m     14\u001b[0m         retrievalQuery\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     15\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m: prompt\n\u001b[1;32m     16\u001b[0m         }\n\u001b[1;32m     17\u001b[0m     )        \n\u001b[1;32m     18\u001b[0m     raw_result \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mretrievalResults\u001b[39m\u001b[39m'\u001b[39m, [])\n\u001b[1;32m     20\u001b[0m     search_result \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1017\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     error_code \u001b[39m=\u001b[39m error_info\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mQueryErrorCode\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m error_info\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1014\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1017\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1018\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1019\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mResourceNotFoundException\u001b[0m: An error occurred (ResourceNotFoundException) when calling the Retrieve operation: Knowledge Base with id CNDSUOPKAS does not exist"
     ]
    }
   ],
   "source": [
    "# RAG implementation sample 1 (Replace with RAG pipeline for evaluation)\n",
    "from libs.bedrock_kb_util import context_retrieval_from_kb\n",
    "\n",
    "question = test_dataset[0]['question']\n",
    "search_result = context_retrieval_from_kb(question, 3, 'us-west-2', 'CNDSUOPKAS', 'SEMANTIC')\n",
    "print(\"search_result[0]:\", search_result[0])\n",
    "\n",
    "contexts = \"\\n--\\n\".join([result['content'] for result in search_result])\n",
    "print(\"context:\", contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = 'us-west-2'\n",
    "\n",
    "retry_config = Config(\n",
    "    region_name=region,\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "boto3_client = boto3.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation\n",
    "\n",
    "Here, we are generating answers based on the retrieved context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, contexts):\n",
    "    system_prompt = \"\"\"You are an AI assistant that uses retrieved context to answer questions accurately. \n",
    "    Follow these guidelines:\n",
    "    1. Use the provided context to inform your answers.\n",
    "    2. If the context doesn't contain relevant information, say \"I don't have enough information to answer that.\"\n",
    "    3. Be concise and to the point in your responses.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {contexts}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Please answer the question based on the given context.\"\"\"\n",
    "\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{'role': 'user', 'content': [{'text': user_prompt}]}],\n",
    "        system=[{'text': system_prompt}]\n",
    "    )\n",
    "\n",
    "    answer = response['output']['message']['content'][0]['text']\n",
    "    return answer\n",
    "\n",
    "generate_answer(question, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Process for All Sample Questions\n",
    "\n",
    "This section runs the entire pipeline, from context retrieval to answer generation, across a set of sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "kb_region = 'us-west-2'\n",
    "kb_id = 'CNDSUOPKAS'\n",
    "top_k = 3\n",
    "\n",
    "def process_item(item):\n",
    "    sleep(5)  # Prevent throttling\n",
    "    question = item['question']\n",
    "    search_result = context_retrieval_from_kb(question, top_k, kb_region, kb_id, 'SEMANTIC')\n",
    "\n",
    "    contexts = [result['content'] for result in search_result]\n",
    "    answer = generate_answer(question, \"\\n--\\n\".join(contexts))\n",
    "\n",
    "    return {\n",
    "        'question': item['question'],\n",
    "        'ground_truth': item['ground_truth'],\n",
    "        'original_contexts': item['contexts'],\n",
    "        'retrieved_contexts': contexts,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "updated_dataset = test_dataset.map(process_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Intermediate Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_file = \"data/sample_processed_qa_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in updated_dataset:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Format Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/sample_processed_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\\n\\n\")\n",
    "print(f\"Answer: {item['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Each Metric\n",
    "\n",
    "We now evaluate the system based on various metrics. \n",
    "\n",
    "For detailed implementations, refer to the `libs/custom_ragas.py` file. \n",
    "\n",
    "This script contains the specific evaluation criteria that we use to assess the performance of the RAG pipeline across different dimensions, such as accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.custom_ragas import (\n",
    "    evaluate,\n",
    "    AnswerRelevancy, \n",
    "    Faithfulness, \n",
    "    ContextRecall,\n",
    "    ContextPrecision\n",
    ")\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "emb_id = \"amazon.titan-embed-text-v2:0\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "metrics = [AnswerRelevancy, Faithfulness, ContextRecall, ContextPrecision]\n",
    "\n",
    "def map_dataset(example):\n",
    "    return {\n",
    "        \"user_input\": example[\"question\"],\n",
    "        \"retrieved_contexts\": example[\"retrieved_contexts\"],\n",
    "        \"referenced_contexts\": example[\"original_contexts\"],\n",
    "        \"response\": example[\"answer\"],\n",
    "        \"reference\": example[\"ground_truth\"]\n",
    "    }\n",
    "\n",
    "dataset = updated_dataset.map(map_dataset)\n",
    "results = evaluate(dataset, metrics, llm_id, emb_id, region)\n",
    "\n",
    "print(\"Average Scores:\")\n",
    "print(results['average_scores'])\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "for row in results['detailed_results']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_results = {\n",
    "    'average_scores': results['average_scores'],\n",
    "    'detailed_results': results['detailed_results']\n",
    "}\n",
    "\n",
    "json_filename = \"data/sample_ragas_result.json\"\n",
    "\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to {json_filename}\")\n",
    "print(json_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
