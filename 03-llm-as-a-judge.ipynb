{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge\n",
    "\n",
    "While human evaluation is the gold standard for assessing preferences, it's slow and costly. \n",
    "\n",
    "Using LLMs as judges can reduce the need for human intervention and allow for faster iterations.\n",
    "\n",
    "### Advantages of LLM-as-a-Judge\n",
    "\n",
    "- Can evaluate answer quality based on user-defined criteria\n",
    "- Adaptable to various LLM use cases\n",
    "\n",
    "### Common Evaluation Criteria\n",
    "\n",
    "- Conciseness: Is the answer brief and to the point?\n",
    "- Relevance: Does the answer relate to the question?\n",
    "- Correctness: Is the answer accurate?\n",
    "- Coherence: Is the answer consistent?\n",
    "- Harmfulness: Does the answer contain harmful content?\n",
    "- Maliciousness: Is the answer malicious or detrimental?\n",
    "- Helpfulness: Is the answer useful?\n",
    "- Controversiality: Is the answer likely to spark debate?\n",
    "- Misogyny: Does the answer demean women?\n",
    "- Criminality: Does the answer promote illegal activities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Answer Grading\n",
    "\n",
    "This approach involves assigning scores to individual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.custom_llm_as_a_judge import Custom_LLM_Judge\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = \"us-west-2\"\n",
    "evaluator = Custom_LLM_Judge(llm_id, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# question & response\n",
    "question = \"Why is the sky blue?\"\n",
    "response = \"The sky is blue because of Rayleigh scattering.\"\n",
    "\n",
    "# evaluation criteria\n",
    "criteria_list = ['conciseness', 'relevance', 'coherence', 'helpfulness', 'controversiality', 'misogyny', 'criminality']\n",
    "\n",
    "for criterion in criteria_list:\n",
    "    basic_result = evaluator.evaluate(\"basic\", question, response, criterion)\n",
    "    print(f\"{criterion}: {basic_result}\")\n",
    "    sleep(3) # preventing API throttling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference-Guided Grading\n",
    "\n",
    "This method provides a reference solution to guide the LLM's evaluation.\n",
    "\n",
    "For assessing correctness, providing a pre-defined ground truth (label) or context can be effective.\n",
    "\n",
    "_In the example below, we provide 'ground_truth' as a reference for evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question & response & ground_truth\n",
    "question = \"Why is the sky blue?\"\n",
    "response = \"The sky is blue because of Rayleigh scattering.\"\n",
    "ground_truth = \"The sky appears blue due to the scattering of sunlight by air molecules, a phenomenon known as Rayleigh scattering.\"\n",
    "\n",
    "# evaluation criteria\n",
    "criteria_list = ['correctness']\n",
    "\n",
    "for criterion in criteria_list:\n",
    "    basic_result = evaluator.evaluate(\"labeled\", question, response, criterion, ground_truth=ground_truth)\n",
    "    print(f\"{criterion}: {basic_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In the example below, we provide 'context' as a reference for evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question & response & context\n",
    "question = \"Why is the sky blue?\"\n",
    "response = \"The sky is blue because of Rayleigh scattering.\"\n",
    "context = \"The color of the sky is determined by the way sunlight interacts with the Earth's atmosphere. This interaction is influenced by various factors including the composition of the atmosphere and the wavelengths of light.\"\n",
    "\n",
    "# evaluation criteria\n",
    "criteria_list = ['conciseness', 'relevance', 'correctness', 'coherence']\n",
    "\n",
    "for criterion in criteria_list:\n",
    "    basic_result = evaluator.evaluate(\"context-based\", question, response, criterion, context=context)\n",
    "    print(f\"{criterion}: {basic_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation based on LLM-as-a-Judge\n",
    "\n",
    "In this section, we evaluate the quality of LLM's RAG responses based on LLM-as-a-Judge approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/sample_processed_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "dataset = Dataset.from_list(list(read_jsonl(input_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "llm_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "region = \"us-west-2\"\n",
    "evaluator = Custom_LLM_Judge(llm_id, region)\n",
    "\n",
    "criteria_list = ['conciseness', 'relevance', 'correctness', 'coherence', 'helpfulness', 'controversiality', 'misogyny', 'criminality']\n",
    "results = []\n",
    "\n",
    "for i in range(min(5, len(dataset))):\n",
    "    item = dataset[i]\n",
    "    question = item['question']\n",
    "    response = item['answer']\n",
    "    ground_truth = item['ground_truth']\n",
    "    #contexts = item['contexts']\n",
    "    \n",
    "    row_results = {'question': question, 'answer': response}\n",
    "    print(f\"Evaluating question {i+1}: {question}\")\n",
    "    for criterion in criteria_list:\n",
    "        result = evaluator.evaluate(\"labeled\", question, response, criterion, ground_truth=ground_truth)\n",
    "        row_results[criterion] = result\n",
    "        print(f\"  {criterion}: {result}\")\n",
    "\n",
    "    results.append(row_results)\n",
    "    sleep(3) # Preventing API throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "json_filename = 'data/sample_llm_judge_results.json'\n",
    "\n",
    "\n",
    "results_list = results_df.to_dict('records')\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\nResults saved to {json_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
