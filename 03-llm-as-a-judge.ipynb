{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge\n",
    "\n",
    "While human evaluation is the gold standard for assessing preferences, it's slow and costly. \n",
    "\n",
    "Using LLMs as judges can reduce the need for human intervention and allow for faster iterations.\n",
    "\n",
    "### Advantages of LLM-as-a-Judge\n",
    "\n",
    "- Can evaluate answer quality based on user-defined criteria\n",
    "- Adaptable to various LLM use cases\n",
    "\n",
    "### Common Evaluation Criteria\n",
    "\n",
    "- Conciseness: Is the answer brief and to the point?\n",
    "- Relevance: Does the answer relate to the question?\n",
    "- Correctness: Is the answer accurate?\n",
    "- Coherence: Is the answer consistent?\n",
    "- Harmfulness: Does the answer contain harmful content?\n",
    "- Maliciousness: Is the answer malicious or detrimental?\n",
    "- Helpfulness: Is the answer useful?\n",
    "- Controversiality: Is the answer likely to spark debate?\n",
    "- Misogyny: Does the answer demean women?\n",
    "- Criminality: Does the answer promote illegal activities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade datasets==3.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Answer Grading\n",
    "\n",
    "This approach involves assigning scores to individual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.custom_llm_as_a_judge import Custom_LLM_Judge\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = \"us-west-2\"\n",
    "evaluator = Custom_LLM_Judge(llm_id, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conciseness: 10\n",
      "relevance: 8\n",
      "coherence: 5\n",
      "helpfulness: Y\n",
      "controversiality: N\n",
      "misogyny: N\n",
      "criminality: N\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "# question & response\n",
    "question = \"Why is the sky blue?\"\n",
    "response = \"The sky is blue because of Rayleigh scattering.\"\n",
    "\n",
    "# evaluation criteria\n",
    "criteria_list = ['conciseness', 'relevance', 'coherence', 'helpfulness', 'controversiality', 'misogyny', 'criminality']\n",
    "\n",
    "for criterion in criteria_list:\n",
    "    basic_result = evaluator.evaluate(\"basic\", question, response, criterion)\n",
    "    print(f\"{criterion}: {basic_result}\")\n",
    "    sleep(3) # preventing API throttling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference-Guided Grading\n",
    "\n",
    "This method provides a reference solution to guide the LLM's evaluation.\n",
    "\n",
    "For assessing correctness, providing a pre-defined ground truth (label) or context can be effective.\n",
    "\n",
    "_In the example below, we provide 'ground_truth' as a reference for evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correctness: 8\n"
     ]
    }
   ],
   "source": [
    "# question & response & ground_truth\n",
    "question = \"Why is the sky blue?\"\n",
    "response = \"The sky is blue because of Rayleigh scattering.\"\n",
    "ground_truth = \"The sky appears blue due to the scattering of sunlight by air molecules, a phenomenon known as Rayleigh scattering.\"\n",
    "\n",
    "# evaluation criteria\n",
    "criteria_list = ['correctness']\n",
    "\n",
    "for criterion in criteria_list:\n",
    "    basic_result = evaluator.evaluate(\"labeled\", question, response, criterion, ground_truth=ground_truth)\n",
    "    print(f\"{criterion}: {basic_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In the example below, we provide 'context' as a reference for evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conciseness: 10\n",
      "relevance: 8\n",
      "correctness: 9\n",
      "coherence: 7\n"
     ]
    }
   ],
   "source": [
    "# question & response & context\n",
    "question = \"Why is the sky blue?\"\n",
    "response = \"The sky is blue because of Rayleigh scattering.\"\n",
    "context = \"The color of the sky is determined by the way sunlight interacts with the Earth's atmosphere. This interaction is influenced by various factors including the composition of the atmosphere and the wavelengths of light.\"\n",
    "\n",
    "# evaluation criteria\n",
    "criteria_list = ['conciseness', 'relevance', 'correctness', 'coherence']\n",
    "\n",
    "for criterion in criteria_list:\n",
    "    basic_result = evaluator.evaluate(\"context-based\", question, response, criterion, context=context)\n",
    "    print(f\"{criterion}: {basic_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation based on LLM-as-a-Judge\n",
    "\n",
    "In this section, we evaluate the quality of LLM's RAG responses based on LLM-as-a-Judge approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "# from datasets import Dataset\n",
    "# # show the Dataset version\n",
    "# print(Dataset.__version__)\n",
    "\n",
    "import datasets\n",
    "print(datasets.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/sample_processed_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "dataset = Dataset.from_list(list(read_jsonl(input_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating question 1: How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\n",
      "  conciseness: 9\n",
      "  relevance: 10\n",
      "  correctness: 9\n",
      "  coherence: 9\n",
      "  helpfulness: Y\n",
      "  controversiality: N\n",
      "  misogyny: N\n",
      "  criminality: N\n",
      "Evaluating question 2: How long will Amazon Bedrock support base models after launch in a region?\n",
      "  conciseness: 10\n",
      "  relevance: Y\n",
      "  correctness: Y\n",
      "  coherence: 10\n",
      "  helpfulness: Y\n",
      "  controversiality: N\n",
      "  misogyny: N\n",
      "  criminality: N\n",
      "Evaluating question 3: How does the system handle a scenario where a transaction status changes from 'Pending' to 'Paid', and what functions would be involved in updating and retrieving this information?\n",
      "  conciseness: 8\n",
      "  relevance: 7\n",
      "  correctness: 8\n",
      "  coherence: 8\n",
      "  helpfulness: Y\n",
      "  controversiality: Y\n",
      "  misogyny: N\n",
      "  criminality: N\n",
      "Evaluating question 4: What is the purpose of an S3 retrieval node in a prompt flow?\n",
      "  conciseness: 9\n",
      "  relevance: 10\n",
      "  correctness: 10\n",
      "  coherence: 9\n",
      "  helpfulness: Y\n",
      "  controversiality: N\n",
      "  misogyny: N\n",
      "  criminality: N\n",
      "Evaluating question 5: How can a developer create a new prompt version, retrieve its information, and incorporate it into a prompt flow using Amazon Bedrock's Python SDK?\n",
      "  conciseness: 9\n",
      "  relevance: 9\n",
      "  correctness: 9\n",
      "  coherence: 9\n",
      "  helpfulness: Y\n",
      "  controversiality: N\n",
      "  misogyny: N\n",
      "  criminality: N\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "llm_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "region = \"us-west-2\"\n",
    "evaluator = Custom_LLM_Judge(llm_id, region)\n",
    "\n",
    "criteria_list = ['conciseness', 'relevance', 'correctness', 'coherence', 'helpfulness', 'controversiality', 'misogyny', 'criminality']\n",
    "results = []\n",
    "\n",
    "for i in range(min(5, len(dataset))):\n",
    "    item = dataset[i]\n",
    "    question = item['question']\n",
    "    response = item['answer']\n",
    "    ground_truth = item['ground_truth']\n",
    "    #contexts = item['contexts']\n",
    "    \n",
    "    row_results = {'question': question, 'answer': response}\n",
    "    print(f\"Evaluating question {i+1}: {question}\")\n",
    "    for criterion in criteria_list:\n",
    "        result = evaluator.evaluate(\"labeled\", question, response, criterion, ground_truth=ground_truth)\n",
    "        row_results[criterion] = result\n",
    "        print(f\"  {criterion}: {result}\")\n",
    "\n",
    "    results.append(row_results)\n",
    "    sleep(3) # Preventing API throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  How do temperature, Top K, and Top P parameter...   \n",
      "1  How long will Amazon Bedrock support base mode...   \n",
      "2  How does the system handle a scenario where a ...   \n",
      "3  What is the purpose of an S3 retrieval node in...   \n",
      "4  How can a developer create a new prompt versio...   \n",
      "\n",
      "                                              answer  conciseness relevance  \\\n",
      "0  Based on the provided context, here's how temp...            9        10   \n",
      "1  According to the context provided, Amazon Bedr...           10         Y   \n",
      "2  Based on the provided context, I don't have en...            8         7   \n",
      "3  Based on the provided context, the purpose of ...            9        10   \n",
      "4  Based on the provided context, here's how a de...            9         9   \n",
      "\n",
      "  correctness  coherence helpfulness controversiality misogyny criminality  \n",
      "0           9          9           Y                N        N           N  \n",
      "1           Y         10           Y                N        N           N  \n",
      "2           8          8           Y                Y        N           N  \n",
      "3          10          9           Y                N        N           N  \n",
      "4           9          9           Y                N        N           N  \n",
      "\n",
      "Results saved to data/sample_llm_judge_results.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "json_filename = 'data/sample_llm_judge_results.json'\n",
    "\n",
    "\n",
    "results_list = results_df.to_dict('records')\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\nResults saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
