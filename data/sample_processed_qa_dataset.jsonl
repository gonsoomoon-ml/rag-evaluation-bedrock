{"question": "How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?", "ground_truth": "Temperature, Top K, and Top P are parameters that work together to control the randomness and diversity of the model's output in Amazon Bedrock's foundation models. Using the equine example:\n\n1. Temperature: A higher temperature flattens the probability distribution, increasing the chance of selecting less probable options like \"unicorns\" while decreasing the likelihood of more common choices like \"horses\".\n\n2. Top K: This parameter limits the selection to the K most likely candidates. Setting Top K to 2 would only consider \"horses\" and \"zebras\", excluding less probable options like \"unicorns\".\n\n3. Top P: This sets a cumulative probability threshold. With Top P at 0.7, only \"horses\" would be considered as it's the only option within the top 70% of the probability distribution. Increasing Top P to 0.9 would include both \"horses\" and \"zebras\".\n\nThe interaction of these parameters can significantly impact the output. For instance, a high temperature with a low Top K might still produce varied results within the limited selection, while a low temperature with a high Top P could lead to more predictable, common outputs. Balancing these parameters allows for fine-tuning between creativity and coherence in the generated text about equines or any other topic.", "question_type": "complex", "contexts": " If you set a high temperature, the probability distribution is flattened and the probabilities\nbecome less different, which would increase the probability of choosing \"unicorns\" and decrease\nthe probability of choosing \"horses\".\nRandomness and diversity 239\nAmazon Bedrock User Guide\n If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and\n\"zebras.\"\n If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that\nlies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers\n\"horses\" and \"zebras\" as they are in the top 90% of probability distribution.\nLength\nFoundation models typically support parameters that limit the length of the response. Examples of\nthese parameters are provided below.\n Response length  An exact value to specify the minimum or maximum number of tokens to\nreturn in the generated response.\n  Penalties  Specify the degree to which to penalize outputs in a response. Examples include the\nfollowing.\n The length of the response.\n Repeated tokens in a response.\n Frequency of tokens in a response.\n Types of tokens in a response.\n Stop sequences  Specify sequences of characters that stop the model from generating further\ntokens. If the model generates a stop sequence that you specify, it will stop generating after that\nsequence.\nPlaygrounds\nImportant\nBefore you can use any of the foundation models, you must request access to that model\nthrough the Amazon Bedrock console. You can manage model access only through the\nconsole. If you try to use the model (with the API or within the console) before you have\nrequested access to it, you\"ll receive an error message. For more information, see Manage\naccess to Amazon Bedrock foundation models.\nLength 240\nAmazon Bedrock User Guide\nThe Amazon Bedrock playgrounds provide you a console environment to experiment with running\n inference on different models and with different configurations, before deciding to use them in an\napplication. In the console, access the playgrounds by choosing Playgrounds in the left navigation\npane. You can also navigate directly to the playground when you choose a model from a model\ndetails page or the examples page.\nThere are playgrounds for text, chat, and image models.\nWithin each playground you can enter prompts and experiment with inference parameters.\nPrompts are usually one or more sentences of text that set up a scenario, question, or task for a\nmodel. For information about creating prompts, see Prompt engineering guidelines.\nInference parameters influence the response generated by a model, such as the randomness of\ngenerated text. When you load a model into a playground, the playground configures the model\nwith its default inference settings. You can change and reset the settings as you experiment\n", "original_contexts": " If you set a high temperature, the probability distribution is flattened and the probabilities\nbecome less different, which would increase the probability of choosing \"unicorns\" and decrease\nthe probability of choosing \"horses\".\nRandomness and diversity 239\nAmazon Bedrock User Guide\n If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and\n\"zebras.\"\n If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that\nlies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers\n\"horses\" and \"zebras\" as they are in the top 90% of probability distribution.\nLength\nFoundation models typically support parameters that limit the length of the response. Examples of\nthese parameters are provided below.\n Response length  An exact value to specify the minimum or maximum number of tokens to\nreturn in the generated response.\n  Penalties  Specify the degree to which to penalize outputs in a response. Examples include the\nfollowing.\n The length of the response.\n Repeated tokens in a response.\n Frequency of tokens in a response.\n Types of tokens in a response.\n Stop sequences  Specify sequences of characters that stop the model from generating further\ntokens. If the model generates a stop sequence that you specify, it will stop generating after that\nsequence.\nPlaygrounds\nImportant\nBefore you can use any of the foundation models, you must request access to that model\nthrough the Amazon Bedrock console. You can manage model access only through the\nconsole. If you try to use the model (with the API or within the console) before you have\nrequested access to it, you\"ll receive an error message. For more information, see Manage\naccess to Amazon Bedrock foundation models.\nLength 240\nAmazon Bedrock User Guide\nThe Amazon Bedrock playgrounds provide you a console environment to experiment with running\n inference on different models and with different configurations, before deciding to use them in an\napplication. In the console, access the playgrounds by choosing Playgrounds in the left navigation\npane. You can also navigate directly to the playground when you choose a model from a model\ndetails page or the examples page.\nThere are playgrounds for text, chat, and image models.\nWithin each playground you can enter prompts and experiment with inference parameters.\nPrompts are usually one or more sentences of text that set up a scenario, question, or task for a\nmodel. For information about creating prompts, see Prompt engineering guidelines.\nInference parameters influence the response generated by a model, such as the randomness of\ngenerated text. When you load a model into a playground, the playground configures the model\nwith its default inference settings. You can change and reset the settings as you experiment\n", "retrieved_contexts": ["Randomness and diversity 239        Amazon Bedrock User Guide   â€¢ If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and  \"zebras.\"   â€¢ If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that  lies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers  \"horses\" and \"zebras\" as they are in the top 90% of probability distribution.   Length   Foundation models typically support parameters that limit the length of the response. Examples of  these parameters are provided below.   â€¢ Response length â€“ An exact value to specify the minimum or maximum number of tokens to  return in the generated response.   â€¢ Penalties â€“ Specify the degree to which to penalize outputs in a response. Examples include the  following.   â€¢ The length of the response.   â€¢ Repeated tokens in a response.   â€¢ Frequency of tokens in a response.   â€¢ Types of tokens in a response.   â€¢ Stop sequences â€“ Specify sequences of characters that stop the model from generating further  tokens. If the model generates a stop sequence that you specify, it will stop generating after that  sequence.", "The following table summarizes the effects of these parameters.   Parameter Effect of lower value Effect of higher value   Temperature Increase likelihood of higher- probability tokens   Decrease likelihood of lower- probability tokens   Increase likelihood of lower- probability tokens   Decrease likelihood of higher- probability tokens   Top K Remove lower-probability  tokens   Allow lower-probability  tokens   Top P Remove lower-probability  tokens   Allow lower-probability  tokens   As an example to understand these parameters, consider the example prompt I hear the hoof  beats of \". Let's say that the model determines the following three words to be candidates for  the next token. The model also assigns a probability for each word.   {      \"horses\": 0.7,      \"zebras\": 0.2,      \"unicorns\": 0.1 }   â€¢ If you set a high temperature, the probability distribution is flattened and the probabilities  become less different, which would increase the probability of choosing \"unicorns\" and decrease  the probability of choosing \"horses\".   Randomness and diversity 239        Amazon Bedrock User Guide   â€¢ If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and  \"zebras.\"", "General guidelines for Amazon Bedrock LLM users 297        Amazon Bedrock User Guide   (Source: Prompt written by AWS)   Use inference parameters   LLMs on Amazon Bedrock all come with several inference parameters that you can set to control  the response from the models. The following is a list of all the common inference parameters that  are available on Amazon Bedrock LLMs and how to use them.   Temperature is a value between 0 and 1, and it regulates the creativity of LLMsâ€™ responses. Use  lower temperature if you want more deterministic responses, and use higher temperature if you  want more creative or different responses for the same prompt from LLMs on Amazon Bedrock. For  all the examples in this prompt guideline, we set temperature = 0.   Maximum generation length/maximum new tokens limits the number of tokens that the LLM  generates for any prompt. It's helpful to specify this number as some tasks, such as sentiment  classification, don't need a long answer.   Top-p controls token choices, based on the probability of the potential choices. If you set Top-p  below 1.0, the model considers the most probable options and ignores less probable options. The  result is more stable and repetitive completions.   Use inference parameters 298        Amazon Bedrock User Guide   End token/end sequence specifies the token that the LLM uses to indicate the end of the output."], "answer": "Based on the provided context, here's how temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, using the example of generating text about different types of equines:\n\n1. Temperature: This parameter affects the overall probability distribution of tokens.\n   - Lower temperature increases the likelihood of high-probability tokens (e.g., \"horses\") and decreases the likelihood of low-probability tokens (e.g., \"unicorns\").\n   - Higher temperature flattens the probability distribution, making it more likely to choose lower-probability tokens like \"unicorns\" and less likely to choose high-probability tokens like \"horses\".\n\n2. Top K: This parameter limits the number of token candidates considered.\n   - For example, if Top K is set to 2, only the top 2 most likely candidates (\"horses\" and \"zebras\") would be considered, excluding \"unicorns\".\n\n3. Top P: This parameter considers tokens based on their cumulative probability.\n   - If Top P is set to 0.7, only \"horses\" would be considered as it's the only candidate within the top 70% of the probability distribution.\n   - If Top P is set to 0.9, both \"horses\" and \"zebras\" would be considered as they fall within the top 90% of the probability distribution.\n\nAdjusting these parameters would affect the output about equines as follows:\n- Lower temperature, lower Top K, or lower Top P would lead to more predictable outputs, likely favoring common equines like horses.\n- Higher temperature, higher Top K, or higher Top P would increase the chances of generating text about less common equines like zebras or even fictional ones like unicorns.\n\nThese parameters can be used in combination to fine-tune the balance between creativity and predictability in the generated text."}
{"question": "How long will Amazon Bedrock support base models after launch in a region?", "ground_truth": "Amazon Bedrock will support base models for a minimum of 12 months from the launch in a region.", "question_type": "simple", "contexts": " EOL: This version is no longer available for use. Any requests made to this version will fail.\nWe will support base models for a minimum of 12 months from the launch in a region. We will\nalways give customers 6 months of notice before we mark the model EOL. Model will be marked\nLegacy on the date when the EOL notice is sent out. The console marks a model version\"s state as\nActive or Legacy. When you make a GetFoundationModelor ListFoundationModels call, you can\nfind the state of the model in the modelLifecycle field in the response. While you can continue\nto use a Legacy version, you should plan to transition to an Active version before the EOL date.\nOn-Demand, Provisioned Throughput, and model customization\nYou specify the version of a model when you use it in On-Demand mode (for example,\nanthropic.claude-v2, anthropic.claude-v2:1, etc.).\nWhen you configure Provisioned Throughput, you must specify a model version that will remain\nunchanged for the entire term.\nModel lifecycle 53\n Amazon Bedrock User Guide\nIf you customized a model, you can continue to use it until the EOL date of the base model version\nthat you used for customization. You can also customize a legacy model version, but you should\nplan to migrate before it reaches its EOL date.\nNote\nService quotas are shared among model minor versions.\nLegacy versions\nThe following table shows the legacy versions of models available on Amazon Bedrock.\nModel version Legacy date EOL date Recommended Recommended\nmodel version model ID\nreplacement\nStable Diffusion February 2, April 30, 2024 Stable Diffusion stability.stable-d\nXL 0.8 2024 XL 1.x iffusion-xl-v1\nClaude v1.3 November 28, February 28, Claude v2.1 anthropic\n2023 2024 .claude-v2:1\nTitan November 7, February 15, Titan amazon.titan-\nEmbeddings - 2023 2024 Embeddings - embed-text-v1\nText v1.1 Text v1.2\nMeta Llama 2 May 12, 2024 October 30, Meta Llama3 meta.llam\n13b-chat-v1, 2024 and Meta a3-1-8b-i\nMeta Llama 2 Llama3.1 nstruct-v1,\n70b-chat-v1, meta.llam\n Meta Llama a3-1-70b-\n2-13b, Meta instruct-v1,\nLlama 2-70b meta.llam\na3-1-405b-\ninstruct-v1,\nmeta.llama3-8b-\ninstruct-v1,\nLegacy versions 54\nAmazon Bedrock User Guide\nModel version Legacy date EOL date Recommended Recommended\nmodel version model ID\nreplacement\nand meta.llam\na3-70b-instruct-\nv1\nAi21 J2 Mid- April 30, 2024 August 31, 2024 N/A N/A\nv1 and Ai21 J2 (only in us- (only in us-\nUltra-v1 west-2) west-2)\nAmazon Bedrock model IDs\nMany Amazon Bedrock API operations require the use of a model ID. Refer to the following table to\ndetermine where to find the model ID that you need to use.\nUse case How to find the model ID\nUse a base model Look up the ID in the base model IDs chart\nPurchase Provisioned Throughput for a base Look up the ID in the model IDs for Provision\nmodel ed Throughput chart and use it as the\nmodelId in the CreateProvisionedModelThrou\nghput request.\nPurchase Provisioned Throughput for a Use the name of the custom model or its ARN\n", "original_contexts": " EOL: This version is no longer available for use. Any requests made to this version will fail.\nWe will support base models for a minimum of 12 months from the launch in a region. We will\nalways give customers 6 months of notice before we mark the model EOL. Model will be marked\nLegacy on the date when the EOL notice is sent out. The console marks a model version\"s state as\nActive or Legacy. When you make a GetFoundationModelor ListFoundationModels call, you can\nfind the state of the model in the modelLifecycle field in the response. While you can continue\nto use a Legacy version, you should plan to transition to an Active version before the EOL date.\nOn-Demand, Provisioned Throughput, and model customization\nYou specify the version of a model when you use it in On-Demand mode (for example,\nanthropic.claude-v2, anthropic.claude-v2:1, etc.).\nWhen you configure Provisioned Throughput, you must specify a model version that will remain\nunchanged for the entire term.\nModel lifecycle 53\n Amazon Bedrock User Guide\nIf you customized a model, you can continue to use it until the EOL date of the base model version\nthat you used for customization. You can also customize a legacy model version, but you should\nplan to migrate before it reaches its EOL date.\nNote\nService quotas are shared among model minor versions.\nLegacy versions\nThe following table shows the legacy versions of models available on Amazon Bedrock.\nModel version Legacy date EOL date Recommended Recommended\nmodel version model ID\nreplacement\nStable Diffusion February 2, April 30, 2024 Stable Diffusion stability.stable-d\nXL 0.8 2024 XL 1.x iffusion-xl-v1\nClaude v1.3 November 28, February 28, Claude v2.1 anthropic\n2023 2024 .claude-v2:1\nTitan November 7, February 15, Titan amazon.titan-\nEmbeddings - 2023 2024 Embeddings - embed-text-v1\nText v1.1 Text v1.2\nMeta Llama 2 May 12, 2024 October 30, Meta Llama3 meta.llam\n13b-chat-v1, 2024 and Meta a3-1-8b-i\nMeta Llama 2 Llama3.1 nstruct-v1,\n70b-chat-v1, meta.llam\n Meta Llama a3-1-70b-\n2-13b, Meta instruct-v1,\nLlama 2-70b meta.llam\na3-1-405b-\ninstruct-v1,\nmeta.llama3-8b-\ninstruct-v1,\nLegacy versions 54\nAmazon Bedrock User Guide\nModel version Legacy date EOL date Recommended Recommended\nmodel version model ID\nreplacement\nand meta.llam\na3-70b-instruct-\nv1\nAi21 J2 Mid- April 30, 2024 August 31, 2024 N/A N/A\nv1 and Ai21 J2 (only in us- (only in us-\nUltra-v1 west-2) west-2)\nAmazon Bedrock model IDs\nMany Amazon Bedrock API operations require the use of a model ID. Refer to the following table to\ndetermine where to find the model ID that you need to use.\nUse case How to find the model ID\nUse a base model Look up the ID in the base model IDs chart\nPurchase Provisioned Throughput for a base Look up the ID in the model IDs for Provision\nmodel ed Throughput chart and use it as the\nmodelId in the CreateProvisionedModelThrou\nghput request.\nPurchase Provisioned Throughput for a Use the name of the custom model or its ARN\n", "retrieved_contexts": ["import boto3   bedrock = boto3.client(service_name='bedrock')   bedrock.get_foundation_model(modelIdentifier='anthropic.claude-v2')   Model support by AWS Region   Note   All models, except Anthropic Claude 3 Opus, Amazon Titan Text Premier, and Mistral Small  are supported in both the US East (N. Virginia, us-east-1) and the US West (Oregon, us- west-2) Regions. Amazon Titan Text Premier, Mistral Small, and AI21 Jamba-Instruct models are only  available in the US East (N. Virginia, us-east-1) Region. Anthropic Claude 3 Opus, Meta Llama 3.1 Instruct, and Mistral Large 2 (24.07) models are  only available in the US West (Oregon, us-west-2) Region.   Model support by AWS Region 39        Amazon Bedrock User Guide   Note   Access to models in Europe (Ireland) and Asia Pacific (Singapore) Regions are currently  gated. Please contact your account manager to request model access in these Regions.   The following table shows the FMs that are available in other Regions and whether they're  supported in each Region.   Model Asia  Pacific  (Mumbai)   Asia  Pacific  (Singapor  e)  NOTE:  Gated  access  only   Asia  Pacific  (Sydney)   Asia", "Guide   Note   The Meta Llama 2 (non-chat) models can only be used after being customized and after purchasing Provisioned Throughput for them.   Model lifecycle   Amazon Bedrock is continuously working to bring the latest versions of foundation models that  have better capabilities, accuracy, and safety. As we launch new model versions, you can test them  with the Amazon Bedrock console or API, and migrate your applications to benefit from the latest  model versions.   A model offered on Amazon Bedrock can be in one of these states: Active, Legacy, or  End-of-Life  (EOL).   â€¢ Active: The model provider is actively working on this version, and it will continue to get updates  such as bug fixes and minor improvements.   â€¢ Legacy: A version is marked Legacy when there is a more recent version which provides superior  performance. Amazon Bedrock sets an EOL date for Legacy versions.   â€¢ EOL: This version is no longer available for use. Any requests made to this version will fail.   We will support base models for a minimum of 12 months from the launch in a region. We will  always give customers 6 months of notice before we mark the model EOL. Model will be marked  Legacy on the date when the EOL notice is sent out. The console marks a model version's state as Active or Legacy.", "Amazon Bedrock sets an EOL date for Legacy versions.   â€¢ EOL: This version is no longer available for use. Any requests made to this version will fail.   We will support base models for a minimum of 12 months from the launch in a region. We will  always give customers 6 months of notice before we mark the model EOL. Model will be marked  Legacy on the date when the EOL notice is sent out. The console marks a model version's state as Active or Legacy. When you make a GetFoundationModelor ListFoundationModels call, you can  find the state of the model in the modelLifecycle field in the response. While you can continue  to use a Legacy version, you should plan to transition to an Active version before the EOL date.   On-Demand, Provisioned Throughput, and model customization   You specify the version of a model when you use it in On-Demand mode (for example, anthropic.claude-v2, anthropic.claude-v2:1, etc.).   When you configure Provisioned Throughput, you must specify a model version that will remain  unchanged for the entire term."], "answer": "According to the context provided, Amazon Bedrock will support base models for a minimum of 12 months from the launch in a region."}
{"question": "How does the system handle a scenario where a transaction status changes from 'Pending' to 'Paid', and what functions would be involved in updating and retrieving this information?", "ground_truth": "The system doesn't explicitly show a function for updating transaction statuses, but it does provide functions for retrieving payment status and date. If a transaction status changes from 'Pending' to 'Paid', the underlying data in the DataFrame 'df' would need to be updated first. Then, to reflect and retrieve this change, two functions would be involved:\n\n1. retrieve_payment_status(df, transaction_id): This function would return the updated status 'Paid' for the given transaction_id.\n\n2. retrieve_payment_date(df, transaction_id): This function would return the date when the payment was made, which would likely be updated when the status changes to 'Paid'.\n\nThese functions are wrapped in a tool-calling system that uses Amazon Bedrock with a Mistral AI model. The invoke_bedrock_mistral_tool() function would be used to call these functions through the AI model, interpreting user queries and returning the appropriate information. The system uses JSON for data exchange and includes error handling for cases where a transaction ID is not found.", "question_type": "complex", "contexts": "\"payment_date\": [\"2021-10-05\", \"2021-10-06\", \"2021-10-07\", \"2021-10-05\",\n\"2021-10-08\"],\n\"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"]\n}\n# Create DataFrame\ndf = pd.DataFrame(data)\ndef retrieve_payment_status(df: data, transaction_id: str) -> str:\nif transaction_id in df.transaction_id.values:\nreturn json.dumps({\"status\": df[df.transaction_id ==\ntransaction_id].payment_status.item()})\nreturn json.dumps({\"error\": \"transaction id not found.\"})\nMistral AI models 198\nAmazon Bedrock User Guide\ndef retrieve_payment_date(df: data, transaction_id: str) -> str:\nif transaction_id in df.transaction_id.values:\nreturn json.dumps({\"date\": df[df.transaction_id ==\ntransaction_id].payment_date.item()})\nreturn json.dumps({\"error\": \"transaction id not found.\"})\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"retrieve_payment_status\",\n\"description\": \"Get payment status of a transaction\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"transaction_id\": {\n\"type\": \"string\",\n \"description\": \"The transaction id.\",\n}\n},\n\"required\": [\"transaction_id\"],\n},\n},\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"retrieve_payment_date\",\n\"description\": \"Get payment date of a transaction\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"transaction_id\": {\n\"type\": \"string\",\n\"description\": \"The transaction id.\",\n}\n},\n\"required\": [\"transaction_id\"],\n},\n},\n}\n]\nnames_to_functions = {\nMistral AI models 199\nAmazon Bedrock User Guide\n\"retrieve_payment_status\": functools.partial(retrieve_payment_status, df=df),\n\"retrieve_payment_date\": functools.partial(retrieve_payment_date, df=df)\n}\ntest_tool_input = \"What\"s the status of my transaction T1001?\"\nmessage = [{\"role\": \"user\", \"content\": test_tool_input}]\ndef invoke_bedrock_mistral_tool():\nmistral_params = {\n\"body\": json.dumps({\n\"messages\": message,\n\"tools\": tools\n}),\n\"modelId\":\"mistral.mistral-large-2407-v1:0\",\n}\nresponse = bedrock.invoke_model(**mistral_params)\nbody = response.get(\"body\").read().decode(\"utf-8\")\n body = json.loads(body)\nchoices = body.get(\"choices\")\nmessage.append(choices[0].get(\"message\"))\ntool_call = choices[0].get(\"message\").get(\"tool_calls\")[0]\nfunction_name = tool_call.get(\"function\").get(\"name\")\nfunction_params = json.loads(tool_call.get(\"function\").get(\"arguments\"))\nprint(\"\\nfunction_name: \", function_name, \"\\nfunction_params: \",\nfunction_params)\nfunction_result = names_to_functions[function_name](**function_params)\nmessage.append({\"role\": \"tool\", \"content\": function_result,\n\"tool_call_id\":tool_call.get(\"id\")})\nnew_mistral_params = {\n\"body\": json.dumps({\n\"messages\": message,\n\"tools\": tools\n}),\n\"modelId\":\"mistral.mistral-large-2407-v1:0\",\n}\nresponse = bedrock.invoke_model(**new_mistral_params)\nbody = response.get(\"body\").read().decode(\"utf-8\")\nMistral AI models 200\nAmazon Bedrock User Guide\nbody = json.loads(body)\nprint(body)\ninvoke_bedrock_mistral_tool()\nStability.ai Diffusion models\n", "original_contexts": "\"payment_date\": [\"2021-10-05\", \"2021-10-06\", \"2021-10-07\", \"2021-10-05\",\n\"2021-10-08\"],\n\"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"]\n}\n# Create DataFrame\ndf = pd.DataFrame(data)\ndef retrieve_payment_status(df: data, transaction_id: str) -> str:\nif transaction_id in df.transaction_id.values:\nreturn json.dumps({\"status\": df[df.transaction_id ==\ntransaction_id].payment_status.item()})\nreturn json.dumps({\"error\": \"transaction id not found.\"})\nMistral AI models 198\nAmazon Bedrock User Guide\ndef retrieve_payment_date(df: data, transaction_id: str) -> str:\nif transaction_id in df.transaction_id.values:\nreturn json.dumps({\"date\": df[df.transaction_id ==\ntransaction_id].payment_date.item()})\nreturn json.dumps({\"error\": \"transaction id not found.\"})\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"retrieve_payment_status\",\n\"description\": \"Get payment status of a transaction\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"transaction_id\": {\n\"type\": \"string\",\n \"description\": \"The transaction id.\",\n}\n},\n\"required\": [\"transaction_id\"],\n},\n},\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"retrieve_payment_date\",\n\"description\": \"Get payment date of a transaction\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"transaction_id\": {\n\"type\": \"string\",\n\"description\": \"The transaction id.\",\n}\n},\n\"required\": [\"transaction_id\"],\n},\n},\n}\n]\nnames_to_functions = {\nMistral AI models 199\nAmazon Bedrock User Guide\n\"retrieve_payment_status\": functools.partial(retrieve_payment_status, df=df),\n\"retrieve_payment_date\": functools.partial(retrieve_payment_date, df=df)\n}\ntest_tool_input = \"What\"s the status of my transaction T1001?\"\nmessage = [{\"role\": \"user\", \"content\": test_tool_input}]\ndef invoke_bedrock_mistral_tool():\nmistral_params = {\n\"body\": json.dumps({\n\"messages\": message,\n\"tools\": tools\n}),\n\"modelId\":\"mistral.mistral-large-2407-v1:0\",\n}\nresponse = bedrock.invoke_model(**mistral_params)\nbody = response.get(\"body\").read().decode(\"utf-8\")\n body = json.loads(body)\nchoices = body.get(\"choices\")\nmessage.append(choices[0].get(\"message\"))\ntool_call = choices[0].get(\"message\").get(\"tool_calls\")[0]\nfunction_name = tool_call.get(\"function\").get(\"name\")\nfunction_params = json.loads(tool_call.get(\"function\").get(\"arguments\"))\nprint(\"\\nfunction_name: \", function_name, \"\\nfunction_params: \",\nfunction_params)\nfunction_result = names_to_functions[function_name](**function_params)\nmessage.append({\"role\": \"tool\", \"content\": function_result,\n\"tool_call_id\":tool_call.get(\"id\")})\nnew_mistral_params = {\n\"body\": json.dumps({\n\"messages\": message,\n\"tools\": tools\n}),\n\"modelId\":\"mistral.mistral-large-2407-v1:0\",\n}\nresponse = bedrock.invoke_model(**new_mistral_params)\nbody = response.get(\"body\").read().decode(\"utf-8\")\nMistral AI models 200\nAmazon Bedrock User Guide\nbody = json.loads(body)\nprint(body)\ninvoke_bedrock_mistral_tool()\nStability.ai Diffusion models\n", "retrieved_contexts": ["tools = [      {          \"type\": \"function\",          \"function\": {              \"name\": \"retrieve_payment_status\",              \"description\": \"Get payment status of a transaction\",              \"parameters\": {                  \"type\": \"object\",                  \"properties\": {                      \"transaction_id\": {                          \"type\": \"string\",                          \"description\": \"The transaction id.\",                      }                  },                  \"required\": [\"transaction_id\"],              },          },      },      {          \"type\": \"function\",          \"function\": {              \"name\": \"retrieve_payment_date\",              \"description\": \"Get payment date of a transaction\",              \"parameters\": {                  \"type\": \"object\",                  \"properties\": {                      \"transaction_id\": {                          \"type\": \"string\",                          \"description\": \"The transaction id.\",                      }                  },                  \"required\": [\"transaction_id\"],              },          },      } ]   names_to_functions = {    Mistral AI models 199        Amazon Bedrock User Guide       'retrieve_payment_status': functools.partial(retrieve_payment_status, df=df),      'retrieve_payment_date': functools.partial(retrieve_payment_date, df=df) }   test_tool_input = \"What's the status of my transaction T1001?\" message = [{\"role\": \"user\", \"content\": test_tool_input}]   def invoke_bedrock_mistral_tool():           mistral_params = {          \"body\": json.dumps({              \"messages\":", "data = {      'transaction_id': ['T1001', 'T1002', 'T1003', 'T1004', 'T1005'],      'customer_id': ['C001', 'C002', 'C003', 'C002', 'C001'],      'payment_amount': [125.50, 89.99, 120.00, 54.30, 210.20],      'payment_date': ['2021-10-05', '2021-10-06', '2021-10-07', '2021-10-05',   '2021-10-08'],      'payment_status': ['Paid', 'Unpaid', 'Paid', 'Paid', 'Pending'] }   # Create DataFrame df = pd.DataFrame(data)   def retrieve_payment_status(df: data, transaction_id: str) -> str:      if transaction_id in df.transaction_id.values:           return json.dumps({'status': df[df.transaction_id ==   transaction_id].payment_status.item()})      return json.dumps({'error': 'transaction id not found.'})", "Mistral AI models 198        Amazon Bedrock User Guide   def retrieve_payment_date(df: data, transaction_id: str) -> str:      if transaction_id in df.transaction_id.values:           return json.dumps({'date': df[df.transaction_id ==   transaction_id].payment_date.item()})      return json.dumps({'error': 'transaction id not found.'})"], "answer": "Based on the provided context, I don't have enough information to fully answer how the system would handle a scenario where a transaction status changes from 'Pending' to 'Paid', or what functions would be involved in updating this information. \n\nThe context shows functions for retrieving payment status and payment date, but it doesn't include any information about updating transaction statuses. Specifically:\n\n1. There's a `retrieve_payment_status` function that can get the current status of a transaction.\n2. There's a `retrieve_payment_date` function that can get the payment date of a transaction.\n\nHowever, there's no information provided about:\n- How status updates are handled\n- Any functions for modifying transaction data\n- How real-time updates would be processed or reflected in the system\n\nTo fully answer your question, we would need additional information about the system's capabilities for updating transaction data."}
{"question": "What is the purpose of an S3 retrieval node in a prompt flow?", "ground_truth": "An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. It takes an object key as input and returns the content of the S3 location as output.", "question_type": "simple", "contexts": "An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. In\nthe configuration, you specify the S3 bucket from which to retrieve data. The input into the node is\nthe object key. The node returns the content in the S3 location as the output.\nNote\nCurrently, the data in the S3 location must be a UTF-8 encoded string.\nThe following shows the general structure of an S3 retrieval FlowNode object:\n{\n\"name\": \"string\",\n\"type\": \"Retrieval\",\n\"inputs\": [\n{\n\"name\": \"objectKey\",\n\"type\": \"String\",\n\"expression\": \"string\"\n}\n],\n\"outputs\": [\n{\n\"name\": \"s3Content\",\n\"type\": \"String\"\n}\n],\n\"configuration\": {\n\"retrieval\": {\n\"serviceConfiguration\": {\n\"s3\": {\n\"bucketName\": \"string\"\n}\n}\n}\n}\n}\nNode types in prompt flow 846\nAmazon Bedrock User Guide\nLambda function node\nA Lambda function node lets you call a Lambda function in which you can define code to carry out\nbusiness logic. When you include a Lambda node in a prompt flow, Amazon Bedrock sends an input\n event to the Lambda function that you specify.\nIn the configuration, specify the Amazon Resource Name (ARN) of the Lambda function. Define\ninputs to send in the Lambda input event. You can write code based on these inputs and define\nwhat the function returns. The function response is returned in the output.\nThe following shows the general structure of a  function FlowNode object:\n{\n\"name\": \"string\",\n\"type\": \"LambdaFunction\",\n\"inputs\": [\n{\n\"name\": \"string\",\n\"type\": \"String | Number | Boolean | Object | Array\",\n\"expression\": \"string\"\n},\n...\n],\n\"outputs\": [\n{\n\"name\": \"functionResponse\",\n\"type\": \"String | Number | Boolean | Object | Array\"\n}\n],\n\"configuration\": {\n\"lambdaFunction\": {\n\"lambdaArn\": \"string\"\n}\n}\n}\nLambda input event for a prompt flow\nThe input event sent to a Lambda function in a Lambda node is of the following format:\n{\n\"messageVersion\": \"1.0\",\nNode types in prompt flow 847\nAmazon Bedrock User Guide\n\"flow\": {\n\"flowArn\": \"string\",\n\"flowAliasArn\": \"string\"\n},\n\"node\": {\n \"name\": \"string\",\n\"nodeInputs\": [\n{\n\"name\": \"string\",\n\"type\": \"String | Number | Boolean | Object | Array\",\n\"expression\": \"string\",\n\"value\": ...\n},\n...\n]\n}\n}\nThe fields for each input match the fields that you specify when defining the Lambda node, while\nthe value of the value field is populated with the whole input into the node after being resolved\nby the expression. For example, if the whole input into the node is [1, 2, 3] and the expression\nis $.data[1], the value sent in the input event to the Lambda function would be 2.\nFor more information about events in Lambda, see Lambda concepts in the AWS Lambda\nDeveloper Guide.\nLambda response for a prompt flow\nWhen you write a Lambda function, you define the response returned by it. This response is\nreturned to your prompt flow as the output of the Lambda node.\nLex node\nA Lex node lets you call a Amazon Lex bot to process an utterance using natural language\n", "original_contexts": "An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. In\nthe configuration, you specify the S3 bucket from which to retrieve data. The input into the node is\nthe object key. The node returns the content in the S3 location as the output.\nNote\nCurrently, the data in the S3 location must be a UTF-8 encoded string.\nThe following shows the general structure of an S3 retrieval FlowNode object:\n{\n\"name\": \"string\",\n\"type\": \"Retrieval\",\n\"inputs\": [\n{\n\"name\": \"objectKey\",\n\"type\": \"String\",\n\"expression\": \"string\"\n}\n],\n\"outputs\": [\n{\n\"name\": \"s3Content\",\n\"type\": \"String\"\n}\n],\n\"configuration\": {\n\"retrieval\": {\n\"serviceConfiguration\": {\n\"s3\": {\n\"bucketName\": \"string\"\n}\n}\n}\n}\n}\nNode types in prompt flow 846\nAmazon Bedrock User Guide\nLambda function node\nA Lambda function node lets you call a Lambda function in which you can define code to carry out\nbusiness logic. When you include a Lambda node in a prompt flow, Amazon Bedrock sends an input\n event to the Lambda function that you specify.\nIn the configuration, specify the Amazon Resource Name (ARN) of the Lambda function. Define\ninputs to send in the Lambda input event. You can write code based on these inputs and define\nwhat the function returns. The function response is returned in the output.\nThe following shows the general structure of a  function FlowNode object:\n{\n\"name\": \"string\",\n\"type\": \"LambdaFunction\",\n\"inputs\": [\n{\n\"name\": \"string\",\n\"type\": \"String | Number | Boolean | Object | Array\",\n\"expression\": \"string\"\n},\n...\n],\n\"outputs\": [\n{\n\"name\": \"functionResponse\",\n\"type\": \"String | Number | Boolean | Object | Array\"\n}\n],\n\"configuration\": {\n\"lambdaFunction\": {\n\"lambdaArn\": \"string\"\n}\n}\n}\nLambda input event for a prompt flow\nThe input event sent to a Lambda function in a Lambda node is of the following format:\n{\n\"messageVersion\": \"1.0\",\nNode types in prompt flow 847\nAmazon Bedrock User Guide\n\"flow\": {\n\"flowArn\": \"string\",\n\"flowAliasArn\": \"string\"\n},\n\"node\": {\n \"name\": \"string\",\n\"nodeInputs\": [\n{\n\"name\": \"string\",\n\"type\": \"String | Number | Boolean | Object | Array\",\n\"expression\": \"string\",\n\"value\": ...\n},\n...\n]\n}\n}\nThe fields for each input match the fields that you specify when defining the Lambda node, while\nthe value of the value field is populated with the whole input into the node after being resolved\nby the expression. For example, if the whole input into the node is [1, 2, 3] and the expression\nis $.data[1], the value sent in the input event to the Lambda function would be 2.\nFor more information about events in Lambda, see Lambda concepts in the AWS Lambda\nDeveloper Guide.\nLambda response for a prompt flow\nWhen you write a Lambda function, you define the response returned by it. This response is\nreturned to your prompt flow as the output of the Lambda node.\nLex node\nA Lex node lets you call a Amazon Lex bot to process an utterance using natural language\n", "retrieved_contexts": ["The following shows the general structure of an S3 retrieval FlowNode object:   {      \"name\": \"string\",      \"type\": \"Retrieval\",      \"inputs\": [          {              \"name\": \"objectKey\",              \"type\": \"String\",              \"expression\": \"string\"          }      ],      \"outputs\": [          {              \"name\": \"s3Content\",              \"type\": \"String\"          }      ],      \"configuration\": {          \"retrieval\": {              \"serviceConfiguration\": {                  \"s3\": {                      \"bucketName\": \"string\"                  }              }          }      } }   Node types in prompt flow 846    https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html  https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_FlowNode.html      Amazon Bedrock User Guide   Lambda function node   A Lambda function node lets you call a Lambda function in which you can define code to carry out  business logic. When you include a Lambda node in a prompt flow, Amazon Bedrock sends an input  event to the Lambda function that you specify.   In the configuration, specify the Amazon Resource Name (ARN) of the Lambda function. Define  inputs to send in the Lambda input event. You can write code based on these inputs and define  what the function returns. The function response is returned in the output.   The following shows the general", "In the configuration,  you specify the S3 bucket to use for data storage. The inputs into the node are the content to store  and the object key. The node returns the URI of the S3 location as its output.   The following shows the general structure of an S3 storage FlowNode object:   {      \"name\": \"string\",      \"type\": \"Storage\",      \"inputs\": [          {              \"name\": \"content\",              \"type\": \"String | Number | Boolean | Object | Array\",              \"expression\": \"string\"          },          {              \"name\": \"objectKey\",              \"type\": \"String\",              \"expression\": \"string\"          }      ],      \"outputs\": [          {              \"name\": \"s3Uri\",              \"type\": \"String\"          }      ],      \"configuration\": {          \"retrieval\": {              \"serviceConfiguration\": {                  \"s3\": {                      \"bucketName\": \"string\"                  }              }          }      } }   Node types in prompt flow 845    https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html  https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_FlowNode.html      Amazon Bedrock User Guide   S3 retrieval node   An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow.", "In  the configuration, you specify the S3 bucket from which to retrieve data. The input into the node is  the object key. The node returns the content in the S3 location as the output.   Note   Currently, the data in the S3 location must be a UTF-8 encoded string.   The following shows the general structure of an S3 retrieval FlowNode object:   {      \"name\": \"string\",      \"type\": \"Retrieval\",      \"inputs\": [          {              \"name\": \"objectKey\",              \"type\": \"String\",              \"expression\": \"string\"          }      ],      \"outputs\": [          {              \"name\": \"s3Content\",              \"type\": \"String\"          }      ],      \"configuration\": {          \"retrieval\": {              \"serviceConfiguration\": {                  \"s3\": {                      \"bucketName\": \"string\"                  }              }          }      } }   Node types in prompt flow 846    https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html  https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_FlowNode.html      Amazon Bedrock User Guide   Lambda function node   A Lambda function node lets you call a Lambda function in which you can define code to carry out  business logic."], "answer": "Based on the provided context, the purpose of an S3 retrieval node in a prompt flow is to retrieve data from an Amazon S3 location and introduce it to the flow. Specifically:\n\n1. It allows you to fetch data stored in an Amazon S3 bucket.\n2. The node takes an object key as input.\n3. It returns the content stored at the specified S3 location as its output.\n4. The retrieved data must be a UTF-8 encoded string.\n\nThe S3 retrieval node is configured with the S3 bucket name from which to retrieve the data, and it provides a way to incorporate external data stored in S3 into the prompt flow process."}
{"question": "How can a developer create a new prompt version, retrieve its information, and incorporate it into a prompt flow using Amazon Bedrock's Python SDK?", "ground_truth": "To create a new prompt version, retrieve its information, and incorporate it into a prompt flow using Amazon Bedrock's Python SDK, a developer should follow these steps:\n\n1. Create a new prompt version using the create_prompt_version() method, specifying the prompt_id.\n2. Retrieve the version number and ARN from the response.\n3. List all versions of the prompt using list_prompts() with the prompt_id.\n4. Get detailed information about the specific version using get_prompt() with prompt_id and prompt_version.\n5. To incorporate the prompt into a flow:\n   a. Create a boto3 client for 'bedrock-agent'.\n   b. Define the flow's structure using Python dictionaries for input, prompt, and output nodes.\n   c. In the prompt node configuration, set the promptArn to the ARN of the created prompt version.\n   d. Ensure the prompt node's inputs match the expected variables (e.g., \"genre\" and \"number\").\n   e. Set up the output node to return the model's completion as a string.\n\nThis process allows for version control of prompts and their seamless integration into prompt flows.", "question_type": "complex", "contexts": "make a CreatePromptVersion Agents for Amazon Bedrock build-time endpoint:\n# Create a version of the prompt that you created\nresponse = client.create_prompt_version(promptIdentifier=prompt_id)\nprompt_version = response.get(\"version\")\nprompt_version_arn = response.get(\"arn\")\n5. View information about the prompt version that you just created, alongside information\nabout the draft version, by running the following code snippet to make a ListPrompts\nAgents for Amazon Bedrock build-time endpoint:\n# List versions of the prompt that you just created\nclient.list_prompts(promptIdentifier=prompt_id)\n6. View information for the prompt version that you just created by running the following\ncode snippet to make a GetPrompt Agents for Amazon Bedrock build-time endpoint:\n# Get information about the prompt version that you created\nclient.get_prompt(\npromptIdentifier=prompt_id,\npromptVersion=prompt_version\n)\n7. Test the prompt by adding it to a prompt flow by following the steps at Run Prompt flows\n code samples. In the first step when you create the flow, run the following code snippet\ninstead to use the prompt that you created instead of defining an inline prompt in the flow\n(replace the ARN of the prompt version in the promptARN field with the ARN of the version\nof the prompt that you created):\n# Import Python SDK and create client\nimport boto3\nclient = boto3.client(service_name=\"bedrock-agent\")\nFLOWS_SERVICE_ROLE = \"arn:aws:iam::123456789012:role/MyPromptFlowsRole\" #\nPrompt flows service role that you created. For more information, see https://\ndocs.aws.amazon.com/bedrock/latest/userguide/flows-permissions.html\nPROMPT_ARN = prompt_version_arn # ARN of the prompt that you created, retrieved\nprogramatically during creation.\nRun code samples 351\nAmazon Bedrock User Guide\n# Define each node\n# The input node validates that the content of the InvokeFlow request is a JSON\nobject.\ninput_node = {\n\"type\": \"Input\",\n\"name\": \"FlowInput\",\n\"outputs\": [\n{\n\"name\": \"document\",\n\"type\": \"Object\"\n }\n]\n}\n# This prompt node contains a prompt that you defined in Prompt management.\n# It validates that the input is a JSON object that minimally contains the\nfields \"genre\" and \"number\", which it will map to the prompt variables.\n# The output must be named \"modelCompletion\" and be of the type \"String\".\nprompt_node = {\n\"type\": \"Prompt\",\n\"name\": \"MakePlaylist\",\n\"configuration\": {\n\"prompt\": {\n\"sourceConfiguration\": {\n\"resource\": {\n\"promptArn\": \"\"\n}\n}\n}\n},\n\"inputs\": [\n{\n\"name\": \"genre\",\n\"type\": \"String\",\n\"expression\": \"$.data.genre\"\n},\n{\n\"name\": \"number\",\n\"type\": \"Number\",\n\"expression\": \"$.data.number\"\n}\n],\nRun code samples 352\nAmazon Bedrock User Guide\n\"outputs\": [\n{\n\"name\": \"modelCompletion\",\n\"type\": \"String\"\n}\n]\n}\n# The output node validates that the output from the last node is a string and\nreturns it as is. The name must be \"document\".\noutput_node = {\n\"type\": \"Output\",\n\"name\": \"FlowOutput\",\n\"inputs\": [\n{\n\"name\": \"document\",\n\"type\": \"String\",\n\"expression\": \"$.data\"\n}\n]\n}\n", "original_contexts": "make a CreatePromptVersion Agents for Amazon Bedrock build-time endpoint:\n# Create a version of the prompt that you created\nresponse = client.create_prompt_version(promptIdentifier=prompt_id)\nprompt_version = response.get(\"version\")\nprompt_version_arn = response.get(\"arn\")\n5. View information about the prompt version that you just created, alongside information\nabout the draft version, by running the following code snippet to make a ListPrompts\nAgents for Amazon Bedrock build-time endpoint:\n# List versions of the prompt that you just created\nclient.list_prompts(promptIdentifier=prompt_id)\n6. View information for the prompt version that you just created by running the following\ncode snippet to make a GetPrompt Agents for Amazon Bedrock build-time endpoint:\n# Get information about the prompt version that you created\nclient.get_prompt(\npromptIdentifier=prompt_id,\npromptVersion=prompt_version\n)\n7. Test the prompt by adding it to a prompt flow by following the steps at Run Prompt flows\n code samples. In the first step when you create the flow, run the following code snippet\ninstead to use the prompt that you created instead of defining an inline prompt in the flow\n(replace the ARN of the prompt version in the promptARN field with the ARN of the version\nof the prompt that you created):\n# Import Python SDK and create client\nimport boto3\nclient = boto3.client(service_name=\"bedrock-agent\")\nFLOWS_SERVICE_ROLE = \"arn:aws:iam::123456789012:role/MyPromptFlowsRole\" #\nPrompt flows service role that you created. For more information, see https://\ndocs.aws.amazon.com/bedrock/latest/userguide/flows-permissions.html\nPROMPT_ARN = prompt_version_arn # ARN of the prompt that you created, retrieved\nprogramatically during creation.\nRun code samples 351\nAmazon Bedrock User Guide\n# Define each node\n# The input node validates that the content of the InvokeFlow request is a JSON\nobject.\ninput_node = {\n\"type\": \"Input\",\n\"name\": \"FlowInput\",\n\"outputs\": [\n{\n\"name\": \"document\",\n\"type\": \"Object\"\n }\n]\n}\n# This prompt node contains a prompt that you defined in Prompt management.\n# It validates that the input is a JSON object that minimally contains the\nfields \"genre\" and \"number\", which it will map to the prompt variables.\n# The output must be named \"modelCompletion\" and be of the type \"String\".\nprompt_node = {\n\"type\": \"Prompt\",\n\"name\": \"MakePlaylist\",\n\"configuration\": {\n\"prompt\": {\n\"sourceConfiguration\": {\n\"resource\": {\n\"promptArn\": \"\"\n}\n}\n}\n},\n\"inputs\": [\n{\n\"name\": \"genre\",\n\"type\": \"String\",\n\"expression\": \"$.data.genre\"\n},\n{\n\"name\": \"number\",\n\"type\": \"Number\",\n\"expression\": \"$.data.number\"\n}\n],\nRun code samples 352\nAmazon Bedrock User Guide\n\"outputs\": [\n{\n\"name\": \"modelCompletion\",\n\"type\": \"String\"\n}\n]\n}\n# The output node validates that the output from the last node is a string and\nreturns it as is. The name must be \"document\".\noutput_node = {\n\"type\": \"Output\",\n\"name\": \"FlowOutput\",\n\"inputs\": [\n{\n\"name\": \"document\",\n\"type\": \"String\",\n\"expression\": \"$.data\"\n}\n]\n}\n", "retrieved_contexts": ["Otherwise, do the following:   To create a version of your prompt   1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock  permissions, and open the Amazon Bedrock console at Getting Started with the AWS  Management Console.   2. Select Prompt management from the left navigation pane. Then, choose a prompt in the Prompts section.   3. In the Prompt versions section, choose Create version to take a snapshot of your draft  version.   API   To create a version of your prompt, send a CreatePromptVersion request (see link for request  and response formats and field details) with an Agents for Amazon Bedrock build-time  endpoint and specify the ARN or ID of the prompt as the promptIdentifier.   The response returns an ID and ARN for the version. Versions are created incrementally, starting  from 1.", "View information about the prompt version that you just created, alongside information  about the draft version, by running the following code snippet to make a ListPrompts Agents for Amazon Bedrock build-time endpoint:   # List versions of the prompt that you just created client.list_prompts(promptIdentifier=prompt_id)   6. View information for the prompt version that you just created by running the following  code snippet to make a GetPrompt Agents for Amazon Bedrock build-time endpoint:   # Get information about the prompt version that you created client.get_prompt(      promptIdentifier=prompt_id,       promptVersion=prompt_version )   7. Test the prompt by adding it to a prompt flow by following the steps at Run Prompt flows  code samples. In the first step when you create the flow, run the following code snippet  instead to use the prompt that you created instead of defining an inline prompt in the flow  (replace the ARN of the prompt version in the promptARN field with the ARN of the version  of the prompt that you created):   # Import Python SDK and create client import boto3   client = boto3.client(service_name='bedrock-agent')   FLOWS_SERVICE_ROLE = \"arn:aws:iam::123456789012:role/MyPromptFlowsRole\" #   Prompt flows service role that you created.", "â€¢ Prompt draft â€“ Contains the prompt message and configurations for the latest saved  draft version of the prompt.   â€¢ Prompt versions â€“ A list of all versions of the prompt that have been created. For more  information about prompt versions, see Deploy prompts using Prompt management by  creating versions.   API   To get information about a prompt, send a GetPrompt request (see link for request and  response formats and field details) with an Agents for Amazon Bedrock build-time endpoint and specify the ARN or ID of the prompt as the promptIdentifier. To get information about  a specific version of the prompt, specify DRAFT or the version number in the promptVersion field.   To list information about your agents, send a ListPrompts request (see link for request and  response formats and field details) with an Agents for Amazon Bedrock build-time endpoint."], "answer": "Based on the provided context, here's how a developer can create a new prompt version, retrieve its information, and incorporate it into a prompt flow using Amazon Bedrock's Python SDK:\n\n1. Create a new prompt version:\n   - Use the CreatePromptVersion API request with an Agents for Amazon Bedrock build-time endpoint.\n   - Specify the ARN or ID of the prompt as the promptIdentifier.\n   - The response will return an ID and ARN for the new version.\n\n2. Retrieve information about the new prompt version:\n   - Use the ListPrompts API to view information about all versions:\n     ```python\n     client.list_prompts(promptIdentifier=prompt_id)\n     ```\n   - Use the GetPrompt API to view information about the specific version:\n     ```python\n     client.get_prompt(\n         promptIdentifier=prompt_id,\n         promptVersion=prompt_version\n     )\n     ```\n\n3. Incorporate the prompt into a prompt flow:\n   - Import the boto3 library and create a client:\n     ```python\n     import boto3\n     client = boto3.client(service_name='bedrock-agent')\n     ```\n   - When creating the flow, use the ARN of the prompt version you created:\n     ```python\n     FLOWS_SERVICE_ROLE = \"arn:aws:iam::123456789012:role/MyPromptFlowsRole\"\n     # Use the ARN of your created prompt version in the flow creation\n     ```\n\n   - Follow the remaining steps in the \"Run Prompt flows code samples\" guide to complete the process of incorporating the prompt into a flow.\n\nThis process allows developers to create, manage, and utilize versioned prompts within Amazon Bedrock's environment using the Python SDK."}
{"question": "What metric is used to evaluate toxicity in general text generation tasks?", "ground_truth": "Toxicity is used as a metric to evaluate toxicity in general text generation tasks, using the RealToxicPrompts dataset.", "question_type": "simple", "contexts": "2. In the navigation pane, choose Model evaluation.\n3. In the Model Evaluation Jobs card, you can find a table that lists the model evaluation jobs\nyou have already created.\n4. Select the radio button next to your job\"s name.\n5. Then, choose Stop evaluation.\nAWS CLI\nIn the AWS CLI, you can use the help command to view parameters are required, and which\nparameters are optional when using list-evaluation-jobs.\nList model evaluation jobs 457\nAmazon Bedrock User Guide\naws bedrock list-evaluation-jobs help\nThe follow is an example of using list-evaluation-jobs and specifying that maximum of\n5 jobs be returned. By default jobs are returned in descending order from the time when they\nwhere started.\naws bedrock list-evaluation-jobs --max-items 5\nSDK for Python\nThe following examples show how to use the AWS SDK for Python to find a model evaluation\njob you have previously created.\nimport boto3\nclient = boto3.client(\"bedrock\")\njob_request = client.list_evaluation_jobs(maxResults=20)\n print (job_request)\nModel evaluation tasks\nIn a model evaluation job, an evaluation task ( taskType ) is a task you want the model\nto perform based on information in your prompts. You can choose one task type per model\nevaluation job.\nThe following topics to learn more about each task type. Each topic also includes a list of available\nbuilt-in datasets and their corresponding metrics that can be used only in automatic model\nevaluation jobs.\nThe following table summarizes available tasks types, built-in datasets, and computer metrics for\neach task type.\nModel evaluation tasks 458\nAmazon Bedrock User Guide\nAvailable built-in datasets for automatic model evaluation jobs in Amazon Bedrock\nTask type MetricBuilt-in Computed metric\ndatasets\nGeneral text AccuraTcRyEX Real world knowledge (RWK) score\ngeneration\nRobusBtnOeLsD Word error rate\ns\nTREX\nWikiText2\nToxicitRyealToxic Toxicity\nityPrompts\nBOLD\nText AccuraGciygaword BERTScore\nsummarization\nToxicitGyigaword Toxicity\n RobusGtnigeas word BERTScore and deltaBERTScore\ns\nQuestion and AccuraBcoyolQ NLP-F1\nanswer\nNaturalQu\nestions\nTriviaQA\nRobusBtnoeosl Q F1 and deltaF1\ns\nNaturalQu\nestions\nTriviaQA\nToxicitByoolQ Toxicity\nModel evaluation tasks 459\nAmazon Bedrock User Guide\nTask type MetricBuilt-in Computed metric\ndatasets\nNaturalQu\nestions\nTriviaQA\nText classific AccuraWcyomen\"s Accuracy (Binary accuracy from classification_accuracy_s\nation Ecommerce core)\nClothing\nReviews\nRobusWtnoems en\"s classification_accuracy_score and delta_classifica\ns Ecommerce tion_accuracy_score\nClothing\nReviews\nTopics\n General text generation\n Text summarization\n Question and answer\n Text classification\nGeneral text generation\nImportant\nFor general text generation, there is a known system issue that prevents Cohere models\nfrom completing the toxicity evaluation successfully.\nGeneral text generation is a task used by applications that include chatbots. The responses\n", "original_contexts": "2. In the navigation pane, choose Model evaluation.\n3. In the Model Evaluation Jobs card, you can find a table that lists the model evaluation jobs\nyou have already created.\n4. Select the radio button next to your job\"s name.\n5. Then, choose Stop evaluation.\nAWS CLI\nIn the AWS CLI, you can use the help command to view parameters are required, and which\nparameters are optional when using list-evaluation-jobs.\nList model evaluation jobs 457\nAmazon Bedrock User Guide\naws bedrock list-evaluation-jobs help\nThe follow is an example of using list-evaluation-jobs and specifying that maximum of\n5 jobs be returned. By default jobs are returned in descending order from the time when they\nwhere started.\naws bedrock list-evaluation-jobs --max-items 5\nSDK for Python\nThe following examples show how to use the AWS SDK for Python to find a model evaluation\njob you have previously created.\nimport boto3\nclient = boto3.client(\"bedrock\")\njob_request = client.list_evaluation_jobs(maxResults=20)\n print (job_request)\nModel evaluation tasks\nIn a model evaluation job, an evaluation task ( taskType ) is a task you want the model\nto perform based on information in your prompts. You can choose one task type per model\nevaluation job.\nThe following topics to learn more about each task type. Each topic also includes a list of available\nbuilt-in datasets and their corresponding metrics that can be used only in automatic model\nevaluation jobs.\nThe following table summarizes available tasks types, built-in datasets, and computer metrics for\neach task type.\nModel evaluation tasks 458\nAmazon Bedrock User Guide\nAvailable built-in datasets for automatic model evaluation jobs in Amazon Bedrock\nTask type MetricBuilt-in Computed metric\ndatasets\nGeneral text AccuraTcRyEX Real world knowledge (RWK) score\ngeneration\nRobusBtnOeLsD Word error rate\ns\nTREX\nWikiText2\nToxicitRyealToxic Toxicity\nityPrompts\nBOLD\nText AccuraGciygaword BERTScore\nsummarization\nToxicitGyigaword Toxicity\n RobusGtnigeas word BERTScore and deltaBERTScore\ns\nQuestion and AccuraBcoyolQ NLP-F1\nanswer\nNaturalQu\nestions\nTriviaQA\nRobusBtnoeosl Q F1 and deltaF1\ns\nNaturalQu\nestions\nTriviaQA\nToxicitByoolQ Toxicity\nModel evaluation tasks 459\nAmazon Bedrock User Guide\nTask type MetricBuilt-in Computed metric\ndatasets\nNaturalQu\nestions\nTriviaQA\nText classific AccuraWcyomen\"s Accuracy (Binary accuracy from classification_accuracy_s\nation Ecommerce core)\nClothing\nReviews\nRobusWtnoems en\"s classification_accuracy_score and delta_classifica\ns Ecommerce tion_accuracy_score\nClothing\nReviews\nTopics\n General text generation\n Text summarization\n Question and answer\n Text classification\nGeneral text generation\nImportant\nFor general text generation, there is a known system issue that prevents Cohere models\nfrom completing the toxicity evaluation successfully.\nGeneral text generation is a task used by applications that include chatbots. The responses\n", "retrieved_contexts": ["Each prompt in your dataset is perturbed approximately 5 times. Then, each  perturbed response is sent for inference, and used to calculate robustness scores automatically.   â€¢ Toxicity: For this metric, the value is calculated using toxicity from the detoxify algorithm. A low  toxicity value indicates that your selected model is not producing large amounts of toxic content.  To learn more about the detoxify algorithm and see how toxicity is calculated, see the detoxify  algorithm on GitHub.   How each available metric is calculated when applied to the text summarization task type   â€¢ Accuracy: For this metric, the value is calculated using BERT Score. BERT Score is calculated  using pre-trained contextual embeddings from BERT models. It matches words in candidate and  reference sentences by cosine similarity.   â€¢ Robustness: For this metric, the value calculated is a percentage. It calculated by taking (Delta  BERTScore / BERTScore) x 100. Delta BERTScore is the difference in BERT Scores between a  perturbed prompt and the original prompt in your dataset. Each prompt in your dataset is  perturbed approximately 5 times. Then, each perturbed response is sent for inference, and used    Automated reports 481    https://github.com/unitaryai/detoxify  https://github.com/unitaryai/detoxify      Amazon Bedrock User Guide   to calculate robustness scores automatically.", "When you create an automatic model evaluation job and choose a Task type Amazon Bedrock  provides you with a list of recommended metrics. For each metric, Amazon Bedrock also provides  recommended built-in datasets. To learn more about available task types, see Model evaluation  tasks.   Bias in Open-ended Language Generation Dataset (BOLD)   The Bias in Open-ended Language Generation Dataset (BOLD) is a dataset that evaluates  fairness in general text generation, focusing on five domains: profession, gender, race, religious  ideologies, and political ideologies. It contains 23,679 different text generation prompts.   RealToxicityPrompts   RealToxicityPrompts is a dataset that evaluates toxicity. It attempts to get the model to  generate racist, sexist, or otherwise toxic language. This dataset contains 100,000 different text  generation prompts.   T-Rex : A Large Scale Alignment of Natural Language with Knowledge Base Triples (TREX)   TREX is dataset consisting of Knowledge Base Triples (KBTs) extracted from Wikipedia.  KBTs are a type of data structure used in natural language processing (NLP) and knowledge  representation. They consist of a subject, predicate, and object, where the subject and object  are linked by a relation. An example of a Knowledge Base Triple (KBT) is \"George Washington  was the president of the United States\".", "Available built-in datasets for general text generation in Amazon Bedrock   Task type Metric Built-in datasets  (Console)   Built-in datasets  (API)   Computed  metric   General text  generation   Accuracy TREX Builtin.T-REx Real world  knowledge  (RWK) score   General text generation 461    https://hadyelsahar.github.io/t-rex/      Amazon Bedrock User Guide   Task type Metric Built-in datasets  (Console)   Built-in datasets  (API)   Computed  metric   BOLD Builtin.BOLD   WikiText2 Builtin.W  ikiText2   Robustnes  s   TREX Builtin.T-REx   Word error  rate   RealToxicityPrompts Builtin.R  ealToxici  tyPrompts   Toxicity   BOLD Builtin.Bold   Toxicity   To learn more about how the computed metric for each built-in dataset is calculated, see Model  evaluation job results   Text summarization   Important   For text summarization, there is a known system issue that prevents Cohere models from  completing the toxicity evaluation successfully.   Text summarization is used for tasks including creating summaries of news, legal documents,  academic papers, content previews, and content curation."], "answer": "According to the context provided, the metric used to evaluate toxicity in general text generation tasks is Toxicity. The context states:\n\n\"Toxicity: For this metric, the value is calculated using toxicity from the detoxify algorithm. A low toxicity value indicates that your selected model is not producing large amounts of toxic content.\"\n\nThe context also mentions that the RealToxicityPrompts dataset is used to evaluate toxicity, attempting to get the model to generate racist, sexist, or otherwise toxic language."}
{"question": "How does the lambda_handler function handle different types of responses from the LLM, and what are the potential outcomes of its parsing process?", "ground_truth": "The lambda_handler function in the given code snippet handles different types of responses from the LLM through a series of parsing steps and error checks:\n\n1. It first sanitizes the LLM response and extracts any rationale.\n\n2. Then, it attempts to parse for a final answer. If successful, it constructs a response with 'invocationType': 'FINISH' and includes the final answer and any generated response parts (citations).\n\n3. If no final answer is found, it tries to parse for an \"ask user\" response. If found, it constructs a response with 'invocationType': 'ASK_USER' and includes the question to ask the user.\n\n4. If neither a final answer nor an \"ask user\" response is found, it attempts to parse for an agent action (though this part is not fully shown in the snippet).\n\n5. Throughout the process, if any parsing errors occur (e.g., incorrect function call format), it adds a reprompt response to guide the LLM to provide the correct format.\n\nThe potential outcomes of this parsing process are:\na) A successful 'FINISH' response with a final answer and possible citations\nb) A successful 'ASK_USER' response with a question to be asked to the user\nc) A reprompt response if there are formatting errors in the LLM output\nd) Potentially, an agent action response (not fully shown in the snippet)\n\nThis structured approach allows the function to handle various LLM output scenarios and provide appropriate responses or error handling for each case.", "question_type": "complex", "contexts": "ANSWER_PART_REGEX = \"<answer_part\\\\s?>(.+?)</answer_part\\\\s?>\"\nANSWER_TEXT_PART_REGEX = \"<text\\\\s?>(.+?)</text\\\\s?>\"\nANSWER_REFERENCE_PART_REGEX = \"<source\\\\s?>(.+?)</source\\\\s?>\"\nANSWER_PART_PATTERN = re.compile(ANSWER_PART_REGEX, re.DOTALL)\nANSWER_TEXT_PART_PATTERN = re.compile(ANSWER_TEXT_PART_REGEX, re.DOTALL)\nANSWER_REFERENCE_PART_PATTERN = re.compile(ANSWER_REFERENCE_PART_REGEX, re.DOTALL)\n# You can provide messages to reprompt the LLM in case the LLM output is not in\nthe expected format\nMISSING_API_INPUT_FOR_USER_REPROMPT_MESSAGE = \"Missing the argument askuser for\nuser::askuser function call. Please try again with the correct argument added\"\nASK_USER_FUNCTION_CALL_STRUCTURE_REMPROMPT_MESSAGE = \"The function call format\nis incorrect. The format for function calls to the askuser function must be:\n<function_call>user::askuser(askuser=\\\"$ASK_USER_INPUT\\\")</function_call>.\"\nFUNCTION_CALL_STRUCTURE_REPROMPT_MESSAGE = \"The function call format\n is incorrect. The format for function calls must be: <function_call>\n$FUNCTION_NAME($FUNCTION_ARGUMENT_NAME=\"\"$FUNCTION_ARGUMENT_NAME\"\")</\nfunction_call>.\"\nlogger = logging.getLogger()\nAdvanced prompts 767\nAmazon Bedrock User Guide\n# This parser lambda is an example of how to parse the LLM output for the default\norchestration prompt\ndef lambda_handler(event, context):\nlogger.info(\"Lambda input: \" + str(event))\n# Sanitize LLM response\nsanitized_response = sanitize_response(event[\"invokeModelRawResponse\"])\n# Parse LLM response for any rationale\nrationale = parse_rationale(sanitized_response)\n# Construct response fields common to all invocation types\nparsed_response = {\n\"promptType\": \"ORCHESTRATION\",\n\"orchestrationParsedResponse\": {\n\"rationale\": rationale\n}\n}\n# Check if there is a final answer\ntry:\nfinal_answer, generated_response_parts = parse_answer(sanitized_response)\nexcept ValueError as e:\naddRepromptResponse(parsed_response, e)\nreturn parsed_response\nif final_answer:\n parsed_response[\"orchestrationParsedResponse\"][\"responseDetails\"] = {\n\"invocationType\": \"FINISH\",\n\"agentFinalResponse\": {\n\"responseText\": final_answer\n}\n}\nif generated_response_parts:\nparsed_response[\"orchestrationParsedResponse\"][\"responseDetails\"]\n[\"agentFinalResponse\"][\"citations\"] = {\n\"generatedResponseParts\": generated_response_parts\n}\nlogger.info(\"Final answer parsed response: \" + str(parsed_response))\nreturn parsed_response\nAdvanced prompts 768\nAmazon Bedrock User Guide\n# Check if there is an ask user\ntry:\nask_user = parse_ask_user(sanitized_response)\nif ask_user:\nparsed_response[\"orchestrationParsedResponse\"][\"responseDetails\"] = {\n\"invocationType\": \"ASK_USER\",\n\"agentAskUser\": {\n\"responseText\": ask_user\n}\n}\nlogger.info(\"Ask user parsed response: \" + str(parsed_response))\nreturn parsed_response\nexcept ValueError as e:\naddRepromptResponse(parsed_response, e)\nreturn parsed_response\n# Check if there is an agent action\ntry:\n", "original_contexts": "ANSWER_PART_REGEX = \"<answer_part\\\\s?>(.+?)</answer_part\\\\s?>\"\nANSWER_TEXT_PART_REGEX = \"<text\\\\s?>(.+?)</text\\\\s?>\"\nANSWER_REFERENCE_PART_REGEX = \"<source\\\\s?>(.+?)</source\\\\s?>\"\nANSWER_PART_PATTERN = re.compile(ANSWER_PART_REGEX, re.DOTALL)\nANSWER_TEXT_PART_PATTERN = re.compile(ANSWER_TEXT_PART_REGEX, re.DOTALL)\nANSWER_REFERENCE_PART_PATTERN = re.compile(ANSWER_REFERENCE_PART_REGEX, re.DOTALL)\n# You can provide messages to reprompt the LLM in case the LLM output is not in\nthe expected format\nMISSING_API_INPUT_FOR_USER_REPROMPT_MESSAGE = \"Missing the argument askuser for\nuser::askuser function call. Please try again with the correct argument added\"\nASK_USER_FUNCTION_CALL_STRUCTURE_REMPROMPT_MESSAGE = \"The function call format\nis incorrect. The format for function calls to the askuser function must be:\n<function_call>user::askuser(askuser=\\\"$ASK_USER_INPUT\\\")</function_call>.\"\nFUNCTION_CALL_STRUCTURE_REPROMPT_MESSAGE = \"The function call format\n is incorrect. The format for function calls must be: <function_call>\n$FUNCTION_NAME($FUNCTION_ARGUMENT_NAME=\"\"$FUNCTION_ARGUMENT_NAME\"\")</\nfunction_call>.\"\nlogger = logging.getLogger()\nAdvanced prompts 767\nAmazon Bedrock User Guide\n# This parser lambda is an example of how to parse the LLM output for the default\norchestration prompt\ndef lambda_handler(event, context):\nlogger.info(\"Lambda input: \" + str(event))\n# Sanitize LLM response\nsanitized_response = sanitize_response(event[\"invokeModelRawResponse\"])\n# Parse LLM response for any rationale\nrationale = parse_rationale(sanitized_response)\n# Construct response fields common to all invocation types\nparsed_response = {\n\"promptType\": \"ORCHESTRATION\",\n\"orchestrationParsedResponse\": {\n\"rationale\": rationale\n}\n}\n# Check if there is a final answer\ntry:\nfinal_answer, generated_response_parts = parse_answer(sanitized_response)\nexcept ValueError as e:\naddRepromptResponse(parsed_response, e)\nreturn parsed_response\nif final_answer:\n parsed_response[\"orchestrationParsedResponse\"][\"responseDetails\"] = {\n\"invocationType\": \"FINISH\",\n\"agentFinalResponse\": {\n\"responseText\": final_answer\n}\n}\nif generated_response_parts:\nparsed_response[\"orchestrationParsedResponse\"][\"responseDetails\"]\n[\"agentFinalResponse\"][\"citations\"] = {\n\"generatedResponseParts\": generated_response_parts\n}\nlogger.info(\"Final answer parsed response: \" + str(parsed_response))\nreturn parsed_response\nAdvanced prompts 768\nAmazon Bedrock User Guide\n# Check if there is an ask user\ntry:\nask_user = parse_ask_user(sanitized_response)\nif ask_user:\nparsed_response[\"orchestrationParsedResponse\"][\"responseDetails\"] = {\n\"invocationType\": \"ASK_USER\",\n\"agentAskUser\": {\n\"responseText\": ask_user\n}\n}\nlogger.info(\"Ask user parsed response: \" + str(parsed_response))\nreturn parsed_response\nexcept ValueError as e:\naddRepromptResponse(parsed_response, e)\nreturn parsed_response\n# Check if there is an agent action\ntry:\n", "retrieved_contexts": [": reference})      return references   Post-processing   The following example shows a post-processing parser Lambda function written in Python.   import json import re import logging     FINAL_RESPONSE_REGEX = r\"&lt;final_response>([\\s\\S]*?)&lt;/final_response>\" FINAL_RESPONSE_PATTERN = re.compile(FINAL_RESPONSE_REGEX, re.DOTALL)   logger = logging.getLogger()     # This parser lambda is an example of how to parse the LLM output for the default   PostProcessing prompt def lambda_handler(event, context):      logger.info(\"Lambda input: \" + str(event))      raw_response = event['invokeModelRawResponse']            parsed_response = {          'promptType': 'POST_PROCESSING',          'postProcessingParsedResponse': {}      }            matcher = FINAL_RESPONSE_PATTERN.search(raw_response)      if not matcher:          raise Exception(\"Could not parse raw LLM output\")      response_text = matcher.group(1).strip()            parsed_response['postProcessingParsedResponse']['responseText'] = response_text    Advanced prompts 810        Amazon Bedrock User Guide            logger.info(parsed_response)      return parsed_response   Control session context   For greater control of session context, you can modify the SessionState object in your agent.  The SessionState object contains information that can be maintained across turns (separate InvokeAgent request and responses). You can use this information to provide conversational  context for the agent during user conversations.", "import json import re import logging     FINAL_RESPONSE_REGEX = r\"&lt;final_response>([\\s\\S]*?)&lt;/final_response>\" FINAL_RESPONSE_PATTERN = re.compile(FINAL_RESPONSE_REGEX, re.DOTALL)   logger = logging.getLogger()     # This parser lambda is an example of how to parse the LLM output for the default   PostProcessing prompt def lambda_handler(event, context):      logger.info(\"Lambda input: \" + str(event))      raw_response = event['invokeModelRawResponse']            parsed_response = {          'promptType': 'POST_PROCESSING',          'postProcessingParsedResponse': {}      }            matcher = FINAL_RESPONSE_PATTERN.search(raw_response)      if not matcher:          raise Exception(\"Could not parse raw LLM output\")      response_text = matcher.group(1).strip()            parsed_response['postProcessingParsedResponse']['responseText'] = response_text    Advanced prompts 810        Amazon Bedrock User Guide            logger.info(parsed_response)      return parsed_response   Control session context   For greater control of session context, you can modify the SessionState object in your agent.  The SessionState object contains information that can be maintained across turns (separate InvokeAgent request and responses). You can use this information to provide conversational  context for the agent during user conversations.   The general format of the SessionState object is as follows.   {      \"sessionAttributes\": {          \"<attributeName1>\": \"<attributeValue1>\",", ">(.+?)&lt;/source\\\\s?>\" ANSWER_PART_PATTERN = re.compile(ANSWER_PART_REGEX, re.DOTALL) ANSWER_TEXT_PART_PATTERN = re.compile(ANSWER_TEXT_PART_REGEX, re.DOTALL) ANSWER_REFERENCE_PART_PATTERN = re.compile(ANSWER_REFERENCE_PART_REGEX, re.DOTALL)   logger = logging.getLogger()     # This parser lambda is an example of how to parse the LLM output for the default KB   response generation prompt def lambda_handler(event, context):      logger.info(\"Lambda input: \" + str(event))      raw_response = event['invokeModelRawResponse']            parsed_response = {          'promptType': 'KNOWLEDGE_BASE_RESPONSE_GENERATION',          'knowledgeBaseResponseGenerationParsedResponse': {              'generatedResponse': parse_generated_response(raw_response)          }      }            logger.info(parsed_response)      return parsed_response        def parse_generated_response(sanitized_llm_response):      results = []            for match in ANSWER_PART_PATTERN.finditer(sanitized_llm_response):          part = match.group(1).strip()                    text_match = ANSWER_TEXT_PART_PATTERN.search(part)          if not text_match:              raise ValueError(\"Could not parse generated response\")                    text = text_match.group(1).strip()                  references = parse_references(sanitized_llm_response, part)          results.append((text, references))            generated_response_parts = []      for text, references in"], "answer": "Based on the provided context, the lambda_handler function handles responses from the LLM in the following ways:\n\n1. It takes an event input containing an 'invokeModelRawResponse' field with the raw LLM output.\n\n2. It creates a parsed_response dictionary with a 'promptType' field set to either 'POST_PROCESSING' or 'KNOWLEDGE_BASE_RESPONSE_GENERATION', depending on the type of prompt being processed.\n\n3. For post-processing prompts:\n   - It uses a regular expression (FINAL_RESPONSE_PATTERN) to search for content between <final_response> tags.\n   - If a match is found, it extracts the response text and adds it to the parsed_response dictionary.\n   - If no match is found, it raises an exception.\n\n4. For knowledge base response generation prompts:\n   - It calls a parse_generated_response function to process the raw LLM output.\n   - This function searches for answer parts using regular expressions and extracts text and references.\n   - The parsed content is added to the 'knowledgeBaseResponseGenerationParsedResponse' field in the parsed_response dictionary.\n\nPotential outcomes of the parsing process:\n\n1. Successful parsing: The function returns a properly structured parsed_response dictionary containing the extracted information.\n\n2. Parsing failure: If the expected patterns are not found in the LLM output, the function may raise an exception (e.g., \"Could not parse raw LLM output\" or \"Could not parse generated response\").\n\n3. Partial parsing: Depending on the content of the LLM response, some parts may be successfully parsed while others might be missing or incomplete.\n\nThe function logs its input and output for debugging purposes, which can help identify any issues with the parsing process."}
{"question": "How can users download third-party audit reports for AWS services?", "ground_truth": "Users can download third-party audit reports using AWS Artifact.", "question_type": "simple", "contexts": " To learn the difference between using roles and resource-based policies for cross-account access,\nsee Cross account resource access in IAM in the IAM User Guide.\nCompliance validation for Amazon Bedrock\nTo learn whether an AWS service is within the scope of specific compliance programs, see AWS\nservices in Scope by Compliance Program and choose the compliance program that you are\ninterested in. For general information, see AWS Compliance Programs.\nCompliance validation 1155\nAmazon Bedrock User Guide\nYou can download third-party audit reports using AWS Artifact. For more information, see\nDownloading Reports in AWS Artifact.\nYour compliance responsibility when using AWS services is determined by the sensitivity of your\ndata, your company\"s compliance objectives, and applicable laws and regulations. AWS provides the\nfollowing resources to help with compliance:\n Security and Compliance Quick Start Guides  These deployment guides discuss architectural\n considerations and provide steps for deploying baseline environments on AWS that are security\nand compliance focused.\n Architecting for HIPAA Security and Compliance on Amazon Web Services  This whitepaper\ndescribes how companies can use AWS to create HIPAA-eligible applications.\nNote\nNot all AWS services are HIPAA eligible. For more information, see the HIPAA Eligible\nServices Reference.\n AWS Compliance Resources  This collection of workbooks and guides might apply to your\nindustry and location.\n AWS Customer Compliance Guides  Understand the shared responsibility model through the\nlens of compliance. The guides summarize the best practices for securing AWS services and map\nthe guidance to security controls across multiple frameworks (including National Institute of\nStandards and Technology (NIST), Payment Card Industry Security Standards Council (PCI), and\nInternational Organization for Standardization (ISO)).\n  Evaluating Resources with Rules in the AWS Config Developer Guide  The AWS Config service\nassesses how well your resource configurations comply with internal practices, industry\nguidelines, and regulations.\n AWS Security Hub  This AWS service provides a comprehensive view of your security state within\nAWS. Security Hub uses security controls to evaluate your AWS resources and to check your\ncompliance against security industry standards and best practices. For a list of supported services\nand controls, see Security Hub controls reference.\n Amazon GuardDuty  This AWS service detects potential threats to your AWS accounts,\nworkloads, containers, and data by monitoring your environment for suspicious and malicious\nactivities. GuardDuty can help you address various compliance requirements, like PCI DSS, by\nmeeting intrusion detection requirements mandated by certain compliance frameworks.\nCompliance validation 1156\nAmazon Bedrock User Guide\n", "original_contexts": " To learn the difference between using roles and resource-based policies for cross-account access,\nsee Cross account resource access in IAM in the IAM User Guide.\nCompliance validation for Amazon Bedrock\nTo learn whether an AWS service is within the scope of specific compliance programs, see AWS\nservices in Scope by Compliance Program and choose the compliance program that you are\ninterested in. For general information, see AWS Compliance Programs.\nCompliance validation 1155\nAmazon Bedrock User Guide\nYou can download third-party audit reports using AWS Artifact. For more information, see\nDownloading Reports in AWS Artifact.\nYour compliance responsibility when using AWS services is determined by the sensitivity of your\ndata, your company\"s compliance objectives, and applicable laws and regulations. AWS provides the\nfollowing resources to help with compliance:\n Security and Compliance Quick Start Guides  These deployment guides discuss architectural\n considerations and provide steps for deploying baseline environments on AWS that are security\nand compliance focused.\n Architecting for HIPAA Security and Compliance on Amazon Web Services  This whitepaper\ndescribes how companies can use AWS to create HIPAA-eligible applications.\nNote\nNot all AWS services are HIPAA eligible. For more information, see the HIPAA Eligible\nServices Reference.\n AWS Compliance Resources  This collection of workbooks and guides might apply to your\nindustry and location.\n AWS Customer Compliance Guides  Understand the shared responsibility model through the\nlens of compliance. The guides summarize the best practices for securing AWS services and map\nthe guidance to security controls across multiple frameworks (including National Institute of\nStandards and Technology (NIST), Payment Card Industry Security Standards Council (PCI), and\nInternational Organization for Standardization (ISO)).\n  Evaluating Resources with Rules in the AWS Config Developer Guide  The AWS Config service\nassesses how well your resource configurations comply with internal practices, industry\nguidelines, and regulations.\n AWS Security Hub  This AWS service provides a comprehensive view of your security state within\nAWS. Security Hub uses security controls to evaluate your AWS resources and to check your\ncompliance against security industry standards and best practices. For a list of supported services\nand controls, see Security Hub controls reference.\n Amazon GuardDuty  This AWS service detects potential threats to your AWS accounts,\nworkloads, containers, and data by monitoring your environment for suspicious and malicious\nactivities. GuardDuty can help you address various compliance requirements, like PCI DSS, by\nmeeting intrusion detection requirements mandated by certain compliance frameworks.\nCompliance validation 1156\nAmazon Bedrock User Guide\n", "retrieved_contexts": ["For more information, see Downloading Reports in AWS Artifact.   Your compliance responsibility when using AWS services is determined by the sensitivity of your  data, your company's compliance objectives, and applicable laws and regulations. AWS provides the  following resources to help with compliance:   â€¢ Security and Compliance Quick Start Guides â€“ These deployment guides discuss architectural  considerations and provide steps for deploying baseline environments on AWS that are security  and compliance focused.   â€¢ Architecting for HIPAA Security and Compliance on Amazon Web Services â€“ This whitepaper  describes how companies can use AWS to create HIPAA-eligible applications.   Note   Not all AWS services are HIPAA eligible. For more information, see the HIPAA Eligible  Services Reference.   â€¢ AWS Compliance Resources â€“ This collection of workbooks and guides might apply to your  industry and location.   â€¢ AWS Customer Compliance Guides â€“ Understand the shared responsibility model through the  lens of compliance. The guides summarize the best practices for securing AWS services and map  the guidance to security controls across multiple frameworks (including National Institute of  Standards and Technology (NIST), Payment Card Industry Security Standards Council (PCI), and  International Organization for Standardization (ISO)).   â€¢ Evaluating Resources with Rules in the AWS Config Developer Guide â€“ The AWS Config service  assesses how well your resource configurations comply with internal practices, industry  guidelines, and regulations.", "â€¢ To learn the difference between using roles and resource-based policies for cross-account access,  see Cross account resource access in IAM in the IAM User Guide.   Compliance validation for Amazon Bedrock   To learn whether an AWS service is within the scope of specific compliance programs, see AWS  services in Scope by Compliance Program and choose the compliance program that you are  interested in. For general information, see AWS Compliance Programs.   Compliance validation 1155    https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-cross-account-resource-access.html  https://aws.amazon.com/compliance/services-in-scope/  https://aws.amazon.com/compliance/services-in-scope/  https://aws.amazon.com/compliance/programs/      Amazon Bedrock User Guide   You can download third-party audit reports using AWS Artifact. For more information, see Downloading Reports in AWS Artifact.", "You can choose either CloudWatch Logs,  Amazon S3, or Amazon Data Firehose as the destination for storing logs. You must specify the  Amazon Resource Name of one of the destination options for where your logs will be stored. You  can choose the outputFormat of the logs to be one of the following: json, plain, w3c, raw, parquet. The following is an example of configuring logs to be stored in an Amazon S3 bucket  and in JSON format.   {     \"deliveryDestinationConfiguration\": {         \"destinationResourceArn\": \"arn:aws:s3:::bucket-name\"     },     \"name\": \"string\",     \"outputFormat\": \"json\",     \"tags\": {         \"key\" : \"value\"      } }   Note that if you are delivering logs cross-account, you must use the PutDeliveryDestinationPolicy API to assign an AWS Identity and Access Management  (IAM) policy to the destination account. The IAM policy allows delivery from one account to  another account.   4. Call CreateDelivery: Use the CreateDelivery API call to link the delivery source to the  destination that you created in the previous steps. This API operation associates the delivery  source with the end destination."], "answer": "According to the context provided, users can download third-party audit reports using AWS Artifact. Specifically, the context states: \"You can download third-party audit reports using AWS Artifact. For more information, see Downloading Reports in AWS Artifact.\""}
{"question": "How can an administrator troubleshoot a failed data source sync event and optimize future sync operations in Amazon Bedrock, considering both console and API methods?", "ground_truth": "To troubleshoot a failed data source sync event and optimize future operations in Amazon Bedrock, an administrator can follow these steps:\n\n1. Console method:\n   a. Navigate to the Amazon Bedrock console and select 'Knowledge bases' from the left navigation pane.\n   b. In the Data source section, select the relevant data source.\n   c. Check the Sync history for details about sync events.\n   d. For failed syncs, select the event and choose 'View warnings' to see specific reasons for failure.\n\n2. API method:\n   a. Use the GetIngestionJob API request to retrieve detailed information about a specific sync event. Provide the dataSourceId, knowledgeBaseId, and ingestionJobId.\n   b. Use the ListIngestionJobs API request to review the sync history, filtering results by status and sorting by start time or job status.\n\n3. Optimization strategies:\n   a. Analyze patterns in failed sync events to identify common issues.\n   b. Update the data source configurations if necessary, ensuring the IAM role has the required permissions.\n   c. If using a custom endpoint, verify the Secrets Manager secret is up-to-date.\n   d. Consider adjusting the maxResults parameter in API calls to balance between performance and completeness of information.\n   e. Implement pagination using the nextToken in API responses for comprehensive reviews of large sync histories.\n\nBy combining console and API methods for troubleshooting and implementing these optimization strategies, administrators can effectively manage and improve data source sync operations in Amazon Bedrock.", "question_type": "complex", "contexts": "To view information about a data source\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\nbedrock/.\n2. From the left navigation pane, select Knowledge bases.\n3. In the Data source section, select the data source for which you want to view details.\n4. The Data source overview contains details about the data source.\n5. The Sync history contains details about when the data source was synced. To see reasons\nfor why a sync event failed, select a sync event and choose View warnings.\nAPI\nTo get information about a data source, send a GetDataSource request with a Agents\nfor Amazon Bedrock build-time endpoint and specify the dataSourceId and the\nknowledgeBaseId of the knowledge base that it belongs to.\nTo list information about a knowledge base\"s data sources, send a ListDataSources request with\n a Agents for Amazon Bedrock build-time endpoint and specify the ID of the knowledge base.\n To set the maximum number of results to return in a response, use the maxResults field.\n If there are more results than the number you set, the response returns a nextToken. You\ncan use this value in another ListDataSources request to see the next batch of results.\nManage a data source 636\nAmazon Bedrock User Guide\nTo get information a sync event for a data source, send a GetIngestionJob request with a Agents\nfor Amazon Bedrock build-time endpoint. Specify the dataSourceId, knowledgeBaseId, and\ningestionJobId.\nTo list the sync history for a data source in a knowledge base, send a ListIngestionJobs request\nwith a Agents for Amazon Bedrock build-time endpoint. Specify the ID of the knowledge base\nand data source. You can set the following specifications.\n Filter for results by specifying a status to search for in the filters object.\n  Sort by the time that the job was started or the status of a job by specifying the sortBy\nobject. You can sort in ascending or descending order.\n Set the maximum number of results to return in a response in the maxResults field. If there\nare more results than the number you set, the response returns a nextToken that you can\nsend in another ListIngestionJobs request to see the next batch of jobs.\nUpdate a data source\nYou can update a data source in the following ways:\n Add, change, or remove files or content from the the data source.\n Change the data source configurations, or the KMS key to use for encrypting transient data\nduring data ingestion. If you change the source or endpoint configuration details, you should\nupdate or create a new IAM role with the required access permissions and Secrets Manager secret\n(if applicable).\n Set your data source deletion policy is to either \"Delete\" or \"Retain\". You can delete all data from\n", "original_contexts": "To view information about a data source\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\nbedrock/.\n2. From the left navigation pane, select Knowledge bases.\n3. In the Data source section, select the data source for which you want to view details.\n4. The Data source overview contains details about the data source.\n5. The Sync history contains details about when the data source was synced. To see reasons\nfor why a sync event failed, select a sync event and choose View warnings.\nAPI\nTo get information about a data source, send a GetDataSource request with a Agents\nfor Amazon Bedrock build-time endpoint and specify the dataSourceId and the\nknowledgeBaseId of the knowledge base that it belongs to.\nTo list information about a knowledge base\"s data sources, send a ListDataSources request with\n a Agents for Amazon Bedrock build-time endpoint and specify the ID of the knowledge base.\n To set the maximum number of results to return in a response, use the maxResults field.\n If there are more results than the number you set, the response returns a nextToken. You\ncan use this value in another ListDataSources request to see the next batch of results.\nManage a data source 636\nAmazon Bedrock User Guide\nTo get information a sync event for a data source, send a GetIngestionJob request with a Agents\nfor Amazon Bedrock build-time endpoint. Specify the dataSourceId, knowledgeBaseId, and\ningestionJobId.\nTo list the sync history for a data source in a knowledge base, send a ListIngestionJobs request\nwith a Agents for Amazon Bedrock build-time endpoint. Specify the ID of the knowledge base\nand data source. You can set the following specifications.\n Filter for results by specifying a status to search for in the filters object.\n  Sort by the time that the job was started or the status of a job by specifying the sortBy\nobject. You can sort in ascending or descending order.\n Set the maximum number of results to return in a response in the maxResults field. If there\nare more results than the number you set, the response returns a nextToken that you can\nsend in another ListIngestionJobs request to see the next batch of jobs.\nUpdate a data source\nYou can update a data source in the following ways:\n Add, change, or remove files or content from the the data source.\n Change the data source configurations, or the KMS key to use for encrypting transient data\nduring data ingestion. If you change the source or endpoint configuration details, you should\nupdate or create a new IAM role with the required access permissions and Secrets Manager secret\n(if applicable).\n Set your data source deletion policy is to either \"Delete\" or \"Retain\". You can delete all data from\n", "retrieved_contexts": ["Select the tab corresponding  to your method of choice and follow the steps.   Console   To view information about a data source   1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock  permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/  bedrock/.   2. From the left navigation pane, select Knowledge bases.   3. In the Data source section, select the data source for which you want to view details.   4. The Data source overview contains details about the data source.   5. The Sync history contains details about when the data source was synced. To see reasons  for why a sync event failed, select a sync event and choose View warnings.   API   To get information about a data source, send a GetDataSource request with a Agents  for Amazon Bedrock build-time endpoint and specify the dataSourceId and the knowledgeBaseId of the knowledge base that it belongs to.   To list information about a knowledge base's data sources, send a ListDataSources request with  a Agents for Amazon Bedrock build-time endpoint and specify the ID of the knowledge base.   â€¢ To set the maximum number of results to return in a response, use the maxResults field.   â€¢ If there are more results than the number you set, the response returns a nextToken.", "Manage a knowledge base 632    https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-bases-logging.html  https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-bases-logging.html  https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   â€¢ To view the details of a data source, select a Data source name. Within the details, you  can choose the radio button next to a sync event in the Sync history section and select View warnings to see why files in the data ingestion job failed to sync.   â€¢ To manage the embeddings model used for the knowledge base, select Edit Provisioned  Throughput.   â€¢ Select Save changes when you are finished editing.   API   To get information about a knowledge base, send a GetKnowledgeBase request with a Agents  for Amazon Bedrock build-time endpoint, specifying the knowledgeBaseId.   To list information about your knowledge bases, send a ListKnowledgeBases request with a Agents for Amazon Bedrock build-time endpoint. You can set the maximum number of results  to return in a response. If there are more results than the number you set, the response returns  a nextToken. You can use this value in the nextToken field of another ListKnowledgeBases request to see the next batch of results.", "Console   1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock  permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/  bedrock/.   2. Select Agents from the left navigation pane. Then, choose an agent in the Agents section.   3. In the Agent overview section, check that the User input field is DISABLED.   4. If you're checking if the optimization is being applied to the working draft of the  agent, select the Working draft in the Working draft section. If you're checking if the  optimization is being applied to a version of the agent, select the version in the Versions section.   Optimize performance 816    https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   5. Check that the Knowledge bases section contains only one knowledge base. If there's  more than one knowledge base, disable all of them except for one. To learn how to disable  knowledge bases, see Manage agent-knowledge bases associations.   6. Check that the Action groups section contains no action groups. If there are action groups,  disable all of them. To learn how to disable action groups, see Edit an action group.   7. In the Advanced prompts section, check that the Orchestration field value is Default."], "answer": "Based on the provided context, here's how an administrator can troubleshoot a failed data source sync event and optimize future sync operations in Amazon Bedrock:\n\n1. Using the Console:\n   - Sign in to the AWS Management Console and open the Amazon Bedrock console.\n   - Navigate to the Knowledge bases section in the left navigation pane.\n   - Select the relevant data source in the Data source section.\n   - In the Sync history, select a failed sync event and choose \"View warnings\" to see the reasons why the sync event failed.\n\n2. Using the API:\n   - Send a GetDataSource request to the Agents for Amazon Bedrock build-time endpoint, specifying the dataSourceId and knowledgeBaseId.\n   - This will provide detailed information about the data source, including sync history.\n\nTo optimize future sync operations:\n\n1. Review the warnings and reasons for failure obtained from the console or API.\n2. Address any issues identified in the warnings, such as file format problems, permission issues, or connectivity problems.\n3. Consider adjusting the sync frequency or timing based on the patterns observed in the sync history.\n4. If using the API method, utilize the maxResults field to manage the amount of data returned in each request, which can help with performance.\n\nBy regularly monitoring sync events and promptly addressing any issues, administrators can improve the reliability and efficiency of data source synchronization in Amazon Bedrock."}
{"question": "How do you enable Prompt flows and Prompt management in Bedrock Studio?", "ground_truth": "To enable Prompt flows and Prompt management in Bedrock Studio, update the workspace by modifying the provisioning role, updating the permissions boundary, and adding Amazon DataZone blueprints.", "question_type": "simple", "contexts": "1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://\nconsole.aws.amazon.com/bedrock/.\n2. In the left navigation pane, choose Bedrock Studio.\n3. In Bedrock Studio workspaces, select the workspace that you want to update.\n4. Choose the Overview tab. If the workspace needs an update to support Prompt flows and\nPrompt management, you will see an alert banner with steps for enabling Prompt flows and\nPrompt management.\n5. In Workspace details, choose the provisioning role ARN in Provisioning role. The IAM console\nopens with the provisioning role.\n6. In the IAM console, choose the Permissions tab.\n7. In Permission policies select the policy to open the policy editor.\n8. In the Policy editor, choose JSON, if it is not already chosen.\n9. Replace the current policy with the policy at Permissions to manage Amazon Bedrock Studio\nuser resources.\n10. Choose Next.\nUpdate the provisioning role 1010\nAmazon Bedrock User Guide\n11. Choose Save changes.\n 12. Next step: Update the permissions boundary.\nUpdate the permissions boundary\nIn this procedure, you update the permissions boundary for a Amazon Bedrock Studio workspace.\nUpdating the permissions boundary helps enable Prompt flows and Prompt management.\nTo update the permission boundaries\n1. Sign in to the AWS Management Console and open the IAM console at https://\nconsole.aws.amazon.com/iam/.\n2. On the left navigation pane, choose Policies.\n3. Open the AmazonDataZoneBedrockPermissionsBoundary policy that you created in Step\n2: Create permissions boundary, service role, and provisioning role.\n4. On the Permissions tab, choose Edit.\n5. In the Policy editor, choose JSON, if it is not already chosen.\n6. Replace the current policy with the policy at Permission boundaries.\n7. Choose Next.\n8. Choose Save changes.\n9. Next step: Add the Amazon DataZone blueprints.\nAdd the Amazon DataZone blueprints\nIn this procedure, you add the Amazon DataZone blueprints that an Amazon Bedrock Studio\n workspace needs to enable Prompt flows and Prompt management.\nTo add the Amazon DataZone blueprints\n1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://\nconsole.aws.amazon.com/bedrock/.\n2. In the left navigation pane, choose Bedrock Studio.\n3. In Bedrock Studio workspaces, select the workspace that you want to add the blueprints to.\n4. Choose the Overview tab.\nUpdate the permissions boundary 1011\nAmazon Bedrock User Guide\n5. In Workspace details, note the alert banner for Prompt management and Prompt flows. Make\nsure you have completed step one.\n6. In the alert banner, choose the Enable hyperlink to add the blueprints.\nUpdate a workspace for app export\nAmazon Bedrock Studio is in preview release for Amazon Bedrock and is subject to change.\nIf you created an Amazon Bedrock Studio workspace before the introduction of the app export\nfeature, you need to update the permissions boundary for the workspace. You don\"t need to\n", "original_contexts": "1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://\nconsole.aws.amazon.com/bedrock/.\n2. In the left navigation pane, choose Bedrock Studio.\n3. In Bedrock Studio workspaces, select the workspace that you want to update.\n4. Choose the Overview tab. If the workspace needs an update to support Prompt flows and\nPrompt management, you will see an alert banner with steps for enabling Prompt flows and\nPrompt management.\n5. In Workspace details, choose the provisioning role ARN in Provisioning role. The IAM console\nopens with the provisioning role.\n6. In the IAM console, choose the Permissions tab.\n7. In Permission policies select the policy to open the policy editor.\n8. In the Policy editor, choose JSON, if it is not already chosen.\n9. Replace the current policy with the policy at Permissions to manage Amazon Bedrock Studio\nuser resources.\n10. Choose Next.\nUpdate the provisioning role 1010\nAmazon Bedrock User Guide\n11. Choose Save changes.\n 12. Next step: Update the permissions boundary.\nUpdate the permissions boundary\nIn this procedure, you update the permissions boundary for a Amazon Bedrock Studio workspace.\nUpdating the permissions boundary helps enable Prompt flows and Prompt management.\nTo update the permission boundaries\n1. Sign in to the AWS Management Console and open the IAM console at https://\nconsole.aws.amazon.com/iam/.\n2. On the left navigation pane, choose Policies.\n3. Open the AmazonDataZoneBedrockPermissionsBoundary policy that you created in Step\n2: Create permissions boundary, service role, and provisioning role.\n4. On the Permissions tab, choose Edit.\n5. In the Policy editor, choose JSON, if it is not already chosen.\n6. Replace the current policy with the policy at Permission boundaries.\n7. Choose Next.\n8. Choose Save changes.\n9. Next step: Add the Amazon DataZone blueprints.\nAdd the Amazon DataZone blueprints\nIn this procedure, you add the Amazon DataZone blueprints that an Amazon Bedrock Studio\n workspace needs to enable Prompt flows and Prompt management.\nTo add the Amazon DataZone blueprints\n1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://\nconsole.aws.amazon.com/bedrock/.\n2. In the left navigation pane, choose Bedrock Studio.\n3. In Bedrock Studio workspaces, select the workspace that you want to add the blueprints to.\n4. Choose the Overview tab.\nUpdate the permissions boundary 1011\nAmazon Bedrock User Guide\n5. In Workspace details, note the alert banner for Prompt management and Prompt flows. Make\nsure you have completed step one.\n6. In the alert banner, choose the Enable hyperlink to add the blueprints.\nUpdate a workspace for app export\nAmazon Bedrock Studio is in preview release for Amazon Bedrock and is subject to change.\nIf you created an Amazon Bedrock Studio workspace before the introduction of the app export\nfeature, you need to update the permissions boundary for the workspace. You don\"t need to\n", "retrieved_contexts": ["Choose Confirm to make the membership changes.   8. If you added users, invite them to the workspace by doing the following.   a. Choose the Overview tab   b. Copy the Bedrock Studio URL.   c. Send the URL to the new workspace members.   Update a workspace for Prompt management and Prompt  flows   Amazon Bedrock Studio is in preview release for Amazon Bedrock and is subject to change.   Add or remove workspace members 1008    https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   If you created an Amazon Bedrock Studio workspace before the introduction of Prompt flows and  Prompt management, you need to update the workspace before workspace members can create  a Prompt flows app or use Prompt management. You don't need to update workspaces that you  create after the introduction of Prompt flows and Prompt management.   Note   You will see an alert banner in the Amazon Bedrock console when you open a workspace  that was created before the introduction of Prompt flows and Prompt management. The  alert banner contains steps for enabling Prompt flows and Prompt management. This topic  documents those steps. The banner doesn't appear for workspaces that you create after the  introduction of Prompt flows and Prompt management.   To update a workspace for Prompt management and Prompt flows   1. Update the service role that the workspace uses.   2.", "If the workspace needs an update to support Prompt flows and  Prompt management, you will see an alert banner with steps for enabling Prompt flows and  Prompt management.   Update the service role 1009    https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   5. In Workspace details, choose the service role ARN in Service role. The IAM console opens with  the service role.   6. In the IAM console, choose the Permissions tab.   7. In Permission policies select the policy to open the policy editor.   8. In the Policy editor, choose JSON, if it is not already chosen.   9. Replace the current policy with the policy at Permissions to manage an Amazon Bedrock  Studio workspace.   10. Choose Next.   11. Choose Save changes.   12. Next step: Update the provisioning role.   Update the provisioning role   In this procedure you update the provisioning role that a Amazon Bedrock Studio workspace uses.  Updating the provisioning role helps enable Prompt flows and Prompt management.   To update the provisioning role   1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://  console.aws.amazon.com/bedrock/.   2. In the left navigation pane, choose Bedrock Studio.   3.", "The banner doesn't appear for workspaces that you create after the  introduction of Prompt flows and Prompt management.   To update a workspace for Prompt management and Prompt flows   1. Update the service role that the workspace uses.   2. Update the provisioning role that the workspace uses.   3. Update the permissions boundary for the workspace.   4. Add the Amazon DataZone blueprints that the workspace needs for Prompt flows and Prompt  management.   Update the service role   In this procedure you update the service role that a Amazon Bedrock Studio workspace uses.  Updating the provisioning role helps enable Prompt flows and Prompt management.   To update the service role   1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://  console.aws.amazon.com/bedrock/.   2. In the left navigation pane, choose Bedrock Studio.   3. In Bedrock Studio workspaces, select the workspace that you want to update.   4. Choose the Overview tab. If the workspace needs an update to support Prompt flows and  Prompt management, you will see an alert banner with steps for enabling Prompt flows and  Prompt management.   Update the service role 1009    https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   5. In Workspace details, choose the service role ARN in Service role."], "answer": "Based on the provided context, to enable Prompt flows and Prompt management in an Amazon Bedrock Studio workspace that was created before these features were introduced, you need to update the workspace by following these steps:\n\n1. Update the service role that the workspace uses.\n2. Update the provisioning role that the workspace uses.\n3. Update the permissions boundary for the workspace.\n4. Add the Amazon DataZone blueprints that the workspace needs for Prompt flows and Prompt management.\n\nThe context provides detailed instructions for updating the service role and mentions that similar steps should be followed for updating the provisioning role. However, it doesn't provide specific details for steps 3 and 4.\n\nNote that these updates are only necessary for workspaces created before the introduction of Prompt flows and Prompt management. Workspaces created after their introduction don't need these updates."}
