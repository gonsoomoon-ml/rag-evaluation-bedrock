{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq ragas==0.1.20\n",
    "# !pip install -Uq langchain==0.2.16 langchain_aws langchain-community>=0.2.41 langchain-core==0.2.41 langchain-experimental==0.0.60\n",
    "# !pip install -Uq nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"data/qa_dataset.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "subset_length = 10\n",
    "test_dataset = Dataset.from_pandas(df.head(subset_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+', '', s)\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    return s\n",
    "\n",
    "def convert_to_list(example):\n",
    "    cleaned_context = clean_string(example[\"contexts\"])\n",
    "    try:\n",
    "        contexts = ast.literal_eval(cleaned_context)\n",
    "    except:\n",
    "        contexts = cleaned_context\n",
    "    return {\"contexts\": contexts}\n",
    "\n",
    "test_dataset = test_dataset.map(convert_to_list)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG implementation sample 1\n",
    "from libs.bedrock_kb_util import context_retrieval_from_kb\n",
    "\n",
    "question = test_dataset[0]['question']\n",
    "search_result = context_retrieval_from_kb(question, 3, 'us-west-2', 'CNDSUOPKAS', 'SEMANTIC')\n",
    "print(search_result[0])\n",
    "\n",
    "contexts = \"\\n--\\n\".join([result['content'] for result in search_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = 'us-west-2'\n",
    "\n",
    "retry_config = Config(\n",
    "    region_name=region,\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "boto3_client = boto3.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, contexts):\n",
    "    system_prompt = \"\"\"You are an AI assistant that uses retrieved context to answer questions accurately. \n",
    "    Follow these guidelines:\n",
    "    1. Use the provided context to inform your answers.\n",
    "    2. If the context doesn't contain relevant information, say \"I don't have enough information to answer that.\"\n",
    "    3. Be concise and to the point in your responses.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {contexts}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Please answer the question based on the given context.\"\"\"\n",
    "\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{'role': 'user', 'content': [{'text': user_prompt}]}],\n",
    "        system=[{'text': system_prompt}]\n",
    "    )\n",
    "\n",
    "    answer = response['output']['message']['content'][0]['text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "kb_region = 'us-west-2'\n",
    "kb_id = 'CNDSUOPKAS'\n",
    "top_k = 3\n",
    "\n",
    "def process_item(item):\n",
    "    sleep(5)  # Prevent throttling\n",
    "    question = item['question']\n",
    "    search_result = context_retrieval_from_kb(question, top_k, kb_region, kb_id, 'SEMANTIC')\n",
    "\n",
    "    contexts = [result['content'] for result in search_result]\n",
    "    answer = generate_answer(question, \"\\n--\\n\".join(contexts))\n",
    "\n",
    "    return {\n",
    "        'question': item['question'],\n",
    "        'ground_truth': item['ground_truth'],\n",
    "        'original_contexts': item['contexts'],\n",
    "        'retrieved_contexts': contexts,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "updated_dataset = test_dataset.map(process_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_file = \"data/updated_qa_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in updated_dataset:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/updated_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\")\n",
    "print(f\"Answer: {item['answer']}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import boto3\n",
    "# from botocore.config import Config\n",
    "\n",
    "# class Faithfulness:\n",
    "#     def __init__(self, llm_id, emb_id, region):\n",
    "#         self.llm_id = llm_id\n",
    "#         self.emb_id = emb_id\n",
    "#         self.region = region\n",
    "#         retry_config = Config(\n",
    "#             region_name=region,\n",
    "#             retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "#         )\n",
    "#         self.boto3_client = boto3.client(\"bedrock-runtime\", config=retry_config)\n",
    "#         self.tool_config = self.init_tool()\n",
    "\n",
    "#     def init_tool(self):\n",
    "#         tool_config = {\n",
    "#             \"tools\": [\n",
    "#                 {\n",
    "#                     \"toolSpec\": {\n",
    "#                         \"name\": \"StatementGenerator\",\n",
    "#                         \"description\": \"Generates simpler statements from paragraphs.\",\n",
    "#                         \"inputSchema\": {\n",
    "#                             \"json\": {\n",
    "#                                 \"type\": \"object\",\n",
    "#                                 \"properties\": {\n",
    "#                                     \"paragraph_index\": {\n",
    "#                                         \"type\": \"integer\",\n",
    "#                                         \"description\": \"The index of the original paragraph\"\n",
    "#                                     },\n",
    "#                                     \"simpler_statements\": {\n",
    "#                                         \"type\": \"array\",\n",
    "#                                         \"items\": {\n",
    "#                                             \"type\": \"string\"\n",
    "#                                         },\n",
    "#                                         \"description\": \"An array of simpler statements derived from the original paragraph\"\n",
    "#                                     }\n",
    "#                                 },\n",
    "#                                 \"required\": [\"paragraph_index\", \"simpler_statements\"]\n",
    "#                             }\n",
    "#                         }\n",
    "#                     }\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"toolSpec\": {\n",
    "#                         \"name\": \"FaithfulnessChecker\",\n",
    "#                         \"description\": \"Checks the faithfulness of statements based on a given context.\",\n",
    "#                         \"inputSchema\": {\n",
    "#                             \"json\": {\n",
    "#                                 \"type\": \"object\",\n",
    "#                                 \"properties\": {\n",
    "#                                     \"statement\": {\n",
    "#                                         \"type\": \"string\",\n",
    "#                                         \"description\": \"The statement to check for faithfulness\"\n",
    "#                                     },\n",
    "#                                     \"reason\": {\n",
    "#                                         \"type\": \"string\",\n",
    "#                                         \"description\": \"The reason for the verdict\"\n",
    "#                                     },\n",
    "#                                     \"verdict\": {\n",
    "#                                         \"type\": \"integer\",\n",
    "#                                         \"description\": \"1 if the statement is faithful, 0 if not\"\n",
    "#                                     }\n",
    "#                                 },\n",
    "#                                 \"required\": [\"statement\", \"reason\", \"verdict\"]\n",
    "#                             }\n",
    "#                         }\n",
    "#                     }\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "#         return tool_config\n",
    "\n",
    "#     def create_message_format(self, sys_template, user_template):\n",
    "#         sys_prompt = [{\"text\": sys_template}]\n",
    "#         usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template}]}]\n",
    "#         return sys_prompt, usr_prompt\n",
    "\n",
    "#     def converse_with_bedrock_tools(self, sys_prompt, usr_prompt):\n",
    "#         inference_config = {\"temperature\": 0.0, \"topP\": 0.1}\n",
    "#         response = self.boto3_client.converse(\n",
    "#             modelId=self.llm_id,\n",
    "#             messages=usr_prompt,\n",
    "#             system=sys_prompt,\n",
    "#             toolConfig=self.tool_config,\n",
    "#             inferenceConfig=inference_config\n",
    "#         )\n",
    "#         return response\n",
    "\n",
    "#     def parse_tool_use(self, message):\n",
    "#         stop_reason = message['stopReason']\n",
    "#         if stop_reason == 'tool_use':\n",
    "#             tool_requests = message['output']['message']['content']\n",
    "#             results = []\n",
    "#             for tool_request in tool_requests:\n",
    "#                 if 'toolUse' in tool_request:\n",
    "#                     tool = tool_request['toolUse']\n",
    "#                     results.append(tool['input'])\n",
    "#             return results\n",
    "#         return None\n",
    "\n",
    "#     def segment_paragraphs(self, text):\n",
    "#         paragraphs = text.split('\\n\\n')\n",
    "#         paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "#         return paragraphs\n",
    "\n",
    "#     def generate_statements(self, question, answer):\n",
    "#         sys_template = \"\"\"\n",
    "#         Given a question, an answer, and paragraphs from the answer, analyze each paragraph and break it down into one or more fully understandable statements while ensuring no pronouns are used in each statement. Use the StatementGenerator tool for each paragraph.\n",
    "#         \"\"\"\n",
    "#         paragraphs = self.segment_paragraphs(answer)\n",
    "#         paragraphs_str = '\\n'.join([f\"{i}: {p}\" for i, p in enumerate(paragraphs)])\n",
    "#         user_template = f\"\"\"\n",
    "#         Question: {question}\n",
    "#         Answer: {answer}\n",
    "#         Paragraphs:\n",
    "#         {paragraphs_str}\n",
    "#         Use the StatementGenerator tool for each paragraph.\n",
    "#         \"\"\"\n",
    "#         sys_prompt, user_prompt = self.create_message_format(sys_template, user_template)\n",
    "#         response = self.converse_with_bedrock_tools(sys_prompt, user_prompt)\n",
    "#         output = self.parse_tool_use(response)\n",
    "\n",
    "#         statements = []\n",
    "#         if output:\n",
    "#             for item in output:\n",
    "#                 statements.extend(item['simpler_statements'])\n",
    "#         return statements\n",
    "\n",
    "#     def check_faithfulness(self, context, statements):\n",
    "#         sys_template = \"\"\"\n",
    "#         Your task is to judge the faithfulness of a series of statements based on given paragraphs. For each statement, use the FaithfulnessChecker tool to determine if the statement can be directly inferred from any of the paragraphs.\n",
    "#         \"\"\"\n",
    "#         paragraphs = self.segment_paragraphs(context)\n",
    "#         paragraphs_str = json.dumps(paragraphs, ensure_ascii=False)\n",
    "#         statements_str = json.dumps(statements, ensure_ascii=False)\n",
    "#         user_template = f\"\"\"\n",
    "#         Paragraphs: {paragraphs_str}\n",
    "#         Statements: {statements_str}\n",
    "#         Use the FaithfulnessChecker tool for each statement.\n",
    "#         \"\"\"\n",
    "#         sys_prompt, user_prompt = self.create_message_format(sys_template, user_template)\n",
    "#         response = self.converse_with_bedrock_tools(sys_prompt, user_prompt)\n",
    "#         output = self.parse_tool_use(response)\n",
    "\n",
    "#         verdicts = []\n",
    "#         if output:\n",
    "#             for item in output:\n",
    "#                 verdicts.append(item['verdict'])\n",
    "#         return verdicts\n",
    "\n",
    "#     def score(self, row):\n",
    "#         question = row['user_input']\n",
    "#         answer = row['response']\n",
    "#         context = '\\n'.join(row['retrieved_contexts'])\n",
    "#         statements = self.generate_statements(question, answer)\n",
    "#         if not statements:\n",
    "#             return 0.0\n",
    "\n",
    "#         verdicts = self.check_faithfulness(context, statements)\n",
    "#         if not verdicts:\n",
    "#             return 0.0\n",
    "\n",
    "#         faithful_statements = sum(verdicts)\n",
    "#         total_statements = len(verdicts)\n",
    "#         score = faithful_statements / total_statements\n",
    "#         return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\n",
      "Answer: Based on the provided context, here's how temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how adjusting them might affect the output when generating text about different types of equines:\n",
      "\n",
      "1. Temperature: \n",
      "- Lower values increase the likelihood of higher-probability tokens and decrease the likelihood of lower-probability tokens. This would make the model more likely to choose \"horses\" in the given example.\n",
      "- Higher values increase the likelihood of lower-probability tokens and decrease the likelihood of higher-probability tokens. This would make the model more likely to consider \"zebras\" or even \"unicorns\" in the example.\n",
      "\n",
      "2. Top K:\n",
      "- A lower Top K value (e.g., 2) would limit the model to consider only the top K most likely candidates. In the example, setting Top K to 2 would make the model only consider \"horses\" and \"zebras,\" excluding \"unicorns.\"\n",
      "- A higher Top K value would allow more lower-probability tokens to be considered.\n",
      "\n",
      "3. Top P:\n",
      "- A lower Top P value (e.g., 0.7) would make the model consider only the tokens that fall within the top percentage of the probability distribution. In the example, setting Top P to 0.7 would result in only \"horses\" being considered.\n",
      "- A higher Top P value (e.g., 0.9) would allow more tokens to be considered. In the example, setting Top P to 0.9 would include both \"horses\" and \"zebras\" in the consideration.\n",
      "\n",
      "When generating text about different types of equines, adjusting these parameters would affect the output as follows:\n",
      "\n",
      "- Lower temperature, lower Top K, and lower Top P would likely result in more predictable and common equine-related responses, focusing primarily on horses.\n",
      "- Higher temperature, higher Top K, and higher Top P would increase the likelihood of the model generating text about less common equines like zebras, and potentially even fictional equines like unicorns.\n",
      "\n",
      "These parameters allow users to control the balance between creativity and predictability in the model's outputs when discussing various types of equines.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/updated_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\")\n",
    "print(f\"Answer: {item['answer']}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, metrics, llm_id, emb_id, region):\n",
    "    \"\"\"\n",
    "    Evaluate the dataset using the specified metrics.\n",
    "\n",
    "    Args:\n",
    "    dataset (List[Dict]): List of dictionaries containing 'user_input', 'response', and 'retrieved_contexts'.\n",
    "    metrics (List[AnswerRelevancy]): List of metric objects to use for evaluation.\n",
    "    llm_id (str): ID of the LLM model to use.\n",
    "    embeddings_id (str): ID of the embeddings model to use.\n",
    "    region (str): AWS region to use for Bedrock.\n",
    "\n",
    "    Returns:\n",
    "    Dict: A dictionary containing the scores for each metric.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, AnswerRelevancy):\n",
    "            metric.llm_id = llm_id\n",
    "            metric.emb_id = emb_id\n",
    "            metric.region = region\n",
    "\n",
    "        scores = []\n",
    "        for row in dataset:\n",
    "            try:\n",
    "                score = metric.score(row)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            results[metric.__class__.__name__] = avg_score\n",
    "        else:\n",
    "            results[metric.__class__.__name__] = \"No valid scores\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2093.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Faithfulness': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from libs.eval_metrics import AnswerRelevancy, Faithfulness\n",
    "\n",
    "llm_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "emb_id = \"amazon.titan-embed-text-v2:0\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "#metrics = [AnswerRelevancy(llm_id=llm_id, emb_id=emb_id, region=region, strictness=1)]\n",
    "metrics = [Faithfulness(llm_id=llm_id, emb_id=emb_id, region=region)]\n",
    "\n",
    "def map_dataset(example):\n",
    "    return {\n",
    "        \"user_input\": example[\"question\"],\n",
    "        \"retrieved_contexts\": example[\"retrieved_contexts\"],\n",
    "        \"referenced_contexts\": example[\"original_contexts\"],\n",
    "        \"response\": example[\"answer\"],\n",
    "        \"reference\": example[\"ground_truth\"]\n",
    "    }\n",
    "\n",
    "dataset = updated_dataset.map(map_dataset).select(range(2))\n",
    "results = evaluate(dataset, metrics, llm_id, emb_id, region)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"data/ragas_evaluation_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[:, \"context_precision\":\"context_recall\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
