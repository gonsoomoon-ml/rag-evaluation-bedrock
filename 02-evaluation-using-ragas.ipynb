{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq ragas==0.1.20\n",
    "# !pip install -Uq langchain==0.2.16 langchain_aws langchain-community>=0.2.41 langchain-core==0.2.41 langchain-experimental==0.0.60\n",
    "# !pip install -Uq nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"data/qa_dataset.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "subset_length = 10\n",
    "test_dataset = Dataset.from_pandas(df.head(subset_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+', '', s)\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    return s\n",
    "\n",
    "def convert_to_list(example):\n",
    "    cleaned_context = clean_string(example[\"contexts\"])\n",
    "    try:\n",
    "        contexts = ast.literal_eval(cleaned_context)\n",
    "    except:\n",
    "        contexts = cleaned_context\n",
    "    return {\"contexts\": contexts}\n",
    "\n",
    "test_dataset = test_dataset.map(convert_to_list)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG implementation sample 1\n",
    "from libs.bedrock_kb_util import context_retrieval_from_kb\n",
    "\n",
    "question = test_dataset[0]['question']\n",
    "search_result = context_retrieval_from_kb(question, 3, 'us-west-2', 'CNDSUOPKAS', 'SEMANTIC')\n",
    "print(search_result[0])\n",
    "\n",
    "contexts = \"\\n--\\n\".join([result['content'] for result in search_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = 'us-west-2'\n",
    "\n",
    "retry_config = Config(\n",
    "    region_name=region,\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "boto3_client = boto3.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, contexts):\n",
    "    system_prompt = \"\"\"You are an AI assistant that uses retrieved context to answer questions accurately. \n",
    "    Follow these guidelines:\n",
    "    1. Use the provided context to inform your answers.\n",
    "    2. If the context doesn't contain relevant information, say \"I don't have enough information to answer that.\"\n",
    "    3. Be concise and to the point in your responses.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {contexts}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Please answer the question based on the given context.\"\"\"\n",
    "\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{'role': 'user', 'content': [{'text': user_prompt}]}],\n",
    "        system=[{'text': system_prompt}]\n",
    "    )\n",
    "\n",
    "    answer = response['output']['message']['content'][0]['text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "kb_region = 'us-west-2'\n",
    "kb_id = 'CNDSUOPKAS'\n",
    "top_k = 3\n",
    "\n",
    "def process_item(item):\n",
    "    sleep(5)  # Prevent throttling\n",
    "    question = item['question']\n",
    "    search_result = context_retrieval_from_kb(question, top_k, kb_region, kb_id, 'SEMANTIC')\n",
    "\n",
    "    contexts = [result['content'] for result in search_result]\n",
    "    answer = generate_answer(question, \"\\n--\\n\".join(contexts))\n",
    "\n",
    "    return {\n",
    "        'question': item['question'],\n",
    "        'ground_truth': item['ground_truth'],\n",
    "        'original_contexts': item['contexts'],\n",
    "        'retrieved_contexts': contexts,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "updated_dataset = test_dataset.map(process_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_file = \"data/updated_qa_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in updated_dataset:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\n",
      "Answer: Based on the provided context, here's how temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how adjusting them might affect the output when generating text about different types of equines:\n",
      "\n",
      "1. Temperature: \n",
      "- Lower values increase the likelihood of higher-probability tokens and decrease the likelihood of lower-probability tokens. This would make the model more likely to choose \"horses\" in the given example.\n",
      "- Higher values increase the likelihood of lower-probability tokens and decrease the likelihood of higher-probability tokens. This would make the model more likely to consider \"zebras\" or even \"unicorns\" in the example.\n",
      "\n",
      "2. Top K:\n",
      "- A lower Top K value (e.g., 2) would limit the model to consider only the top K most likely candidates. In the example, setting Top K to 2 would make the model only consider \"horses\" and \"zebras,\" excluding \"unicorns.\"\n",
      "- A higher Top K value would allow more lower-probability tokens to be considered.\n",
      "\n",
      "3. Top P:\n",
      "- A lower Top P value (e.g., 0.7) would make the model consider only the tokens that fall within the top percentage of the probability distribution. In the example, setting Top P to 0.7 would result in only \"horses\" being considered.\n",
      "- A higher Top P value (e.g., 0.9) would allow more tokens to be considered. In the example, setting Top P to 0.9 would include both \"horses\" and \"zebras\" in the consideration.\n",
      "\n",
      "When generating text about different types of equines, adjusting these parameters would affect the output as follows:\n",
      "\n",
      "- Lower temperature, lower Top K, and lower Top P would likely result in more predictable and common equine-related responses, focusing primarily on horses.\n",
      "- Higher temperature, higher Top K, and higher Top P would increase the likelihood of the model generating text about less common equines like zebras, and potentially even fictional equines like unicorns.\n",
      "\n",
      "These parameters allow users to control the balance between creativity and predictability in the model's outputs when discussing various types of equines.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/updated_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\")\n",
    "print(f\"Answer: {item['answer']}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Randomness and diversity 239        Amazon Bedrock User Guide   â€¢ If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and  \"zebras.\"   â€¢ If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that  lies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers  \"horses\" and \"zebras\" as they are in the top 90% of probability distribution.   Length   Foundation models typically support parameters that limit the length of the response. Examples of  these parameters are provided below.   â€¢ Response length â€“ An exact value to specify the minimum or maximum number of tokens to  return in the generated response.   â€¢ Penalties â€“ Specify the degree to which to penalize outputs in a response. Examples include the  following.   â€¢ The length of the response.   â€¢ Repeated tokens in a response.   â€¢ Frequency of tokens in a response.   â€¢ Types of tokens in a response.   â€¢ Stop sequences â€“ Specify sequences of characters that stop the model from generating further  tokens. If the model generates a stop sequence that you specify, it will stop generating after that  sequence.', 'The following table summarizes the effects of these parameters.   Parameter Effect of lower value Effect of higher value   Temperature Increase likelihood of higher- probability tokens   Decrease likelihood of lower- probability tokens   Increase likelihood of lower- probability tokens   Decrease likelihood of higher- probability tokens   Top K Remove lower-probability  tokens   Allow lower-probability  tokens   Top P Remove lower-probability  tokens   Allow lower-probability  tokens   As an example to understand these parameters, consider the example prompt I hear the hoof  beats of \". Let\\'s say that the model determines the following three words to be candidates for  the next token. The model also assigns a probability for each word.   {      \"horses\": 0.7,      \"zebras\": 0.2,      \"unicorns\": 0.1 }   â€¢ If you set a high temperature, the probability distribution is flattened and the probabilities  become less different, which would increase the probability of choosing \"unicorns\" and decrease  the probability of choosing \"horses\".   Randomness and diversity 239        Amazon Bedrock User Guide   â€¢ If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and  \"zebras.\"', \"General guidelines for Amazon Bedrock LLM users 297        Amazon Bedrock User Guide   (Source: Prompt written by AWS)   Use inference parameters   LLMs on Amazon Bedrock all come with several inference parameters that you can set to control  the response from the models. The following is a list of all the common inference parameters that  are available on Amazon Bedrock LLMs and how to use them.   Temperature is a value between 0 and 1, and it regulates the creativity of LLMsâ€™ responses. Use  lower temperature if you want more deterministic responses, and use higher temperature if you  want more creative or different responses for the same prompt from LLMs on Amazon Bedrock. For  all the examples in this prompt guideline, we set temperature = 0.   Maximum generation length/maximum new tokens limits the number of tokens that the LLM  generates for any prompt. It's helpful to specify this number as some tasks, such as sentiment  classification, don't need a long answer.   Top-p controls token choices, based on the probability of the potential choices. If you set Top-p  below 1.0, the model considers the most probable options and ignores less probable options. The  result is more stable and repetitive completions.   Use inference parameters 298        Amazon Bedrock User Guide   End token/end sequence specifies the token that the LLM uses to indicate the end of the output.\"], [\"import boto3   bedrock = boto3.client(service_name='bedrock')   bedrock.get_foundation_model(modelIdentifier='anthropic.claude-v2')   Model support by AWS Region   Note   All models, except Anthropic Claude 3 Opus, Amazon Titan Text Premier, and Mistral Small  are supported in both the US East (N. Virginia, us-east-1) and the US West (Oregon, us- west-2) Regions. Amazon Titan Text Premier, Mistral Small, and AI21 Jamba-Instruct models are only  available in the US East (N. Virginia, us-east-1) Region. Anthropic Claude 3 Opus, Meta Llama 3.1 Instruct, and Mistral Large 2 (24.07) models are  only available in the US West (Oregon, us-west-2) Region.   Model support by AWS Region 39        Amazon Bedrock User Guide   Note   Access to models in Europe (Ireland) and Asia Pacific (Singapore) Regions are currently  gated. Please contact your account manager to request model access in these Regions.   The following table shows the FMs that are available in other Regions and whether they're  supported in each Region.   Model Asia  Pacific  (Mumbai)   Asia  Pacific  (Singapor  e)  NOTE:  Gated  access  only   Asia  Pacific  (Sydney)   Asia\", \"Guide   Note   The Meta Llama 2 (non-chat) models can only be used after being customized and after purchasing Provisioned Throughput for them.   Model lifecycle   Amazon Bedrock is continuously working to bring the latest versions of foundation models that  have better capabilities, accuracy, and safety. As we launch new model versions, you can test them  with the Amazon Bedrock console or API, and migrate your applications to benefit from the latest  model versions.   A model offered on Amazon Bedrock can be in one of these states: Active, Legacy, or  End-of-Life  (EOL).   â€¢ Active: The model provider is actively working on this version, and it will continue to get updates  such as bug fixes and minor improvements.   â€¢ Legacy: A version is marked Legacy when there is a more recent version which provides superior  performance. Amazon Bedrock sets an EOL date for Legacy versions.   â€¢ EOL: This version is no longer available for use. Any requests made to this version will fail.   We will support base models for a minimum of 12 months from the launch in a region. We will  always give customers 6 months of notice before we mark the model EOL. Model will be marked  Legacy on the date when the EOL notice is sent out. The console marks a model version's state as Active or Legacy.\", \"Amazon Bedrock sets an EOL date for Legacy versions.   â€¢ EOL: This version is no longer available for use. Any requests made to this version will fail.   We will support base models for a minimum of 12 months from the launch in a region. We will  always give customers 6 months of notice before we mark the model EOL. Model will be marked  Legacy on the date when the EOL notice is sent out. The console marks a model version's state as Active or Legacy. When you make a GetFoundationModelor ListFoundationModels call, you can  find the state of the model in the modelLifecycle field in the response. While you can continue  to use a Legacy version, you should plan to transition to an Active version before the EOL date.   On-Demand, Provisioned Throughput, and model customization   You specify the version of a model when you use it in On-Demand mode (for example, anthropic.claude-v2, anthropic.claude-v2:1, etc.).   When you configure Provisioned Throughput, you must specify a model version that will remain  unchanged for the entire term.\"], ['tools = [      {          \"type\": \"function\",          \"function\": {              \"name\": \"retrieve_payment_status\",              \"description\": \"Get payment status of a transaction\",              \"parameters\": {                  \"type\": \"object\",                  \"properties\": {                      \"transaction_id\": {                          \"type\": \"string\",                          \"description\": \"The transaction id.\",                      }                  },                  \"required\": [\"transaction_id\"],              },          },      },      {          \"type\": \"function\",          \"function\": {              \"name\": \"retrieve_payment_date\",              \"description\": \"Get payment date of a transaction\",              \"parameters\": {                  \"type\": \"object\",                  \"properties\": {                      \"transaction_id\": {                          \"type\": \"string\",                          \"description\": \"The transaction id.\",                      }                  },                  \"required\": [\"transaction_id\"],              },          },      } ]   names_to_functions = {    Mistral AI models 199        Amazon Bedrock User Guide       \\'retrieve_payment_status\\': functools.partial(retrieve_payment_status, df=df),      \\'retrieve_payment_date\\': functools.partial(retrieve_payment_date, df=df) }   test_tool_input = \"What\\'s the status of my transaction T1001?\" message = [{\"role\": \"user\", \"content\": test_tool_input}]   def invoke_bedrock_mistral_tool():           mistral_params = {          \"body\": json.dumps({              \"messages\":', \"data = {      'transaction_id': ['T1001', 'T1002', 'T1003', 'T1004', 'T1005'],      'customer_id': ['C001', 'C002', 'C003', 'C002', 'C001'],      'payment_amount': [125.50, 89.99, 120.00, 54.30, 210.20],      'payment_date': ['2021-10-05', '2021-10-06', '2021-10-07', '2021-10-05',   '2021-10-08'],      'payment_status': ['Paid', 'Unpaid', 'Paid', 'Paid', 'Pending'] }   # Create DataFrame df = pd.DataFrame(data)   def retrieve_payment_status(df: data, transaction_id: str) -> str:      if transaction_id in df.transaction_id.values:           return json.dumps({'status': df[df.transaction_id ==   transaction_id].payment_status.item()})      return json.dumps({'error': 'transaction id not found.'})\", \"Mistral AI models 198        Amazon Bedrock User Guide   def retrieve_payment_date(df: data, transaction_id: str) -> str:      if transaction_id in df.transaction_id.values:           return json.dumps({'date': df[df.transaction_id ==   transaction_id].payment_date.item()})      return json.dumps({'error': 'transaction id not found.'})\"], ['The following shows the general structure of an S3 retrieval FlowNode object:   {      \"name\": \"string\",      \"type\": \"Retrieval\",      \"inputs\": [          {              \"name\": \"objectKey\",              \"type\": \"String\",              \"expression\": \"string\"          }      ],      \"outputs\": [          {              \"name\": \"s3Content\",              \"type\": \"String\"          }      ],      \"configuration\": {          \"retrieval\": {              \"serviceConfiguration\": {                  \"s3\": {                      \"bucketName\": \"string\"                  }              }          }      } }   Node types in prompt flow 846    https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html  https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_FlowNode.html      Amazon Bedrock User Guide   Lambda function node   A Lambda function node lets you call a Lambda function in which you can define code to carry out  business logic. When you include a Lambda node in a prompt flow, Amazon Bedrock sends an input  event to the Lambda function that you specify.   In the configuration, specify the Amazon Resource Name (ARN) of the Lambda function. Define  inputs to send in the Lambda input event. You can write code based on these inputs and define  what the function returns. The function response is returned in the output.   The following shows the general', 'In the configuration,  you specify the S3 bucket to use for data storage. The inputs into the node are the content to store  and the object key. The node returns the URI of the S3 location as its output.   The following shows the general structure of an S3 storage FlowNode object:   {      \"name\": \"string\",      \"type\": \"Storage\",      \"inputs\": [          {              \"name\": \"content\",              \"type\": \"String | Number | Boolean | Object | Array\",              \"expression\": \"string\"          },          {              \"name\": \"objectKey\",              \"type\": \"String\",              \"expression\": \"string\"          }      ],      \"outputs\": [          {              \"name\": \"s3Uri\",              \"type\": \"String\"          }      ],      \"configuration\": {          \"retrieval\": {              \"serviceConfiguration\": {                  \"s3\": {                      \"bucketName\": \"string\"                  }              }          }      } }   Node types in prompt flow 845    https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html  https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_FlowNode.html      Amazon Bedrock User Guide   S3 retrieval node   An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow.', 'In  the configuration, you specify the S3 bucket from which to retrieve data. The input into the node is  the object key. The node returns the content in the S3 location as the output.   Note   Currently, the data in the S3 location must be a UTF-8 encoded string.   The following shows the general structure of an S3 retrieval FlowNode object:   {      \"name\": \"string\",      \"type\": \"Retrieval\",      \"inputs\": [          {              \"name\": \"objectKey\",              \"type\": \"String\",              \"expression\": \"string\"          }      ],      \"outputs\": [          {              \"name\": \"s3Content\",              \"type\": \"String\"          }      ],      \"configuration\": {          \"retrieval\": {              \"serviceConfiguration\": {                  \"s3\": {                      \"bucketName\": \"string\"                  }              }          }      } }   Node types in prompt flow 846    https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html  https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_FlowNode.html      Amazon Bedrock User Guide   Lambda function node   A Lambda function node lets you call a Lambda function in which you can define code to carry out  business logic.'], ['Otherwise, do the following:   To create a version of your prompt   1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock  permissions, and open the Amazon Bedrock console at Getting Started with the AWS  Management Console.   2. Select Prompt management from the left navigation pane. Then, choose a prompt in the Prompts section.   3. In the Prompt versions section, choose Create version to take a snapshot of your draft  version.   API   To create a version of your prompt, send a CreatePromptVersion request (see link for request  and response formats and field details) with an Agents for Amazon Bedrock build-time  endpoint and specify the ARN or ID of the prompt as the promptIdentifier.   The response returns an ID and ARN for the version. Versions are created incrementally, starting  from 1.', 'View information about the prompt version that you just created, alongside information  about the draft version, by running the following code snippet to make a ListPrompts Agents for Amazon Bedrock build-time endpoint:   # List versions of the prompt that you just created client.list_prompts(promptIdentifier=prompt_id)   6. View information for the prompt version that you just created by running the following  code snippet to make a GetPrompt Agents for Amazon Bedrock build-time endpoint:   # Get information about the prompt version that you created client.get_prompt(      promptIdentifier=prompt_id,       promptVersion=prompt_version )   7. Test the prompt by adding it to a prompt flow by following the steps at Run Prompt flows  code samples. In the first step when you create the flow, run the following code snippet  instead to use the prompt that you created instead of defining an inline prompt in the flow  (replace the ARN of the prompt version in the promptARN field with the ARN of the version  of the prompt that you created):   # Import Python SDK and create client import boto3   client = boto3.client(service_name=\\'bedrock-agent\\')   FLOWS_SERVICE_ROLE = \"arn:aws:iam::123456789012:role/MyPromptFlowsRole\" #   Prompt flows service role that you created.', 'â€¢ Prompt draft â€“ Contains the prompt message and configurations for the latest saved  draft version of the prompt.   â€¢ Prompt versions â€“ A list of all versions of the prompt that have been created. For more  information about prompt versions, see Deploy prompts using Prompt management by  creating versions.   API   To get information about a prompt, send a GetPrompt request (see link for request and  response formats and field details) with an Agents for Amazon Bedrock build-time endpoint and specify the ARN or ID of the prompt as the promptIdentifier. To get information about  a specific version of the prompt, specify DRAFT or the version number in the promptVersion field.   To list information about your agents, send a ListPrompts request (see link for request and  response formats and field details) with an Agents for Amazon Bedrock build-time endpoint.'], ['Each prompt in your dataset is perturbed approximately 5 times. Then, each  perturbed response is sent for inference, and used to calculate robustness scores automatically.   â€¢ Toxicity: For this metric, the value is calculated using toxicity from the detoxify algorithm. A low  toxicity value indicates that your selected model is not producing large amounts of toxic content.  To learn more about the detoxify algorithm and see how toxicity is calculated, see the detoxify  algorithm on GitHub.   How each available metric is calculated when applied to the text summarization task type   â€¢ Accuracy: For this metric, the value is calculated using BERT Score. BERT Score is calculated  using pre-trained contextual embeddings from BERT models. It matches words in candidate and  reference sentences by cosine similarity.   â€¢ Robustness: For this metric, the value calculated is a percentage. It calculated by taking (Delta  BERTScore / BERTScore) x 100. Delta BERTScore is the difference in BERT Scores between a  perturbed prompt and the original prompt in your dataset. Each prompt in your dataset is  perturbed approximately 5 times. Then, each perturbed response is sent for inference, and used    Automated reports 481    https://github.com/unitaryai/detoxify  https://github.com/unitaryai/detoxify      Amazon Bedrock User Guide   to calculate robustness scores automatically.', 'When you create an automatic model evaluation job and choose a Task type Amazon Bedrock  provides you with a list of recommended metrics. For each metric, Amazon Bedrock also provides  recommended built-in datasets. To learn more about available task types, see Model evaluation  tasks.   Bias in Open-ended Language Generation Dataset (BOLD)   The Bias in Open-ended Language Generation Dataset (BOLD) is a dataset that evaluates  fairness in general text generation, focusing on five domains: profession, gender, race, religious  ideologies, and political ideologies. It contains 23,679 different text generation prompts.   RealToxicityPrompts   RealToxicityPrompts is a dataset that evaluates toxicity. It attempts to get the model to  generate racist, sexist, or otherwise toxic language. This dataset contains 100,000 different text  generation prompts.   T-Rex : A Large Scale Alignment of Natural Language with Knowledge Base Triples (TREX)   TREX is dataset consisting of Knowledge Base Triples (KBTs) extracted from Wikipedia.  KBTs are a type of data structure used in natural language processing (NLP) and knowledge  representation. They consist of a subject, predicate, and object, where the subject and object  are linked by a relation. An example of a Knowledge Base Triple (KBT) is \"George Washington  was the president of the United States\".', 'Available built-in datasets for general text generation in Amazon Bedrock   Task type Metric Built-in datasets  (Console)   Built-in datasets  (API)   Computed  metric   General text  generation   Accuracy TREX Builtin.T-REx Real world  knowledge  (RWK) score   General text generation 461    https://hadyelsahar.github.io/t-rex/      Amazon Bedrock User Guide   Task type Metric Built-in datasets  (Console)   Built-in datasets  (API)   Computed  metric   BOLD Builtin.BOLD   WikiText2 Builtin.W  ikiText2   Robustnes  s   TREX Builtin.T-REx   Word error  rate   RealToxicityPrompts Builtin.R  ealToxici  tyPrompts   Toxicity   BOLD Builtin.Bold   Toxicity   To learn more about how the computed metric for each built-in dataset is calculated, see Model  evaluation job results   Text summarization   Important   For text summarization, there is a known system issue that prevents Cohere models from  completing the toxicity evaluation successfully.   Text summarization is used for tasks including creating summaries of news, legal documents,  academic papers, content previews, and content curation.'], [': reference})      return references   Post-processing   The following example shows a post-processing parser Lambda function written in Python.   import json import re import logging     FINAL_RESPONSE_REGEX = r\"&lt;final_response>([\\\\s\\\\S]*?)&lt;/final_response>\" FINAL_RESPONSE_PATTERN = re.compile(FINAL_RESPONSE_REGEX, re.DOTALL)   logger = logging.getLogger()     # This parser lambda is an example of how to parse the LLM output for the default   PostProcessing prompt def lambda_handler(event, context):      logger.info(\"Lambda input: \" + str(event))      raw_response = event[\\'invokeModelRawResponse\\']            parsed_response = {          \\'promptType\\': \\'POST_PROCESSING\\',          \\'postProcessingParsedResponse\\': {}      }            matcher = FINAL_RESPONSE_PATTERN.search(raw_response)      if not matcher:          raise Exception(\"Could not parse raw LLM output\")      response_text = matcher.group(1).strip()            parsed_response[\\'postProcessingParsedResponse\\'][\\'responseText\\'] = response_text    Advanced prompts 810        Amazon Bedrock User Guide            logger.info(parsed_response)      return parsed_response   Control session context   For greater control of session context, you can modify the SessionState object in your agent.  The SessionState object contains information that can be maintained across turns (separate InvokeAgent request and responses). You can use this information to provide conversational  context for the agent during user conversations.', 'import json import re import logging     FINAL_RESPONSE_REGEX = r\"&lt;final_response>([\\\\s\\\\S]*?)&lt;/final_response>\" FINAL_RESPONSE_PATTERN = re.compile(FINAL_RESPONSE_REGEX, re.DOTALL)   logger = logging.getLogger()     # This parser lambda is an example of how to parse the LLM output for the default   PostProcessing prompt def lambda_handler(event, context):      logger.info(\"Lambda input: \" + str(event))      raw_response = event[\\'invokeModelRawResponse\\']            parsed_response = {          \\'promptType\\': \\'POST_PROCESSING\\',          \\'postProcessingParsedResponse\\': {}      }            matcher = FINAL_RESPONSE_PATTERN.search(raw_response)      if not matcher:          raise Exception(\"Could not parse raw LLM output\")      response_text = matcher.group(1).strip()            parsed_response[\\'postProcessingParsedResponse\\'][\\'responseText\\'] = response_text    Advanced prompts 810        Amazon Bedrock User Guide            logger.info(parsed_response)      return parsed_response   Control session context   For greater control of session context, you can modify the SessionState object in your agent.  The SessionState object contains information that can be maintained across turns (separate InvokeAgent request and responses). You can use this information to provide conversational  context for the agent during user conversations.   The general format of the SessionState object is as follows.   {      \"sessionAttributes\": {          \"<attributeName1>\": \"<attributeValue1>\",', '>(.+?)&lt;/source\\\\\\\\s?>\" ANSWER_PART_PATTERN = re.compile(ANSWER_PART_REGEX, re.DOTALL) ANSWER_TEXT_PART_PATTERN = re.compile(ANSWER_TEXT_PART_REGEX, re.DOTALL) ANSWER_REFERENCE_PART_PATTERN = re.compile(ANSWER_REFERENCE_PART_REGEX, re.DOTALL)   logger = logging.getLogger()     # This parser lambda is an example of how to parse the LLM output for the default KB   response generation prompt def lambda_handler(event, context):      logger.info(\"Lambda input: \" + str(event))      raw_response = event[\\'invokeModelRawResponse\\']            parsed_response = {          \\'promptType\\': \\'KNOWLEDGE_BASE_RESPONSE_GENERATION\\',          \\'knowledgeBaseResponseGenerationParsedResponse\\': {              \\'generatedResponse\\': parse_generated_response(raw_response)          }      }            logger.info(parsed_response)      return parsed_response        def parse_generated_response(sanitized_llm_response):      results = []            for match in ANSWER_PART_PATTERN.finditer(sanitized_llm_response):          part = match.group(1).strip()                    text_match = ANSWER_TEXT_PART_PATTERN.search(part)          if not text_match:              raise ValueError(\"Could not parse generated response\")                    text = text_match.group(1).strip()                  references = parse_references(sanitized_llm_response, part)          results.append((text, references))            generated_response_parts = []      for text, references in'], [\"For more information, see Downloading Reports in AWS Artifact.   Your compliance responsibility when using AWS services is determined by the sensitivity of your  data, your company's compliance objectives, and applicable laws and regulations. AWS provides the  following resources to help with compliance:   â€¢ Security and Compliance Quick Start Guides â€“ These deployment guides discuss architectural  considerations and provide steps for deploying baseline environments on AWS that are security  and compliance focused.   â€¢ Architecting for HIPAA Security and Compliance on Amazon Web Services â€“ This whitepaper  describes how companies can use AWS to create HIPAA-eligible applications.   Note   Not all AWS services are HIPAA eligible. For more information, see the HIPAA Eligible  Services Reference.   â€¢ AWS Compliance Resources â€“ This collection of workbooks and guides might apply to your  industry and location.   â€¢ AWS Customer Compliance Guides â€“ Understand the shared responsibility model through the  lens of compliance. The guides summarize the best practices for securing AWS services and map  the guidance to security controls across multiple frameworks (including National Institute of  Standards and Technology (NIST), Payment Card Industry Security Standards Council (PCI), and  International Organization for Standardization (ISO)).   â€¢ Evaluating Resources with Rules in the AWS Config Developer Guide â€“ The AWS Config service  assesses how well your resource configurations comply with internal practices, industry  guidelines, and regulations.\", 'â€¢ To learn the difference between using roles and resource-based policies for cross-account access,  see Cross account resource access in IAM in the IAM User Guide.   Compliance validation for Amazon Bedrock   To learn whether an AWS service is within the scope of specific compliance programs, see AWS  services in Scope by Compliance Program and choose the compliance program that you are  interested in. For general information, see AWS Compliance Programs.   Compliance validation 1155    https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html  https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies-cross-account-resource-access.html  https://aws.amazon.com/compliance/services-in-scope/  https://aws.amazon.com/compliance/services-in-scope/  https://aws.amazon.com/compliance/programs/      Amazon Bedrock User Guide   You can download third-party audit reports using AWS Artifact. For more information, see Downloading Reports in AWS Artifact.', 'You can choose either CloudWatch Logs,  Amazon S3, or Amazon Data Firehose as the destination for storing logs. You must specify the  Amazon Resource Name of one of the destination options for where your logs will be stored. You  can choose the outputFormat of the logs to be one of the following: json, plain, w3c, raw, parquet. The following is an example of configuring logs to be stored in an Amazon S3 bucket  and in JSON format.   {     \"deliveryDestinationConfiguration\": {         \"destinationResourceArn\": \"arn:aws:s3:::bucket-name\"     },     \"name\": \"string\",     \"outputFormat\": \"json\",     \"tags\": {         \"key\" : \"value\"      } }   Note that if you are delivering logs cross-account, you must use the PutDeliveryDestinationPolicy API to assign an AWS Identity and Access Management  (IAM) policy to the destination account. The IAM policy allows delivery from one account to  another account.   4. Call CreateDelivery: Use the CreateDelivery API call to link the delivery source to the  destination that you created in the previous steps. This API operation associates the delivery  source with the end destination.'], [\"Select the tab corresponding  to your method of choice and follow the steps.   Console   To view information about a data source   1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock  permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/  bedrock/.   2. From the left navigation pane, select Knowledge bases.   3. In the Data source section, select the data source for which you want to view details.   4. The Data source overview contains details about the data source.   5. The Sync history contains details about when the data source was synced. To see reasons  for why a sync event failed, select a sync event and choose View warnings.   API   To get information about a data source, send a GetDataSource request with a Agents  for Amazon Bedrock build-time endpoint and specify the dataSourceId and the knowledgeBaseId of the knowledge base that it belongs to.   To list information about a knowledge base's data sources, send a ListDataSources request with  a Agents for Amazon Bedrock build-time endpoint and specify the ID of the knowledge base.   â€¢ To set the maximum number of results to return in a response, use the maxResults field.   â€¢ If there are more results than the number you set, the response returns a nextToken.\", 'Manage a knowledge base 632    https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-bases-logging.html  https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-bases-logging.html  https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   â€¢ To view the details of a data source, select a Data source name. Within the details, you  can choose the radio button next to a sync event in the Sync history section and select View warnings to see why files in the data ingestion job failed to sync.   â€¢ To manage the embeddings model used for the knowledge base, select Edit Provisioned  Throughput.   â€¢ Select Save changes when you are finished editing.   API   To get information about a knowledge base, send a GetKnowledgeBase request with a Agents  for Amazon Bedrock build-time endpoint, specifying the knowledgeBaseId.   To list information about your knowledge bases, send a ListKnowledgeBases request with a Agents for Amazon Bedrock build-time endpoint. You can set the maximum number of results  to return in a response. If there are more results than the number you set, the response returns  a nextToken. You can use this value in the nextToken field of another ListKnowledgeBases request to see the next batch of results.', \"Console   1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock  permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/  bedrock/.   2. Select Agents from the left navigation pane. Then, choose an agent in the Agents section.   3. In the Agent overview section, check that the User input field is DISABLED.   4. If you're checking if the optimization is being applied to the working draft of the  agent, select the Working draft in the Working draft section. If you're checking if the  optimization is being applied to a version of the agent, select the version in the Versions section.   Optimize performance 816    https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   5. Check that the Knowledge bases section contains only one knowledge base. If there's  more than one knowledge base, disable all of them except for one. To learn how to disable  knowledge bases, see Manage agent-knowledge bases associations.   6. Check that the Action groups section contains no action groups. If there are action groups,  disable all of them. To learn how to disable action groups, see Edit an action group.   7. In the Advanced prompts section, check that the Orchestration field value is Default.\"], [\"Choose Confirm to make the membership changes.   8. If you added users, invite them to the workspace by doing the following.   a. Choose the Overview tab   b. Copy the Bedrock Studio URL.   c. Send the URL to the new workspace members.   Update a workspace for Prompt management and Prompt  flows   Amazon Bedrock Studio is in preview release for Amazon Bedrock and is subject to change.   Add or remove workspace members 1008    https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   If you created an Amazon Bedrock Studio workspace before the introduction of Prompt flows and  Prompt management, you need to update the workspace before workspace members can create  a Prompt flows app or use Prompt management. You don't need to update workspaces that you  create after the introduction of Prompt flows and Prompt management.   Note   You will see an alert banner in the Amazon Bedrock console when you open a workspace  that was created before the introduction of Prompt flows and Prompt management. The  alert banner contains steps for enabling Prompt flows and Prompt management. This topic  documents those steps. The banner doesn't appear for workspaces that you create after the  introduction of Prompt flows and Prompt management.   To update a workspace for Prompt management and Prompt flows   1. Update the service role that the workspace uses.   2.\", 'If the workspace needs an update to support Prompt flows and  Prompt management, you will see an alert banner with steps for enabling Prompt flows and  Prompt management.   Update the service role 1009    https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   5. In Workspace details, choose the service role ARN in Service role. The IAM console opens with  the service role.   6. In the IAM console, choose the Permissions tab.   7. In Permission policies select the policy to open the policy editor.   8. In the Policy editor, choose JSON, if it is not already chosen.   9. Replace the current policy with the policy at Permissions to manage an Amazon Bedrock  Studio workspace.   10. Choose Next.   11. Choose Save changes.   12. Next step: Update the provisioning role.   Update the provisioning role   In this procedure you update the provisioning role that a Amazon Bedrock Studio workspace uses.  Updating the provisioning role helps enable Prompt flows and Prompt management.   To update the provisioning role   1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://  console.aws.amazon.com/bedrock/.   2. In the left navigation pane, choose Bedrock Studio.   3.', \"The banner doesn't appear for workspaces that you create after the  introduction of Prompt flows and Prompt management.   To update a workspace for Prompt management and Prompt flows   1. Update the service role that the workspace uses.   2. Update the provisioning role that the workspace uses.   3. Update the permissions boundary for the workspace.   4. Add the Amazon DataZone blueprints that the workspace needs for Prompt flows and Prompt  management.   Update the service role   In this procedure you update the service role that a Amazon Bedrock Studio workspace uses.  Updating the provisioning role helps enable Prompt flows and Prompt management.   To update the service role   1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://  console.aws.amazon.com/bedrock/.   2. In the left navigation pane, choose Bedrock Studio.   3. In Bedrock Studio workspaces, select the workspace that you want to update.   4. Choose the Overview tab. If the workspace needs an update to support Prompt flows and  Prompt management, you will see an alert banner with steps for enabling Prompt flows and  Prompt management.   Update the service role 1009    https://console.aws.amazon.com/bedrock/  https://console.aws.amazon.com/bedrock/      Amazon Bedrock User Guide   5. In Workspace details, choose the service role ARN in Service role.\"]]\n"
     ]
    }
   ],
   "source": [
    "print(updated_dataset['retrieved_contexts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\n",
      "Answer: Based on the provided context, here's how temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how adjusting them might affect the output when generating text about different types of equines:\n",
      "\n",
      "1. Temperature: \n",
      "- Lower values increase the likelihood of higher-probability tokens and decrease the likelihood of lower-probability tokens. This would make the model more likely to choose \"horses\" in the given example.\n",
      "- Higher values increase the likelihood of lower-probability tokens and decrease the likelihood of higher-probability tokens. This would make the model more likely to consider \"zebras\" or even \"unicorns\" in the example.\n",
      "\n",
      "2. Top K:\n",
      "- A lower Top K value (e.g., 2) would limit the model to consider only the top K most likely candidates. In the example, setting Top K to 2 would make the model only consider \"horses\" and \"zebras,\" excluding \"unicorns.\"\n",
      "- A higher Top K value would allow more lower-probability tokens to be considered.\n",
      "\n",
      "3. Top P:\n",
      "- A lower Top P value (e.g., 0.7) would make the model consider only the tokens that fall within the top percentage of the probability distribution. In the example, setting Top P to 0.7 would result in only \"horses\" being considered.\n",
      "- A higher Top P value (e.g., 0.9) would allow more tokens to be considered. In the example, setting Top P to 0.9 would include both \"horses\" and \"zebras\" in the consideration.\n",
      "\n",
      "When generating text about different types of equines, adjusting these parameters would affect the output as follows:\n",
      "\n",
      "- Lower temperature, lower Top K, and lower Top P would likely result in more predictable and common equine-related responses, focusing primarily on horses.\n",
      "- Higher temperature, higher Top K, and higher Top P would increase the likelihood of the model generating text about less common equines like zebras, and potentially even fictional equines like unicorns.\n",
      "\n",
      "These parameters allow users to control the balance between creativity and predictability in the model's outputs when discussing various types of equines.\n",
      "Answer: Temperature, Top K, and Top P are parameters that work together to control the randomness and diversity of the model's output in Amazon Bedrock's foundation models. Using the equine example:\n",
      "\n",
      "1. Temperature: A higher temperature flattens the probability distribution, increasing the chance of selecting less probable options like \"unicorns\" while decreasing the likelihood of more common choices like \"horses\".\n",
      "\n",
      "2. Top K: This parameter limits the selection to the K most likely candidates. Setting Top K to 2 would only consider \"horses\" and \"zebras\", excluding less probable options like \"unicorns\".\n",
      "\n",
      "3. Top P: This sets a cumulative probability threshold. With Top P at 0.7, only \"horses\" would be considered as it's the only option within the top 70% of the probability distribution. Increasing Top P to 0.9 would include both \"horses\" and \"zebras\".\n",
      "\n",
      "The interaction of these parameters can significantly impact the output. For instance, a high temperature with a low Top K might still produce varied results within the limited selection, while a low temperature with a high Top P could lead to more predictable, common outputs. Balancing these parameters allows for fine-tuning between creativity and coherence in the generated text about equines or any other topic.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/updated_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\")\n",
    "print(f\"Answer: {item['answer']}\")\n",
    "print(f\"Answer: {item['ground_truth']}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, metrics, llm_id, emb_id, region):\n",
    "    \"\"\"\n",
    "    Evaluate the dataset using the specified metrics.\n",
    "\n",
    "    Args:\n",
    "    dataset (List[Dict]): List of dictionaries containing 'user_input', 'response', and 'retrieved_contexts'.\n",
    "    metrics (List[AnswerRelevancy]): List of metric objects to use for evaluation.\n",
    "    llm_id (str): ID of the LLM model to use.\n",
    "    embeddings_id (str): ID of the embeddings model to use.\n",
    "    region (str): AWS region to use for Bedrock.\n",
    "\n",
    "    Returns:\n",
    "    Dict: A dictionary containing the scores for each metric.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, AnswerRelevancy):\n",
    "            metric.llm_id = llm_id\n",
    "            metric.emb_id = emb_id\n",
    "            metric.region = region\n",
    "        if isinstance(metric, Faithfulness):\n",
    "            metric.llm_id = llm_id\n",
    "            metric.region = region\n",
    "        if isinstance(metric, ContextRecall):\n",
    "            metric.llm_id = llm_id\n",
    "            metric.region = region\n",
    "\n",
    "        scores = []\n",
    "        for row in dataset:\n",
    "            try:\n",
    "                score = metric.score(row)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            results[metric.__class__.__name__] = avg_score\n",
    "        else:\n",
    "            results[metric.__class__.__name__] = \"No valid scores\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2093.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n",
      "[1]\n",
      "{'ContextRecall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from libs.eval_metrics import (\n",
    "    AnswerRelevancy, \n",
    "    Faithfulness, \n",
    "    ContextRecall\n",
    ")\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "emb_id = \"amazon.titan-embed-text-v2:0\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "#metrics = [AnswerRelevancy(llm_id=llm_id, emb_id=emb_id, region=region, strictness=1)]\n",
    "#metrics = [Faithfulness(llm_id=llm_id, region=region)]\n",
    "metrics = [ContextRecall(llm_id=llm_id, region=region)]\n",
    "\n",
    "def map_dataset(example):\n",
    "    return {\n",
    "        \"user_input\": example[\"question\"],\n",
    "        \"retrieved_contexts\": example[\"retrieved_contexts\"],\n",
    "        \"referenced_contexts\": example[\"original_contexts\"],\n",
    "        \"response\": example[\"answer\"],\n",
    "        \"reference\": example[\"ground_truth\"]\n",
    "    }\n",
    "\n",
    "dataset = updated_dataset.map(map_dataset).select(range(2))\n",
    "results = evaluate(dataset, metrics, llm_id, emb_id, region)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"data/ragas_evaluation_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[:, \"context_precision\":\"context_recall\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
