{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq ragas==0.1.20\n",
    "# !pip install -Uq langchain==0.2.16 langchain_aws langchain-community>=0.2.41 langchain-core==0.2.41 langchain-experimental==0.0.60\n",
    "# !pip install -Uq nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>question_type</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do temperature, Top K, and Top P parameter...</td>\n",
       "      <td>Temperature, Top K, and Top P are parameters t...</td>\n",
       "      <td>complex</td>\n",
       "      <td>• If you set a high temperature, the probabili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long will Amazon Bedrock support base mode...</td>\n",
       "      <td>Amazon Bedrock will support base models for a ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>• EOL: This version is no longer available for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the system handle a scenario where a ...</td>\n",
       "      <td>The system doesn't explicitly show a function ...</td>\n",
       "      <td>complex</td>\n",
       "      <td>'payment_date': ['2021-10-05', '2021-10-06', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of an S3 retrieval node in...</td>\n",
       "      <td>An S3 retrieval node lets you retrieve data fr...</td>\n",
       "      <td>simple</td>\n",
       "      <td>An S3 retrieval node lets you retrieve data fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can a developer create a new prompt versio...</td>\n",
       "      <td>To create a new prompt version, retrieve its i...</td>\n",
       "      <td>complex</td>\n",
       "      <td>make a CreatePromptVersion Agents for Amazon B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How do temperature, Top K, and Top P parameter...   \n",
       "1  How long will Amazon Bedrock support base mode...   \n",
       "2  How does the system handle a scenario where a ...   \n",
       "3  What is the purpose of an S3 retrieval node in...   \n",
       "4  How can a developer create a new prompt versio...   \n",
       "\n",
       "                                        ground_truth question_type  \\\n",
       "0  Temperature, Top K, and Top P are parameters t...       complex   \n",
       "1  Amazon Bedrock will support base models for a ...        simple   \n",
       "2  The system doesn't explicitly show a function ...       complex   \n",
       "3  An S3 retrieval node lets you retrieve data fr...        simple   \n",
       "4  To create a new prompt version, retrieve its i...       complex   \n",
       "\n",
       "                                            contexts  \n",
       "0  • If you set a high temperature, the probabili...  \n",
       "1  • EOL: This version is no longer available for...  \n",
       "2  'payment_date': ['2021-10-05', '2021-10-06', '...  \n",
       "3  An S3 retrieval node lets you retrieve data fr...  \n",
       "4  make a CreatePromptVersion Agents for Amazon B...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"data/qa_dataset.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "subset_length = 10\n",
    "test_dataset = Dataset.from_pandas(df.head(subset_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3060.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'ground_truth', 'question_type', 'contexts'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+', '', s)\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    return s\n",
    "\n",
    "def convert_to_list(example):\n",
    "    cleaned_context = clean_string(example[\"contexts\"])\n",
    "    try:\n",
    "        contexts = ast.literal_eval(cleaned_context)\n",
    "    except:\n",
    "        contexts = cleaned_context\n",
    "    return {\"contexts\": contexts}\n",
    "\n",
    "test_dataset = test_dataset.map(convert_to_list)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 1, 'content': 'Randomness and diversity 239        Amazon Bedrock User Guide   â€¢ If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and  \"zebras.\"   â€¢ If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that  lies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers  \"horses\" and \"zebras\" as they are in the top 90% of probability distribution.   Length   Foundation models typically support parameters that limit the length of the response. Examples of  these parameters are provided below.   â€¢ Response length â€“ An exact value to specify the minimum or maximum number of tokens to  return in the generated response.   â€¢ Penalties â€“ Specify the degree to which to penalize outputs in a response. Examples include the  following.   â€¢ The length of the response.   â€¢ Repeated tokens in a response.   â€¢ Frequency of tokens in a response.   â€¢ Types of tokens in a response.   â€¢ Stop sequences â€“ Specify sequences of characters that stop the model from generating further  tokens. If the model generates a stop sequence that you specify, it will stop generating after that  sequence.', 'source': {'s3Location': {'uri': 's3://bedrock-manual-kb/bedrock-manual.pdf'}, 'type': 'S3'}, 'score': 0.60214484}\n"
     ]
    }
   ],
   "source": [
    "# RAG implementation sample 1\n",
    "from libs.bedrock_kb_util import context_retrieval_from_kb\n",
    "\n",
    "question = test_dataset[0]['question']\n",
    "search_result = context_retrieval_from_kb(question, 3, 'us-west-2', 'CNDSUOPKAS', 'SEMANTIC')\n",
    "print(search_result[0])\n",
    "\n",
    "contexts = \"\\n--\\n\".join([result['content'] for result in search_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomness and diversity 239        Amazon Bedrock User Guide   â€¢ If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and  \"zebras.\"   â€¢ If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that  lies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers  \"horses\" and \"zebras\" as they are in the top 90% of probability distribution.   Length   Foundation models typically support parameters that limit the length of the response. Examples of  these parameters are provided below.   â€¢ Response length â€“ An exact value to specify the minimum or maximum number of tokens to  return in the generated response.   â€¢ Penalties â€“ Specify the degree to which to penalize outputs in a response. Examples include the  following.   â€¢ The length of the response.   â€¢ Repeated tokens in a response.   â€¢ Frequency of tokens in a response.   â€¢ Types of tokens in a response.   â€¢ Stop sequences â€“ Specify sequences of characters that stop the model from generating further  tokens. If the model generates a stop sequence that you specify, it will stop generating after that  sequence.\n",
      "--\n",
      "The following table summarizes the effects of these parameters.   Parameter Effect of lower value Effect of higher value   Temperature Increase likelihood of higher- probability tokens   Decrease likelihood of lower- probability tokens   Increase likelihood of lower- probability tokens   Decrease likelihood of higher- probability tokens   Top K Remove lower-probability  tokens   Allow lower-probability  tokens   Top P Remove lower-probability  tokens   Allow lower-probability  tokens   As an example to understand these parameters, consider the example prompt I hear the hoof  beats of \". Let's say that the model determines the following three words to be candidates for  the next token. The model also assigns a probability for each word.   {      \"horses\": 0.7,      \"zebras\": 0.2,      \"unicorns\": 0.1 }   â€¢ If you set a high temperature, the probability distribution is flattened and the probabilities  become less different, which would increase the probability of choosing \"unicorns\" and decrease  the probability of choosing \"horses\".   Randomness and diversity 239        Amazon Bedrock User Guide   â€¢ If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and  \"zebras.\"\n",
      "--\n",
      "General guidelines for Amazon Bedrock LLM users 297        Amazon Bedrock User Guide   (Source: Prompt written by AWS)   Use inference parameters   LLMs on Amazon Bedrock all come with several inference parameters that you can set to control  the response from the models. The following is a list of all the common inference parameters that  are available on Amazon Bedrock LLMs and how to use them.   Temperature is a value between 0 and 1, and it regulates the creativity of LLMsâ€™ responses. Use  lower temperature if you want more deterministic responses, and use higher temperature if you  want more creative or different responses for the same prompt from LLMs on Amazon Bedrock. For  all the examples in this prompt guideline, we set temperature = 0.   Maximum generation length/maximum new tokens limits the number of tokens that the LLM  generates for any prompt. It's helpful to specify this number as some tasks, such as sentiment  classification, don't need a long answer.   Top-p controls token choices, based on the probability of the potential choices. If you set Top-p  below 1.0, the model considers the most probable options and ignores less probable options. The  result is more stable and repetitive completions.   Use inference parameters 298        Amazon Bedrock User Guide   End token/end sequence specifies the token that the LLM uses to indicate the end of the output.\n"
     ]
    }
   ],
   "source": [
    "print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "region = 'us-west-2'\n",
    "\n",
    "retry_config = Config(\n",
    "    region_name=region,\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "boto3_client = boto3.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, contexts):\n",
    "    system_prompt = \"\"\"You are an AI assistant that uses retrieved context to answer questions accurately. \n",
    "    Follow these guidelines:\n",
    "    1. Use the provided context to inform your answers.\n",
    "    2. If the context doesn't contain relevant information, say \"I don't have enough information to answer that.\"\n",
    "    3. Be concise and to the point in your responses.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {contexts}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Please answer the question based on the given context.\"\"\"\n",
    "\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{'role': 'user', 'content': [{'text': user_prompt}]}],\n",
    "        system=[{'text': system_prompt}]\n",
    "    )\n",
    "\n",
    "    answer = response['output']['message']['content'][0]['text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'ground_truth', 'question_type', 'contexts'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_item at 0x7fc98bcedf70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     14\u001b[0m     answer \u001b[38;5;241m=\u001b[39m generate_answer(question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(contexts))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m: answer\n\u001b[1;32m     22\u001b[0m     }\n\u001b[0;32m---> 24\u001b[0m updated_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_item\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3035\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3036\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3037\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:3408\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3406\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3408\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:3300\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3299\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3300\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3302\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3303\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3304\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36mprocess_item\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_item\u001b[39m(item):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Prevent throttling\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     question \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m     search_result \u001b[38;5;241m=\u001b[39m context_retrieval_from_kb(question, top_k, kb_region, kb_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEMANTIC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "kb_region = 'us-west-2'\n",
    "kb_id = 'CNDSUOPKAS'\n",
    "top_k = 3\n",
    "\n",
    "def process_item(item):\n",
    "    sleep(5)  # Prevent throttling\n",
    "    question = item['question']\n",
    "    search_result = context_retrieval_from_kb(question, top_k, kb_region, kb_id, 'SEMANTIC')\n",
    "\n",
    "    contexts = [result['content'] for result in search_result]\n",
    "    answer = generate_answer(question, \"\\n--\\n\".join(contexts))\n",
    "\n",
    "    return {\n",
    "        'question': item['question'],\n",
    "        'ground_truth': item['ground_truth'],\n",
    "        'original_contexts': item['contexts'],\n",
    "        'retrieved_contexts': contexts,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "updated_dataset = test_dataset.map(process_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_file = \"data/updated_qa_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in updated_dataset:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/updated_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\")\n",
    "print(f\"Answer: {item['answer']}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\n",
      "Answer: Based on the provided context, here's how temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how adjusting them might affect the output when generating text about different types of equines:\n",
      "\n",
      "1. Temperature: \n",
      "- Lower values increase the likelihood of higher-probability tokens and decrease the likelihood of lower-probability tokens. This would make the model more likely to choose \"horses\" in the given example.\n",
      "- Higher values increase the likelihood of lower-probability tokens and decrease the likelihood of higher-probability tokens. This would make the model more likely to consider \"zebras\" or even \"unicorns\" in the example.\n",
      "\n",
      "2. Top K:\n",
      "- A lower Top K value (e.g., 2) would limit the model to consider only the top K most likely candidates. In the example, setting Top K to 2 would make the model only consider \"horses\" and \"zebras,\" excluding \"unicorns.\"\n",
      "- A higher Top K value would allow more lower-probability tokens to be considered.\n",
      "\n",
      "3. Top P:\n",
      "- A lower Top P value (e.g., 0.7) would make the model consider only the tokens that fall within the top percentage of the probability distribution. In the example, setting Top P to 0.7 would result in only \"horses\" being considered.\n",
      "- A higher Top P value (e.g., 0.9) would allow more tokens to be considered. In the example, setting Top P to 0.9 would include both \"horses\" and \"zebras\" in the consideration.\n",
      "\n",
      "When generating text about different types of equines, adjusting these parameters would affect the output as follows:\n",
      "\n",
      "- Lower temperature, lower Top K, and lower Top P would likely result in more predictable and common equine-related responses, focusing primarily on horses.\n",
      "- Higher temperature, higher Top K, and higher Top P would increase the likelihood of the model generating text about less common equines like zebras, and potentially even fictional equines like unicorns.\n",
      "\n",
      "These parameters allow users to control the balance between creativity and predictability in the model's outputs when discussing various types of equines.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_file = \"data/updated_qa_dataset.jsonl\"\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "\n",
    "updated_dataset = Dataset.from_list(list(read_jsonl(input_file)))\n",
    "\n",
    "item = updated_dataset[0]\n",
    "print(f\"Question: {item['question']}\")\n",
    "print(f\"Answer: {item['answer']}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, metrics, llm_id, emb_id, region):\n",
    "    \"\"\"\n",
    "    Evaluate the dataset using the specified metrics.\n",
    "\n",
    "    Args:\n",
    "    dataset (List[Dict]): List of dictionaries containing 'user_input', 'response', and 'retrieved_contexts'.\n",
    "    metrics (List[AnswerRelevancy]): List of metric objects to use for evaluation.\n",
    "    llm_id (str): ID of the LLM model to use.\n",
    "    embeddings_id (str): ID of the embeddings model to use.\n",
    "    region (str): AWS region to use for Bedrock.\n",
    "\n",
    "    Returns:\n",
    "    Dict: A dictionary containing the scores for each metric.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, AnswerRelevancy):\n",
    "            metric.llm_id = llm_id\n",
    "            metric.emb_id = emb_id\n",
    "            metric.region = region\n",
    "        if isinstance(metric, Faithfulness):\n",
    "            metric.llm_id = llm_id\n",
    "            metric.region = region\n",
    "\n",
    "        scores = []\n",
    "        for row in dataset:\n",
    "            try:\n",
    "                score = metric.score(row)\n",
    "                scores.append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            results[metric.__class__.__name__] = avg_score\n",
    "        else:\n",
    "            results[metric.__class__.__name__] = \"No valid scores\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1900.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0]\n",
      "[1]\n",
      "{'Faithfulness': 0.9285714285714286}\n"
     ]
    }
   ],
   "source": [
    "from libs.eval_metrics import AnswerRelevancy, Faithfulness\n",
    "\n",
    "llm_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "emb_id = \"amazon.titan-embed-text-v2:0\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "#metrics = [AnswerRelevancy(llm_id=llm_id, emb_id=emb_id, region=region, strictness=1)]\n",
    "metrics = [Faithfulness(llm_id=llm_id, region=region)]\n",
    "\n",
    "def map_dataset(example):\n",
    "    return {\n",
    "        \"user_input\": example[\"question\"],\n",
    "        \"retrieved_contexts\": example[\"retrieved_contexts\"],\n",
    "        \"referenced_contexts\": example[\"original_contexts\"],\n",
    "        \"response\": example[\"answer\"],\n",
    "        \"reference\": example[\"ground_truth\"]\n",
    "    }\n",
    "\n",
    "dataset = updated_dataset.map(map_dataset).select(range(2))\n",
    "results = evaluate(dataset, metrics, llm_id, emb_id, region)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"data/ragas_evaluation_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[:, \"context_precision\":\"context_recall\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
