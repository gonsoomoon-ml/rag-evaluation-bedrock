{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Test Datasets\n",
    "\n",
    "**Why use Synthetic Test Datasets?**\n",
    "\n",
    "Evaluating the performance of RAG (Retrieval-Augmented Generation) augmented pipelines is crucial.\n",
    "\n",
    "However, manually creating hundreds of QA (Question-Answer-Context) samples from documents can be time-consuming and labor-intensive. Additionally, human-generated questions may struggle to reach the level of complexity needed for thorough evaluation, ultimately affecting the quality of the assessment.\n",
    "\n",
    "Using synthetic data generation can reduce developer time in the data aggregation process **by up to 90%**.\n",
    "\n",
    "    RAGAS: https://docs.ragas.io/en/latest/concepts/testset_generation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class PDFLoader:\n",
    "    def __init__(self, file_path: str, start_page: int = None, end_page: int = None):\n",
    "        self.file_path = file_path\n",
    "        self.start_page = start_page\n",
    "        self.end_page = end_page\n",
    "\n",
    "    def load(self) -> Dict[str, Any]:\n",
    "        combined_text = \"\"\n",
    "        metadata = {}\n",
    "\n",
    "        with pdfplumber.open(self.file_path) as pdf:\n",
    "            total_pages = len(pdf.pages)\n",
    "\n",
    "            start = (self.start_page or 1) - 1\n",
    "            end = min(self.end_page or total_pages, total_pages)\n",
    "\n",
    "            for page_num in range(start, end):\n",
    "                page = pdf.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                combined_text += text + \"\\n\"\n",
    "\n",
    "            metadata = {\n",
    "                \"source\": self.file_path,\n",
    "                \"filename\": self.file_path,\n",
    "                \"total_pages\": total_pages,\n",
    "                \"extracted_pages\": f\"{start + 1}-{end}\"\n",
    "            }\n",
    "\n",
    "            for key, value in pdf.metadata.items():\n",
    "                if isinstance(value, (str, int)):\n",
    "                    metadata[key] = value\n",
    "\n",
    "        return {\n",
    "            \"page_content\": combined_text.strip(),\n",
    "            \"metadata\": metadata\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Used for Practice\n",
    "\n",
    "Amazon Bedrock Manual Documentation (https://docs.aws.amazon.com/bedrock/latest/userguide/)\n",
    "\n",
    "- Link: https://d1jp7kj5nqor8j.cloudfront.net/bedrock-manual.pdf\n",
    "- File name: `bedrock-manual.pdf`\n",
    "\n",
    "_Please copy the downloaded file to the data folder for the practice session_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFLoader(\"data/bedrock-manual.pdf\", start_page=16, end_page=1574)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def split_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100, separators: Optional[List[str]] = None) -> List[str]:\n",
    "    separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "\n",
    "    def _split_text_recursive(text: str, separators: List[str]) -> List[str]:\n",
    "        if not separators:\n",
    "            return [text]\n",
    "\n",
    "        separator = separators[0]\n",
    "        splits = re.split(f\"({re.escape(separator)})\", text)\n",
    "        splits = [\"\".join(splits[i:i+2]) for i in range(0, len(splits), 2)]\n",
    "\n",
    "        final_chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for split in splits:\n",
    "            if len(current_chunk) + len(split) <= chunk_size:\n",
    "                current_chunk += split\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    final_chunks.append(current_chunk)\n",
    "                if len(split) > chunk_size:\n",
    "                    subsplits = _split_text_recursive(split, separators[1:])\n",
    "                    final_chunks.extend(subsplits)\n",
    "                else:\n",
    "                    current_chunk = split\n",
    "\n",
    "        if current_chunk:\n",
    "            final_chunks.append(current_chunk)\n",
    "\n",
    "        return final_chunks\n",
    "\n",
    "    chunks = _split_text_recursive(text, separators)\n",
    "\n",
    "    if chunk_overlap > 0:\n",
    "        overlapped_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                overlapped_chunks.append(chunk)\n",
    "            else:\n",
    "                overlap_text = chunks[i-1][-chunk_overlap:]\n",
    "                overlapped_chunks.append(overlap_text + chunk)\n",
    "        chunks = overlapped_chunks\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2136"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_text(docs['page_content'], 1000, 0)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_with_metadata = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunks_with_metadata.append({\n",
    "        'content': chunk,\n",
    "        'metadata': {\n",
    "            'chunk_id': i,\n",
    "            'filename': docs['metadata'].get('filename', 'unknown')\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"Amazon Bedrock User Guide\\nWhat is Amazon Bedrock?\\nAmazon Bedrock is a fully managed service that makes high-performing foundation models (FMs)\\nfrom leading AI startups and Amazon available for your use through a unified API. You can choose\\nfrom a wide range of foundation models to find the model that is best suited for your use case.\\nAmazon Bedrock also offers a broad set of capabilities to build generative AI applications with\\nsecurity, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and\\nevaluate top foundation models for your use cases, privately customize them with your data using\\ntechniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that\\nexecute tasks using your enterprise systems and data sources.\\nWith Amazon Bedrock's serverless experience, you can get started quickly, privately customize\\nfoundation models with your own data, and easily and securely integrate and deploy them into\\n\",\n",
       " 'metadata': {'chunk_id': 0, 'filename': 'data/bedrock-manual.pdf'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_with_metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Q&A Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region = 'us-west-2'\n",
    "retry_config = Config(\n",
    "    region_name=region,\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "boto3_client = boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "def converse_with_bedrock(model_id, sys_prompt, usr_prompt):\n",
    "    temperature = 0.5\n",
    "    top_p = 0.9\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def create_prompt(sys_template, user_template):\n",
    "    sys_prompt = [{\"text\": sys_template}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template}]}]\n",
    "    return sys_prompt, usr_prompt\n",
    "\n",
    "def get_context_chunks(chunks_with_metadata, start_id):\n",
    "    context_chunks = [\n",
    "        chunks_with_metadata[start_id]['content'],\n",
    "        chunks_with_metadata[start_id + 1]['content'],\n",
    "        chunks_with_metadata[start_id + 2]['content']\n",
    "    ]\n",
    "    return \" \".join(context_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Use \n",
    "\n",
    "LLM will generate Q&A dataset that conforms to the schema description in the tooluse config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"QuestionAnswerGenerator\",\n",
    "                \"description\": \"Generates questions and answers based on the given context.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"question\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The generated question\"\n",
    "                            },\n",
    "                            \"answer\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The answer to the generated question\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"question\", \"answer\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse_with_bedrock_tools(sys_prompt, usr_prompt, tool_config):\n",
    "    temperature = 0.0\n",
    "    top_p = 0.1\n",
    "    top_k = 1\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "        messages=usr_prompt,\n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def parse_tool_use(message):\n",
    "    stop_reason = message['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        tool_requests = message['output']['message']['content']\n",
    "        for tool_request in tool_requests:\n",
    "            if 'toolUse' in tool_request:\n",
    "                tool = tool_request['toolUse']\n",
    "\n",
    "                if tool['name'] == 'QuestionAnswerGenerator':\n",
    "                    return tool['input']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A Dataset Generation Instruction\n",
    "\n",
    "- `simple`: directly answerable questions from the given context\n",
    "- `complex`: reasoning questions and answers.\n",
    "\n",
    "_Modify the system/user prompts tailored to your dataset_\n",
    "\n",
    "Generated Q&A pair will be stored in `data/qa_dataset.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_dataset(chunks, num_pairs=5, output_file=\"data/qa_dataset.jsonl\"):\n",
    "    total_chunks = len(chunks)\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        start_id = random.randint(0, total_chunks - 3)\n",
    "        context = get_context_chunks(chunks_with_metadata, start_id)\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            sys_template = \"\"\"\n",
    "            You are an expert at generating practical questions based on given documentation.\n",
    "            Your task is to generate complex, reasoning questions and answers.\n",
    "\n",
    "            Follow these rules:\n",
    "            1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
    "            2. Ensure questions are relevant, concise, preferably under 25 words, and fully answerable with the provided information\n",
    "            3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
    "            4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
    "            5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
    "            6. When generating answers, focus on providing only the necessary information directly related to the question. Keep answers concise and to the point.\n",
    "            \"\"\"\n",
    "            question_type = \"complex\"\n",
    "        else:\n",
    "            sys_template = \"\"\"\n",
    "            You are an expert at generating practical questions based on given documentation.\n",
    "            Your task is to create simple, directly answerable questions from the given context.\n",
    "\n",
    "            Follow these rules:\n",
    "            1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
    "            2. Ensure questions are relevant, concise, preferably under 10 words, and fully answerable with the provided information\n",
    "            3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
    "            4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
    "            5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
    "            6. When generating answers, provide only the essential information that directly addresses the question. Keep answers brief and to the point.\n",
    "            \"\"\"\n",
    "            question_type = \"simple\"\n",
    "\n",
    "        user_template = f\"\"\"\n",
    "        Generate a {question_type} question and its answer based on the following context:\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Use the QuestionAnswerGenerator tool to provide the output.\n",
    "        \"\"\"\n",
    "\n",
    "        sys_prompt, user_prompt = create_prompt(sys_template, user_template)\n",
    "        response = converse_with_bedrock_tools(sys_prompt, user_prompt, tool_config)\n",
    "        qa_data = parse_tool_use(response)\n",
    "\n",
    "        if qa_data:\n",
    "            qa_item = {\n",
    "                \"question\": qa_data[\"question\"],\n",
    "                \"ground_truth\": qa_data[\"answer\"],\n",
    "                \"question_type\": question_type,\n",
    "                \"contexts\": context\n",
    "            }\n",
    "\n",
    "            print(qa_item)\n",
    "\n",
    "            with open(output_file, 'a') as f:\n",
    "                json.dump(qa_item, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "            dataset.append(qa_item)\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\", 'ground_truth': 'Temperature, Top K, and Top P are parameters that work together to control the randomness and diversity of the model\\'s output in Amazon Bedrock\\'s foundation models. Using the equine example:\\n\\n1. Temperature: A higher temperature flattens the probability distribution, increasing the chance of selecting less probable options like \"unicorns\" while decreasing the likelihood of more common choices like \"horses\".\\n\\n2. Top K: This parameter limits the selection to the K most likely candidates. Setting Top K to 2 would only consider \"horses\" and \"zebras\", excluding less probable options like \"unicorns\".\\n\\n3. Top P: This sets a cumulative probability threshold. With Top P at 0.7, only \"horses\" would be considered as it\\'s the only option within the top 70% of the probability distribution. Increasing Top P to 0.9 would include both \"horses\" and \"zebras\".\\n\\nThe interaction of these parameters can significantly impact the output. For instance, a high temperature with a low Top K might still produce varied results within the limited selection, while a low temperature with a high Top P could lead to more predictable, common outputs. Balancing these parameters allows for fine-tuning between creativity and coherence in the generated text about equines or any other topic.', 'question_type': 'complex', 'context': '• If you set a high temperature, the probability distribution is flattened and the probabilities\\nbecome less different, which would increase the probability of choosing \"unicorns\" and decrease\\nthe probability of choosing \"horses\".\\nRandomness and diversity 239\\nAmazon Bedrock User Guide\\n• If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and\\n\"zebras.\"\\n• If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that\\nlies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers\\n\"horses\" and \"zebras\" as they are in the top 90% of probability distribution.\\nLength\\nFoundation models typically support parameters that limit the length of the response. Examples of\\nthese parameters are provided below.\\n• Response length – An exact value to specify the minimum or maximum number of tokens to\\nreturn in the generated response.\\n • Penalties – Specify the degree to which to penalize outputs in a response. Examples include the\\nfollowing.\\n• The length of the response.\\n• Repeated tokens in a response.\\n• Frequency of tokens in a response.\\n• Types of tokens in a response.\\n• Stop sequences – Specify sequences of characters that stop the model from generating further\\ntokens. If the model generates a stop sequence that you specify, it will stop generating after that\\nsequence.\\nPlaygrounds\\nImportant\\nBefore you can use any of the foundation models, you must request access to that model\\nthrough the Amazon Bedrock console. You can manage model access only through the\\nconsole. If you try to use the model (with the API or within the console) before you have\\nrequested access to it, you\\'ll receive an error message. For more information, see Manage\\naccess to Amazon Bedrock foundation models.\\nLength 240\\nAmazon Bedrock User Guide\\nThe Amazon Bedrock playgrounds provide you a console environment to experiment with running\\n inference on different models and with different configurations, before deciding to use them in an\\napplication. In the console, access the playgrounds by choosing Playgrounds in the left navigation\\npane. You can also navigate directly to the playground when you choose a model from a model\\ndetails page or the examples page.\\nThere are playgrounds for text, chat, and image models.\\nWithin each playground you can enter prompts and experiment with inference parameters.\\nPrompts are usually one or more sentences of text that set up a scenario, question, or task for a\\nmodel. For information about creating prompts, see Prompt engineering guidelines.\\nInference parameters influence the response generated by a model, such as the randomness of\\ngenerated text. When you load a model into a playground, the playground configures the model\\nwith its default inference settings. You can change and reset the settings as you experiment\\n'}\n",
      "{'question': 'How long will Amazon Bedrock support base models after launch in a region?', 'ground_truth': 'Amazon Bedrock will support base models for a minimum of 12 months from the launch in a region.', 'question_type': 'simple', 'context': \"• EOL: This version is no longer available for use. Any requests made to this version will fail.\\nWe will support base models for a minimum of 12 months from the launch in a region. We will\\nalways give customers 6 months of notice before we mark the model EOL. Model will be marked\\nLegacy on the date when the EOL notice is sent out. The console marks a model version's state as\\nActive or Legacy. When you make a GetFoundationModelor ListFoundationModels call, you can\\nfind the state of the model in the modelLifecycle field in the response. While you can continue\\nto use a Legacy version, you should plan to transition to an Active version before the EOL date.\\nOn-Demand, Provisioned Throughput, and model customization\\nYou specify the version of a model when you use it in On-Demand mode (for example,\\nanthropic.claude-v2, anthropic.claude-v2:1, etc.).\\nWhen you configure Provisioned Throughput, you must specify a model version that will remain\\nunchanged for the entire term.\\nModel lifecycle 53\\n Amazon Bedrock User Guide\\nIf you customized a model, you can continue to use it until the EOL date of the base model version\\nthat you used for customization. You can also customize a legacy model version, but you should\\nplan to migrate before it reaches its EOL date.\\nNote\\nService quotas are shared among model minor versions.\\nLegacy versions\\nThe following table shows the legacy versions of models available on Amazon Bedrock.\\nModel version Legacy date EOL date Recommended Recommended\\nmodel version model ID\\nreplacement\\nStable Diffusion February 2, April 30, 2024 Stable Diffusion stability.stable-d\\nXL 0.8 2024 XL 1.x iffusion-xl-v1\\nClaude v1.3 November 28, February 28, Claude v2.1 anthropic\\n2023 2024 .claude-v2:1\\nTitan November 7, February 15, Titan amazon.titan-\\nEmbeddings - 2023 2024 Embeddings - embed-text-v1\\nText v1.1 Text v1.2\\nMeta Llama 2 May 12, 2024 October 30, Meta Llama3 meta.llam\\n13b-chat-v1, 2024 and Meta a3-1-8b-i\\nMeta Llama 2 Llama3.1 nstruct-v1,\\n70b-chat-v1, meta.llam\\n Meta Llama a3-1-70b-\\n2-13b, Meta instruct-v1,\\nLlama 2-70b meta.llam\\na3-1-405b-\\ninstruct-v1,\\nmeta.llama3-8b-\\ninstruct-v1,\\nLegacy versions 54\\nAmazon Bedrock User Guide\\nModel version Legacy date EOL date Recommended Recommended\\nmodel version model ID\\nreplacement\\nand meta.llam\\na3-70b-instruct-\\nv1\\nAi21 J2 Mid- April 30, 2024 August 31, 2024 N/A N/A\\nv1 and Ai21 J2 (only in us- (only in us-\\nUltra-v1 west-2) west-2)\\nAmazon Bedrock model IDs\\nMany Amazon Bedrock API operations require the use of a model ID. Refer to the following table to\\ndetermine where to find the model ID that you need to use.\\nUse case How to find the model ID\\nUse a base model Look up the ID in the base model IDs chart\\nPurchase Provisioned Throughput for a base Look up the ID in the model IDs for Provision\\nmodel ed Throughput chart and use it as the\\nmodelId in the CreateProvisionedModelThrou\\nghput request.\\nPurchase Provisioned Throughput for a Use the name of the custom model or its ARN\\n\"}\n",
      "{'question': \"How does the system handle a scenario where a transaction status changes from 'Pending' to 'Paid', and what functions would be involved in updating and retrieving this information?\", 'ground_truth': \"The system doesn't explicitly show a function for updating transaction statuses, but it does provide functions for retrieving payment status and date. If a transaction status changes from 'Pending' to 'Paid', the underlying data in the DataFrame 'df' would need to be updated first. Then, to reflect and retrieve this change, two functions would be involved:\\n\\n1. retrieve_payment_status(df, transaction_id): This function would return the updated status 'Paid' for the given transaction_id.\\n\\n2. retrieve_payment_date(df, transaction_id): This function would return the date when the payment was made, which would likely be updated when the status changes to 'Paid'.\\n\\nThese functions are wrapped in a tool-calling system that uses Amazon Bedrock with a Mistral AI model. The invoke_bedrock_mistral_tool() function would be used to call these functions through the AI model, interpreting user queries and returning the appropriate information. The system uses JSON for data exchange and includes error handling for cases where a transaction ID is not found.\", 'question_type': 'complex', 'context': '\\'payment_date\\': [\\'2021-10-05\\', \\'2021-10-06\\', \\'2021-10-07\\', \\'2021-10-05\\',\\n\\'2021-10-08\\'],\\n\\'payment_status\\': [\\'Paid\\', \\'Unpaid\\', \\'Paid\\', \\'Paid\\', \\'Pending\\']\\n}\\n# Create DataFrame\\ndf = pd.DataFrame(data)\\ndef retrieve_payment_status(df: data, transaction_id: str) -> str:\\nif transaction_id in df.transaction_id.values:\\nreturn json.dumps({\\'status\\': df[df.transaction_id ==\\ntransaction_id].payment_status.item()})\\nreturn json.dumps({\\'error\\': \\'transaction id not found.\\'})\\nMistral AI models 198\\nAmazon Bedrock User Guide\\ndef retrieve_payment_date(df: data, transaction_id: str) -> str:\\nif transaction_id in df.transaction_id.values:\\nreturn json.dumps({\\'date\\': df[df.transaction_id ==\\ntransaction_id].payment_date.item()})\\nreturn json.dumps({\\'error\\': \\'transaction id not found.\\'})\\ntools = [\\n{\\n\"type\": \"function\",\\n\"function\": {\\n\"name\": \"retrieve_payment_status\",\\n\"description\": \"Get payment status of a transaction\",\\n\"parameters\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"transaction_id\": {\\n\"type\": \"string\",\\n \"description\": \"The transaction id.\",\\n}\\n},\\n\"required\": [\"transaction_id\"],\\n},\\n},\\n},\\n{\\n\"type\": \"function\",\\n\"function\": {\\n\"name\": \"retrieve_payment_date\",\\n\"description\": \"Get payment date of a transaction\",\\n\"parameters\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"transaction_id\": {\\n\"type\": \"string\",\\n\"description\": \"The transaction id.\",\\n}\\n},\\n\"required\": [\"transaction_id\"],\\n},\\n},\\n}\\n]\\nnames_to_functions = {\\nMistral AI models 199\\nAmazon Bedrock User Guide\\n\\'retrieve_payment_status\\': functools.partial(retrieve_payment_status, df=df),\\n\\'retrieve_payment_date\\': functools.partial(retrieve_payment_date, df=df)\\n}\\ntest_tool_input = \"What\\'s the status of my transaction T1001?\"\\nmessage = [{\"role\": \"user\", \"content\": test_tool_input}]\\ndef invoke_bedrock_mistral_tool():\\nmistral_params = {\\n\"body\": json.dumps({\\n\"messages\": message,\\n\"tools\": tools\\n}),\\n\"modelId\":\"mistral.mistral-large-2407-v1:0\",\\n}\\nresponse = bedrock.invoke_model(**mistral_params)\\nbody = response.get(\\'body\\').read().decode(\\'utf-8\\')\\n body = json.loads(body)\\nchoices = body.get(\"choices\")\\nmessage.append(choices[0].get(\"message\"))\\ntool_call = choices[0].get(\"message\").get(\"tool_calls\")[0]\\nfunction_name = tool_call.get(\"function\").get(\"name\")\\nfunction_params = json.loads(tool_call.get(\"function\").get(\"arguments\"))\\nprint(\"\\\\nfunction_name: \", function_name, \"\\\\nfunction_params: \",\\nfunction_params)\\nfunction_result = names_to_functions[function_name](**function_params)\\nmessage.append({\"role\": \"tool\", \"content\": function_result,\\n\"tool_call_id\":tool_call.get(\"id\")})\\nnew_mistral_params = {\\n\"body\": json.dumps({\\n\"messages\": message,\\n\"tools\": tools\\n}),\\n\"modelId\":\"mistral.mistral-large-2407-v1:0\",\\n}\\nresponse = bedrock.invoke_model(**new_mistral_params)\\nbody = response.get(\\'body\\').read().decode(\\'utf-8\\')\\nMistral AI models 200\\nAmazon Bedrock User Guide\\nbody = json.loads(body)\\nprint(body)\\ninvoke_bedrock_mistral_tool()\\nStability.ai Diffusion models\\n'}\n",
      "{'question': 'What is the purpose of an S3 retrieval node in a prompt flow?', 'ground_truth': 'An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. It takes an object key as input and returns the content of the S3 location as output.', 'question_type': 'simple', 'context': 'An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. In\\nthe configuration, you specify the S3 bucket from which to retrieve data. The input into the node is\\nthe object key. The node returns the content in the S3 location as the output.\\nNote\\nCurrently, the data in the S3 location must be a UTF-8 encoded string.\\nThe following shows the general structure of an S3 retrieval FlowNode object:\\n{\\n\"name\": \"string\",\\n\"type\": \"Retrieval\",\\n\"inputs\": [\\n{\\n\"name\": \"objectKey\",\\n\"type\": \"String\",\\n\"expression\": \"string\"\\n}\\n],\\n\"outputs\": [\\n{\\n\"name\": \"s3Content\",\\n\"type\": \"String\"\\n}\\n],\\n\"configuration\": {\\n\"retrieval\": {\\n\"serviceConfiguration\": {\\n\"s3\": {\\n\"bucketName\": \"string\"\\n}\\n}\\n}\\n}\\n}\\nNode types in prompt flow 846\\nAmazon Bedrock User Guide\\nLambda function node\\nA Lambda function node lets you call a Lambda function in which you can define code to carry out\\nbusiness logic. When you include a Lambda node in a prompt flow, Amazon Bedrock sends an input\\n event to the Lambda function that you specify.\\nIn the configuration, specify the Amazon Resource Name (ARN) of the Lambda function. Define\\ninputs to send in the Lambda input event. You can write code based on these inputs and define\\nwhat the function returns. The function response is returned in the output.\\nThe following shows the general structure of a Λ function FlowNode object:\\n{\\n\"name\": \"string\",\\n\"type\": \"LambdaFunction\",\\n\"inputs\": [\\n{\\n\"name\": \"string\",\\n\"type\": \"String | Number | Boolean | Object | Array\",\\n\"expression\": \"string\"\\n},\\n...\\n],\\n\"outputs\": [\\n{\\n\"name\": \"functionResponse\",\\n\"type\": \"String | Number | Boolean | Object | Array\"\\n}\\n],\\n\"configuration\": {\\n\"lambdaFunction\": {\\n\"lambdaArn\": \"string\"\\n}\\n}\\n}\\nLambda input event for a prompt flow\\nThe input event sent to a Lambda function in a Lambda node is of the following format:\\n{\\n\"messageVersion\": \"1.0\",\\nNode types in prompt flow 847\\nAmazon Bedrock User Guide\\n\"flow\": {\\n\"flowArn\": \"string\",\\n\"flowAliasArn\": \"string\"\\n},\\n\"node\": {\\n \"name\": \"string\",\\n\"nodeInputs\": [\\n{\\n\"name\": \"string\",\\n\"type\": \"String | Number | Boolean | Object | Array\",\\n\"expression\": \"string\",\\n\"value\": ...\\n},\\n...\\n]\\n}\\n}\\nThe fields for each input match the fields that you specify when defining the Lambda node, while\\nthe value of the value field is populated with the whole input into the node after being resolved\\nby the expression. For example, if the whole input into the node is [1, 2, 3] and the expression\\nis $.data[1], the value sent in the input event to the Lambda function would be 2.\\nFor more information about events in Lambda, see Lambda concepts in the AWS Lambda\\nDeveloper Guide.\\nLambda response for a prompt flow\\nWhen you write a Lambda function, you define the response returned by it. This response is\\nreturned to your prompt flow as the output of the Lambda node.\\nLex node\\nA Lex node lets you call a Amazon Lex bot to process an utterance using natural language\\n'}\n",
      "{'question': \"How can a developer create a new prompt version, retrieve its information, and incorporate it into a prompt flow using Amazon Bedrock's Python SDK?\", 'ground_truth': 'To create a new prompt version, retrieve its information, and incorporate it into a prompt flow using Amazon Bedrock\\'s Python SDK, a developer should follow these steps:\\n\\n1. Create a new prompt version using the create_prompt_version() method, specifying the prompt_id.\\n2. Retrieve the version number and ARN from the response.\\n3. List all versions of the prompt using list_prompts() with the prompt_id.\\n4. Get detailed information about the specific version using get_prompt() with prompt_id and prompt_version.\\n5. To incorporate the prompt into a flow:\\n   a. Create a boto3 client for \\'bedrock-agent\\'.\\n   b. Define the flow\\'s structure using Python dictionaries for input, prompt, and output nodes.\\n   c. In the prompt node configuration, set the promptArn to the ARN of the created prompt version.\\n   d. Ensure the prompt node\\'s inputs match the expected variables (e.g., \"genre\" and \"number\").\\n   e. Set up the output node to return the model\\'s completion as a string.\\n\\nThis process allows for version control of prompts and their seamless integration into prompt flows.', 'question_type': 'complex', 'context': 'make a CreatePromptVersion Agents for Amazon Bedrock build-time endpoint:\\n# Create a version of the prompt that you created\\nresponse = client.create_prompt_version(promptIdentifier=prompt_id)\\nprompt_version = response.get(\"version\")\\nprompt_version_arn = response.get(\"arn\")\\n5. View information about the prompt version that you just created, alongside information\\nabout the draft version, by running the following code snippet to make a ListPrompts\\nAgents for Amazon Bedrock build-time endpoint:\\n# List versions of the prompt that you just created\\nclient.list_prompts(promptIdentifier=prompt_id)\\n6. View information for the prompt version that you just created by running the following\\ncode snippet to make a GetPrompt Agents for Amazon Bedrock build-time endpoint:\\n# Get information about the prompt version that you created\\nclient.get_prompt(\\npromptIdentifier=prompt_id,\\npromptVersion=prompt_version\\n)\\n7. Test the prompt by adding it to a prompt flow by following the steps at Run Prompt flows\\n code samples. In the first step when you create the flow, run the following code snippet\\ninstead to use the prompt that you created instead of defining an inline prompt in the flow\\n(replace the ARN of the prompt version in the promptARN field with the ARN of the version\\nof the prompt that you created):\\n# Import Python SDK and create client\\nimport boto3\\nclient = boto3.client(service_name=\\'bedrock-agent\\')\\nFLOWS_SERVICE_ROLE = \"arn:aws:iam::123456789012:role/MyPromptFlowsRole\" #\\nPrompt flows service role that you created. For more information, see https://\\ndocs.aws.amazon.com/bedrock/latest/userguide/flows-permissions.html\\nPROMPT_ARN = prompt_version_arn # ARN of the prompt that you created, retrieved\\nprogramatically during creation.\\nRun code samples 351\\nAmazon Bedrock User Guide\\n# Define each node\\n# The input node validates that the content of the InvokeFlow request is a JSON\\nobject.\\ninput_node = {\\n\"type\": \"Input\",\\n\"name\": \"FlowInput\",\\n\"outputs\": [\\n{\\n\"name\": \"document\",\\n\"type\": \"Object\"\\n }\\n]\\n}\\n# This prompt node contains a prompt that you defined in Prompt management.\\n# It validates that the input is a JSON object that minimally contains the\\nfields \"genre\" and \"number\", which it will map to the prompt variables.\\n# The output must be named \"modelCompletion\" and be of the type \"String\".\\nprompt_node = {\\n\"type\": \"Prompt\",\\n\"name\": \"MakePlaylist\",\\n\"configuration\": {\\n\"prompt\": {\\n\"sourceConfiguration\": {\\n\"resource\": {\\n\"promptArn\": \"\"\\n}\\n}\\n}\\n},\\n\"inputs\": [\\n{\\n\"name\": \"genre\",\\n\"type\": \"String\",\\n\"expression\": \"$.data.genre\"\\n},\\n{\\n\"name\": \"number\",\\n\"type\": \"Number\",\\n\"expression\": \"$.data.number\"\\n}\\n],\\nRun code samples 352\\nAmazon Bedrock User Guide\\n\"outputs\": [\\n{\\n\"name\": \"modelCompletion\",\\n\"type\": \"String\"\\n}\\n]\\n}\\n# The output node validates that the output from the last node is a string and\\nreturns it as is. The name must be \"document\".\\noutput_node = {\\n\"type\": \"Output\",\\n\"name\": \"FlowOutput\",\\n\"inputs\": [\\n{\\n\"name\": \"document\",\\n\"type\": \"String\",\\n\"expression\": \"$.data\"\\n}\\n]\\n}\\n'}\n",
      "{'question': 'What metric is used to evaluate toxicity in general text generation tasks?', 'ground_truth': 'Toxicity is used as a metric to evaluate toxicity in general text generation tasks, using the RealToxicPrompts dataset.', 'question_type': 'simple', 'context': \"2. In the navigation pane, choose Model evaluation.\\n3. In the Model Evaluation Jobs card, you can find a table that lists the model evaluation jobs\\nyou have already created.\\n4. Select the radio button next to your job's name.\\n5. Then, choose Stop evaluation.\\nAWS CLI\\nIn the AWS CLI, you can use the help command to view parameters are required, and which\\nparameters are optional when using list-evaluation-jobs.\\nList model evaluation jobs 457\\nAmazon Bedrock User Guide\\naws bedrock list-evaluation-jobs help\\nThe follow is an example of using list-evaluation-jobs and specifying that maximum of\\n5 jobs be returned. By default jobs are returned in descending order from the time when they\\nwhere started.\\naws bedrock list-evaluation-jobs --max-items 5\\nSDK for Python\\nThe following examples show how to use the AWS SDK for Python to find a model evaluation\\njob you have previously created.\\nimport boto3\\nclient = boto3.client('bedrock')\\njob_request = client.list_evaluation_jobs(maxResults=20)\\n print (job_request)\\nModel evaluation tasks\\nIn a model evaluation job, an evaluation task ( taskType ) is a task you want the model\\nto perform based on information in your prompts. You can choose one task type per model\\nevaluation job.\\nThe following topics to learn more about each task type. Each topic also includes a list of available\\nbuilt-in datasets and their corresponding metrics that can be used only in automatic model\\nevaluation jobs.\\nThe following table summarizes available tasks types, built-in datasets, and computer metrics for\\neach task type.\\nModel evaluation tasks 458\\nAmazon Bedrock User Guide\\nAvailable built-in datasets for automatic model evaluation jobs in Amazon Bedrock\\nTask type MetricBuilt-in Computed metric\\ndatasets\\nGeneral text AccuraTcRyEX Real world knowledge (RWK) score\\ngeneration\\nRobusBtnOeLsD Word error rate\\ns\\nTREX\\nWikiText2\\nToxicitRyealToxic Toxicity\\nityPrompts\\nBOLD\\nText AccuraGciygaword BERTScore\\nsummarization\\nToxicitGyigaword Toxicity\\n RobusGtnigeas word BERTScore and deltaBERTScore\\ns\\nQuestion and AccuraBcoyolQ NLP-F1\\nanswer\\nNaturalQu\\nestions\\nTriviaQA\\nRobusBtnoeosl Q F1 and deltaF1\\ns\\nNaturalQu\\nestions\\nTriviaQA\\nToxicitByoolQ Toxicity\\nModel evaluation tasks 459\\nAmazon Bedrock User Guide\\nTask type MetricBuilt-in Computed metric\\ndatasets\\nNaturalQu\\nestions\\nTriviaQA\\nText classific AccuraWcyomen's Accuracy (Binary accuracy from classification_accuracy_s\\nation Ecommerce core)\\nClothing\\nReviews\\nRobusWtnoems en's classification_accuracy_score and delta_classifica\\ns Ecommerce tion_accuracy_score\\nClothing\\nReviews\\nTopics\\n• General text generation\\n• Text summarization\\n• Question and answer\\n• Text classification\\nGeneral text generation\\nImportant\\nFor general text generation, there is a known system issue that prevents Cohere models\\nfrom completing the toxicity evaluation successfully.\\nGeneral text generation is a task used by applications that include chatbots. The responses\\n\"}\n",
      "{'question': 'How does the lambda_handler function handle different types of responses from the LLM, and what are the potential outcomes of its parsing process?', 'ground_truth': 'The lambda_handler function in the given code snippet handles different types of responses from the LLM through a series of parsing steps and error checks:\\n\\n1. It first sanitizes the LLM response and extracts any rationale.\\n\\n2. Then, it attempts to parse for a final answer. If successful, it constructs a response with \\'invocationType\\': \\'FINISH\\' and includes the final answer and any generated response parts (citations).\\n\\n3. If no final answer is found, it tries to parse for an \"ask user\" response. If found, it constructs a response with \\'invocationType\\': \\'ASK_USER\\' and includes the question to ask the user.\\n\\n4. If neither a final answer nor an \"ask user\" response is found, it attempts to parse for an agent action (though this part is not fully shown in the snippet).\\n\\n5. Throughout the process, if any parsing errors occur (e.g., incorrect function call format), it adds a reprompt response to guide the LLM to provide the correct format.\\n\\nThe potential outcomes of this parsing process are:\\na) A successful \\'FINISH\\' response with a final answer and possible citations\\nb) A successful \\'ASK_USER\\' response with a question to be asked to the user\\nc) A reprompt response if there are formatting errors in the LLM output\\nd) Potentially, an agent action response (not fully shown in the snippet)\\n\\nThis structured approach allows the function to handle various LLM output scenarios and provide appropriate responses or error handling for each case.', 'question_type': 'complex', 'context': 'ANSWER_PART_REGEX = \"<answer_part\\\\\\\\s?>(.+?)</answer_part\\\\\\\\s?>\"\\nANSWER_TEXT_PART_REGEX = \"<text\\\\\\\\s?>(.+?)</text\\\\\\\\s?>\"\\nANSWER_REFERENCE_PART_REGEX = \"<source\\\\\\\\s?>(.+?)</source\\\\\\\\s?>\"\\nANSWER_PART_PATTERN = re.compile(ANSWER_PART_REGEX, re.DOTALL)\\nANSWER_TEXT_PART_PATTERN = re.compile(ANSWER_TEXT_PART_REGEX, re.DOTALL)\\nANSWER_REFERENCE_PART_PATTERN = re.compile(ANSWER_REFERENCE_PART_REGEX, re.DOTALL)\\n# You can provide messages to reprompt the LLM in case the LLM output is not in\\nthe expected format\\nMISSING_API_INPUT_FOR_USER_REPROMPT_MESSAGE = \"Missing the argument askuser for\\nuser::askuser function call. Please try again with the correct argument added\"\\nASK_USER_FUNCTION_CALL_STRUCTURE_REMPROMPT_MESSAGE = \"The function call format\\nis incorrect. The format for function calls to the askuser function must be:\\n<function_call>user::askuser(askuser=\\\\\"$ASK_USER_INPUT\\\\\")</function_call>.\"\\nFUNCTION_CALL_STRUCTURE_REPROMPT_MESSAGE = \\'The function call format\\n is incorrect. The format for function calls must be: <function_call>\\n$FUNCTION_NAME($FUNCTION_ARGUMENT_NAME=\"\"$FUNCTION_ARGUMENT_NAME\"\")</\\nfunction_call>.\\'\\nlogger = logging.getLogger()\\nAdvanced prompts 767\\nAmazon Bedrock User Guide\\n# This parser lambda is an example of how to parse the LLM output for the default\\norchestration prompt\\ndef lambda_handler(event, context):\\nlogger.info(\"Lambda input: \" + str(event))\\n# Sanitize LLM response\\nsanitized_response = sanitize_response(event[\\'invokeModelRawResponse\\'])\\n# Parse LLM response for any rationale\\nrationale = parse_rationale(sanitized_response)\\n# Construct response fields common to all invocation types\\nparsed_response = {\\n\\'promptType\\': \"ORCHESTRATION\",\\n\\'orchestrationParsedResponse\\': {\\n\\'rationale\\': rationale\\n}\\n}\\n# Check if there is a final answer\\ntry:\\nfinal_answer, generated_response_parts = parse_answer(sanitized_response)\\nexcept ValueError as e:\\naddRepromptResponse(parsed_response, e)\\nreturn parsed_response\\nif final_answer:\\n parsed_response[\\'orchestrationParsedResponse\\'][\\'responseDetails\\'] = {\\n\\'invocationType\\': \\'FINISH\\',\\n\\'agentFinalResponse\\': {\\n\\'responseText\\': final_answer\\n}\\n}\\nif generated_response_parts:\\nparsed_response[\\'orchestrationParsedResponse\\'][\\'responseDetails\\']\\n[\\'agentFinalResponse\\'][\\'citations\\'] = {\\n\\'generatedResponseParts\\': generated_response_parts\\n}\\nlogger.info(\"Final answer parsed response: \" + str(parsed_response))\\nreturn parsed_response\\nAdvanced prompts 768\\nAmazon Bedrock User Guide\\n# Check if there is an ask user\\ntry:\\nask_user = parse_ask_user(sanitized_response)\\nif ask_user:\\nparsed_response[\\'orchestrationParsedResponse\\'][\\'responseDetails\\'] = {\\n\\'invocationType\\': \\'ASK_USER\\',\\n\\'agentAskUser\\': {\\n\\'responseText\\': ask_user\\n}\\n}\\nlogger.info(\"Ask user parsed response: \" + str(parsed_response))\\nreturn parsed_response\\nexcept ValueError as e:\\naddRepromptResponse(parsed_response, e)\\nreturn parsed_response\\n# Check if there is an agent action\\ntry:\\n'}\n",
      "{'question': 'How can users download third-party audit reports for AWS services?', 'ground_truth': 'Users can download third-party audit reports using AWS Artifact.', 'question_type': 'simple', 'context': \"• To learn the difference between using roles and resource-based policies for cross-account access,\\nsee Cross account resource access in IAM in the IAM User Guide.\\nCompliance validation for Amazon Bedrock\\nTo learn whether an AWS service is within the scope of specific compliance programs, see AWS\\nservices in Scope by Compliance Program and choose the compliance program that you are\\ninterested in. For general information, see AWS Compliance Programs.\\nCompliance validation 1155\\nAmazon Bedrock User Guide\\nYou can download third-party audit reports using AWS Artifact. For more information, see\\nDownloading Reports in AWS Artifact.\\nYour compliance responsibility when using AWS services is determined by the sensitivity of your\\ndata, your company's compliance objectives, and applicable laws and regulations. AWS provides the\\nfollowing resources to help with compliance:\\n• Security and Compliance Quick Start Guides – These deployment guides discuss architectural\\n considerations and provide steps for deploying baseline environments on AWS that are security\\nand compliance focused.\\n• Architecting for HIPAA Security and Compliance on Amazon Web Services – This whitepaper\\ndescribes how companies can use AWS to create HIPAA-eligible applications.\\nNote\\nNot all AWS services are HIPAA eligible. For more information, see the HIPAA Eligible\\nServices Reference.\\n• AWS Compliance Resources – This collection of workbooks and guides might apply to your\\nindustry and location.\\n• AWS Customer Compliance Guides – Understand the shared responsibility model through the\\nlens of compliance. The guides summarize the best practices for securing AWS services and map\\nthe guidance to security controls across multiple frameworks (including National Institute of\\nStandards and Technology (NIST), Payment Card Industry Security Standards Council (PCI), and\\nInternational Organization for Standardization (ISO)).\\n • Evaluating Resources with Rules in the AWS Config Developer Guide – The AWS Config service\\nassesses how well your resource configurations comply with internal practices, industry\\nguidelines, and regulations.\\n• AWS Security Hub – This AWS service provides a comprehensive view of your security state within\\nAWS. Security Hub uses security controls to evaluate your AWS resources and to check your\\ncompliance against security industry standards and best practices. For a list of supported services\\nand controls, see Security Hub controls reference.\\n• Amazon GuardDuty – This AWS service detects potential threats to your AWS accounts,\\nworkloads, containers, and data by monitoring your environment for suspicious and malicious\\nactivities. GuardDuty can help you address various compliance requirements, like PCI DSS, by\\nmeeting intrusion detection requirements mandated by certain compliance frameworks.\\nCompliance validation 1156\\nAmazon Bedrock User Guide\\n\"}\n",
      "{'question': 'How can an administrator troubleshoot a failed data source sync event and optimize future sync operations in Amazon Bedrock, considering both console and API methods?', 'ground_truth': \"To troubleshoot a failed data source sync event and optimize future operations in Amazon Bedrock, an administrator can follow these steps:\\n\\n1. Console method:\\n   a. Navigate to the Amazon Bedrock console and select 'Knowledge bases' from the left navigation pane.\\n   b. In the Data source section, select the relevant data source.\\n   c. Check the Sync history for details about sync events.\\n   d. For failed syncs, select the event and choose 'View warnings' to see specific reasons for failure.\\n\\n2. API method:\\n   a. Use the GetIngestionJob API request to retrieve detailed information about a specific sync event. Provide the dataSourceId, knowledgeBaseId, and ingestionJobId.\\n   b. Use the ListIngestionJobs API request to review the sync history, filtering results by status and sorting by start time or job status.\\n\\n3. Optimization strategies:\\n   a. Analyze patterns in failed sync events to identify common issues.\\n   b. Update the data source configurations if necessary, ensuring the IAM role has the required permissions.\\n   c. If using a custom endpoint, verify the Secrets Manager secret is up-to-date.\\n   d. Consider adjusting the maxResults parameter in API calls to balance between performance and completeness of information.\\n   e. Implement pagination using the nextToken in API responses for comprehensive reviews of large sync histories.\\n\\nBy combining console and API methods for troubleshooting and implementing these optimization strategies, administrators can effectively manage and improve data source sync operations in Amazon Bedrock.\", 'question_type': 'complex', 'context': 'To view information about a data source\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\\nbedrock/.\\n2. From the left navigation pane, select Knowledge bases.\\n3. In the Data source section, select the data source for which you want to view details.\\n4. The Data source overview contains details about the data source.\\n5. The Sync history contains details about when the data source was synced. To see reasons\\nfor why a sync event failed, select a sync event and choose View warnings.\\nAPI\\nTo get information about a data source, send a GetDataSource request with a Agents\\nfor Amazon Bedrock build-time endpoint and specify the dataSourceId and the\\nknowledgeBaseId of the knowledge base that it belongs to.\\nTo list information about a knowledge base\\'s data sources, send a ListDataSources request with\\n a Agents for Amazon Bedrock build-time endpoint and specify the ID of the knowledge base.\\n• To set the maximum number of results to return in a response, use the maxResults field.\\n• If there are more results than the number you set, the response returns a nextToken. You\\ncan use this value in another ListDataSources request to see the next batch of results.\\nManage a data source 636\\nAmazon Bedrock User Guide\\nTo get information a sync event for a data source, send a GetIngestionJob request with a Agents\\nfor Amazon Bedrock build-time endpoint. Specify the dataSourceId, knowledgeBaseId, and\\ningestionJobId.\\nTo list the sync history for a data source in a knowledge base, send a ListIngestionJobs request\\nwith a Agents for Amazon Bedrock build-time endpoint. Specify the ID of the knowledge base\\nand data source. You can set the following specifications.\\n• Filter for results by specifying a status to search for in the filters object.\\n • Sort by the time that the job was started or the status of a job by specifying the sortBy\\nobject. You can sort in ascending or descending order.\\n• Set the maximum number of results to return in a response in the maxResults field. If there\\nare more results than the number you set, the response returns a nextToken that you can\\nsend in another ListIngestionJobs request to see the next batch of jobs.\\nUpdate a data source\\nYou can update a data source in the following ways:\\n• Add, change, or remove files or content from the the data source.\\n• Change the data source configurations, or the KMS key to use for encrypting transient data\\nduring data ingestion. If you change the source or endpoint configuration details, you should\\nupdate or create a new IAM role with the required access permissions and Secrets Manager secret\\n(if applicable).\\n• Set your data source deletion policy is to either \"Delete\" or \"Retain\". You can delete all data from\\n'}\n",
      "{'question': 'How do you enable Prompt flows and Prompt management in Bedrock Studio?', 'ground_truth': 'To enable Prompt flows and Prompt management in Bedrock Studio, update the workspace by modifying the provisioning role, updating the permissions boundary, and adding Amazon DataZone blueprints.', 'question_type': 'simple', 'context': \"1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://\\nconsole.aws.amazon.com/bedrock/.\\n2. In the left navigation pane, choose Bedrock Studio.\\n3. In Bedrock Studio workspaces, select the workspace that you want to update.\\n4. Choose the Overview tab. If the workspace needs an update to support Prompt flows and\\nPrompt management, you will see an alert banner with steps for enabling Prompt flows and\\nPrompt management.\\n5. In Workspace details, choose the provisioning role ARN in Provisioning role. The IAM console\\nopens with the provisioning role.\\n6. In the IAM console, choose the Permissions tab.\\n7. In Permission policies select the policy to open the policy editor.\\n8. In the Policy editor, choose JSON, if it is not already chosen.\\n9. Replace the current policy with the policy at Permissions to manage Amazon Bedrock Studio\\nuser resources.\\n10. Choose Next.\\nUpdate the provisioning role 1010\\nAmazon Bedrock User Guide\\n11. Choose Save changes.\\n 12. Next step: Update the permissions boundary.\\nUpdate the permissions boundary\\nIn this procedure, you update the permissions boundary for a Amazon Bedrock Studio workspace.\\nUpdating the permissions boundary helps enable Prompt flows and Prompt management.\\nTo update the permission boundaries\\n1. Sign in to the AWS Management Console and open the IAM console at https://\\nconsole.aws.amazon.com/iam/.\\n2. On the left navigation pane, choose Policies.\\n3. Open the AmazonDataZoneBedrockPermissionsBoundary policy that you created in Step\\n2: Create permissions boundary, service role, and provisioning role.\\n4. On the Permissions tab, choose Edit.\\n5. In the Policy editor, choose JSON, if it is not already chosen.\\n6. Replace the current policy with the policy at Permission boundaries.\\n7. Choose Next.\\n8. Choose Save changes.\\n9. Next step: Add the Amazon DataZone blueprints.\\nAdd the Amazon DataZone blueprints\\nIn this procedure, you add the Amazon DataZone blueprints that an Amazon Bedrock Studio\\n workspace needs to enable Prompt flows and Prompt management.\\nTo add the Amazon DataZone blueprints\\n1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://\\nconsole.aws.amazon.com/bedrock/.\\n2. In the left navigation pane, choose Bedrock Studio.\\n3. In Bedrock Studio workspaces, select the workspace that you want to add the blueprints to.\\n4. Choose the Overview tab.\\nUpdate the permissions boundary 1011\\nAmazon Bedrock User Guide\\n5. In Workspace details, note the alert banner for Prompt management and Prompt flows. Make\\nsure you have completed step one.\\n6. In the alert banner, choose the Enable hyperlink to add the blueprints.\\nUpdate a workspace for app export\\nAmazon Bedrock Studio is in preview release for Amazon Bedrock and is subject to change.\\nIf you created an Amazon Bedrock Studio workspace before the introduction of the app export\\nfeature, you need to update the permissions boundary for the workspace. You don't need to\\n\"}\n",
      "{'question': 'How might the choice between using the RetrieveAndGenerate API and the Retrieve API in Amazon Bedrock affect the flexibility of model selection for response generation?', 'ground_truth': 'The choice between RetrieveAndGenerate API and Retrieve API in Amazon Bedrock significantly affects model selection flexibility. RetrieveAndGenerate API queries the knowledge base and generates responses using only supported Amazon Bedrock knowledge base models, limiting options to specific models like Amazon Titan Text Premier and Anthropic Claude variants. In contrast, the Retrieve API only queries the knowledge base without generating responses, allowing users to subsequently use the retrieved results with any Amazon Bedrock or SageMaker model in an InvokeModel request. This approach offers greater flexibility in choosing models for response generation, potentially enabling the use of specialized or custom models beyond the predefined set supported by RetrieveAndGenerate.', 'question_type': 'complex', 'context': \"Cohere Embed (Multilingual) cohere.embed-multilingual-v3\\nYou can use the following models to generate responses after retrieving information from\\nknowledge bases:\\nNote\\nThe RetrieveAndGenerate API queries the knowledge base and uses supported Amazon\\nBedrock knowledge base models to generate responses from the information it retrieves.\\nThe Retrieve API only queries the knowledge base; it doesn't generate responses.\\nTherefore, after retrieving results with the Retrieve API, you could use the results in\\nan InvokeModel request with any Amazon Bedrock or SageMaker model to generate\\nresponses.\\nModel Model ID\\nAmazon Titan Text Premier amazon.titan-text-premier-v1:0\\nAnthropic Claude v2.0 anthropic.claude-v2\\nSupported regions and models 528\\nAmazon Bedrock User Guide\\nModel Model ID\\nAnthropic Claude v2.1 anthropic.claude-v2:1\\nAnthropic Claude 3 Sonnet v1 anthropic.claude-3-sonnet-20240229-v1:0\\nAnthropic Claude 3 Haiku v1 anthropic.claude-3-haiku-20240307-v1:0\\n Anthropic Claude Instant v1 anthropic.claude-instant-v1\\nPrerequisites for Knowledge bases for Amazon Bedrock\\nBefore you can create a knowledge base, you need to fulfill the following prerequisites:\\n1. Prepare your source of data that contains the information that you want to supply to your\\nknowledge base. You can connect to your source of data. See Supported data source connectors.\\n2. (Optional) Set up a vector store of your choice. You can use the AWS Management Console to\\nautomatically create a vector store in Amazon OpenSearch Serverless for you.\\n3. (Optional) Create a custom AWS Identity and Access Management (IAM) service role with the\\nproper permissions by following the instructions at Create a service role for Knowledge bases for\\nAmazon Bedrock. You can skip this prerequisite if you plan to use the AWS Management Console\\nto automatically create a service role for you.\\n4. (Optional) Set up extra security configurations by following the steps at Encryption of\\n knowledge base resources.\\nTopics\\n• Set up a data source connector for your knowledge base\\n• Set up a vector index for your knowledge base in a supported vector store\\nSet up a data source connector for your knowledge base\\nA data source repository contains files or content with information that can be retrieved when\\nyour knowledge base is queried. You must store your documents or content in at least one of the\\nsupported data sources.\\nPrerequisites 529\\nAmazon Bedrock User Guide\\nTo configure a data source connector to connect and crawl your data from your data source\\nrepository, see Supported data source connectors.\\nCreate a knowledge base with your data source configured, then sync your data source.\\nAfter you sync your data source, you can query your knowledge base.\\nTopics\\n• Supported document formats and limits\\n• Metadata and filtering\\n• Source chunks\\nSupported document formats and limits\\nCheck that each source document file conforms to the following requirements:\\n\"}\n",
      "{'question': 'What are the two main methods of model customization in Amazon Bedrock?', 'ground_truth': 'The two main methods of model customization in Amazon Bedrock are Fine-tuning and Continued Pre-training.', 'question_type': 'simple', 'context': \"your data. You can also use an existing role or let the console automatically create a role with\\nthe proper permissions.\\n3. (Optional) Configure KMS keys and/or VPC for extra security.\\n4. Create a Fine-tuning or Continued Pre-training job, controlling the training process by adjusting\\nthe hyperparameter values.\\n5. Analyze the results by looking at the training or validation metrics or by using model evaluation.\\n6. Purchase Provisioned Throughput for your newly created custom model.\\n7. Use your custom model as you would a base model in Amazon Bedrock tasks, such as model\\ninference.\\nTopics\\n• Supported regions and models for model customization\\n• Prerequisites for model customization\\n• Submit a model customization job\\n• Manage a model customization job\\n• Analyze the results of a model customization job\\n• Import a model with Custom Model Import\\n• Share a model for another account to use\\n• Copy a model to use in a region\\n• Use a custom model\\n• Code samples for model customization\\n • Guidelines for model customization\\n• Troubleshooting\\nSupported regions and models for model customization\\nThe following table shows regional support for each customization method:\\nRegion Fine-tuning Continued pre-training\\nUS East (N. Virginia) Yes Yes\\nSupported regions and models 906\\nAmazon Bedrock User Guide\\nRegion Fine-tuning Continued pre-training\\nUS West (Oregon) Yes Yes\\nAWS GovCloud (US-West) Yes No\\nNote\\n• Amazon Titan Text Premier : This model is currently only supported in us-east-1 (IAD).\\n• Anthropic Claude 3 Haiku : This model is in preview. To request to be considered for\\naccess to the preview of fine-tuning Anthropic's Claude 3 Haiku in Amazon Bedrock,\\ncontact your AWS account team or submit a support ticket via the AWS Management\\nConsole. To create a support ticket in the AWS Management Console, for the Service,\\nselect Bedrock and for Category, select Model. Regions supported during the preview are\\nsubject to change.\\n The following table shows model support for each customization method:\\nModel name Model ID Fine-tuning Continued pre-train\\ning\\nAmazon Titan Text amazon.titan-text- Yes Yes\\nG1 - Express express-v1\\nAmazon Titan Text amazon.titan-text- Yes Yes\\nG1 - Lite lite-v1\\nAmazon Titan Text amazon.titan-text- Yes (in preview - No\\nPremier premier-v1:0:32k contact AWS to get\\naccess)\\nAmazon Titan Image amazon.titan-image- Yes No\\nGenerator G1 V1 generator-v1\\nAmazon Titan Image amazon.titan-image- Yes No\\nGenerator G1 V2 generator-v2:0\\nSupported regions and models 907\\nAmazon Bedrock User Guide\\nModel name Model ID Fine-tuning Continued pre-train\\ning\\nAmazon Titan amazon.titan-embed- Yes No\\nMultimodal image-v1\\nEmbeddings G1 G1\\nCohere Command cohere.command-tex Yes No\\nt-v14\\nCohere Command cohere.command-lig Yes No\\nLight ht-text-v14\\nMeta Llama 2 13B meta.llama2-13b-ch Yes No\\nat-v1\\nMeta Llama 2 70B meta.llama2-70b-ch Yes No\\nat-v1\\nAnthropic Claude 3 anthropic.claude-3- Yes (in preview - No\\n\"}\n",
      "{'question': 'How does the code handle different response formats from various foundation models when invoking Anthropic Claude 2 through Amazon Bedrock, and what specific steps are taken to process the response?', 'ground_truth': 'The code handles different response formats by using model-specific request and response structures. For Anthropic Claude 2, it defines custom structs (ClaudeRequest and ClaudeResponse) to match the model\\'s expected input and output formats. The process involves several steps:\\n\\n1. It creates a ClaudeRequest struct with the prompt, max tokens, temperature, and stop sequences.\\n2. The request is marshaled into JSON format.\\n3. The InvokeModel function is called with the model ID \"anthropic.claude-v2\" and the JSON body.\\n4. After receiving the response, it unmarshals the output into a ClaudeResponse struct.\\n5. Finally, it extracts the generated text from the Completion field of the response.\\n\\nThis approach allows for flexible handling of different model responses while maintaining type safety and ease of use specific to each foundation model.', 'question_type': 'complex', 'context': 'var userMessage = \"Describe the purpose of a \\'hello world\\' program in one line.\";\\n//Format the request payload using the model\\'s native structure.\\nvar nativeRequest = JsonSerializer.Serialize(new\\n{\\nanthropic_version = \"bedrock-2023-05-31\",\\nmax_tokens = 512,\\ntemperature = 0.5,\\nmessages = new[]\\n{\\nnew { role = \"user\", content = userMessage }\\n}\\n});\\n// Create a request with the model ID and the model\\'s native request payload.\\nvar request = new InvokeModelRequest()\\n{\\nModelId = modelId,\\nBody = new MemoryStream(System.Text.Encoding.UTF8.GetBytes(nativeRequest)),\\nContentType = \"application/json\"\\n};\\ntry\\n{\\n// Send the request to the Bedrock Runtime and wait for the response.\\nvar response = await client.InvokeModelAsync(request);\\n// Decode the response body.\\nvar modelResponse = await JsonNode.ParseAsync(response.Body);\\nAnthropic Claude 1328\\nAmazon Bedrock User Guide\\n// Extract and print the response text.\\nvar responseText = modelResponse[\"content\"]?[0]?[\"text\"] ?? \"\";\\n Console.WriteLine(responseText);\\n}\\ncatch (AmazonBedrockRuntimeException e)\\n{\\nConsole.WriteLine($\"ERROR: Can\\'t invoke \\'{modelId}\\'. Reason: {e.Message}\");\\nthrow;\\n}\\n• For API details, see InvokeModel in AWS SDK for .NET API Reference.\\nGo\\nSDK for Go V2\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nInvoke the Anthropic Claude 2 foundation model to generate text.\\n// Each model provider has their own individual request and response formats.\\n// For the format, ranges, and default values for Anthropic Claude, refer to:\\n// https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-\\nclaude.html\\ntype ClaudeRequest struct {\\nPrompt string `json:\"prompt\"`\\nMaxTokensToSample int `json:\"max_tokens_to_sample\"`\\nTemperature float64 `json:\"temperature,omitempty\"`\\nStopSequences []string `json:\"stop_sequences,omitempty\"`\\n}\\ntype ClaudeResponse struct {\\nCompletion string `json:\"completion\"`\\n}\\nAnthropic Claude 1329\\n Amazon Bedrock User Guide\\n// Invokes Anthropic Claude on Amazon Bedrock to run an inference using the input\\n// provided in the request body.\\nfunc (wrapper InvokeModelWrapper) InvokeClaude(prompt string) (string, error) {\\nmodelId := \"anthropic.claude-v2\"\\n// Anthropic Claude requires enclosing the prompt as follows:\\nenclosedPrompt := \"Human: \" + prompt + \"\\\\n\\\\nAssistant:\"\\nbody, err := json.Marshal(ClaudeRequest{\\nPrompt: enclosedPrompt,\\nMaxTokensToSample: 200,\\nTemperature: 0.5,\\nStopSequences: []string{\"\\\\n\\\\nHuman:\"},\\n})\\nif err != nil {\\nlog.Fatal(\"failed to marshal\", err)\\n}\\noutput, err := wrapper.BedrockRuntimeClient.InvokeModel(context.TODO(),\\n&bedrockruntime.InvokeModelInput{\\nModelId: aws.String(modelId),\\nContentType: aws.String(\"application/json\"),\\nBody: body,\\n})\\nif err != nil {\\nProcessError(err, modelId)\\n}\\nvar response ClaudeResponse\\nif err := json.Unmarshal(output.Body, &response); err != nil {\\nlog.Fatal(\"failed to unmarshal\", err)\\n}\\nreturn response.Completion, nil\\n}\\n'}\n",
      "{'question': 'How can you retrieve information about an Amazon Bedrock Agent using Python?', 'ground_truth': 'You can use the get_agent function from the AWS SDK for Python (Boto3). This function takes an agent_id as a parameter and returns information about the specified Amazon Bedrock Agent.', 'question_type': 'simple', 'context': 'response = self.client.delete_agent_alias(\\nagentId=agent_id, agentAliasId=agent_alias_id\\n)\\nexcept ClientError as e:\\nlogger.error(f\"Couldn\\'t delete agent alias. {e}\")\\nraise\\nelse:\\nreturn response\\n• For API details, see DeleteAgentAlias in AWS SDK for Python (Boto3) API Reference.\\nFor a complete list of AWS SDK developer guides and code examples, see Using this service with\\nan AWS SDK. This topic also includes information about getting started and details about previous\\nSDK versions.\\nBasics 1526\\nAmazon Bedrock User Guide\\nUse GetAgent with an AWS SDK or CLI\\nThe following code examples show how to use GetAgent.\\nAction examples are code excerpts from larger programs and must be run in context. You can see\\nthis action in context in the following code example:\\n• Create and invoke an agent\\nJavaScript\\nSDK for JavaScript (v3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nGet an agent.\\n // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport { checkForPlaceholders } from \"../lib/utils.js\";\\nimport {\\nBedrockAgentClient,\\nGetAgentCommand,\\n} from \"@aws-sdk/client-bedrock-agent\";\\n/**\\n* Retrieves the details of an Amazon Bedrock Agent.\\n*\\n* @param {string} agentId - The unique identifier of the agent.\\n* @param {string} [region=\\'us-east-1\\'] - The AWS region in use.\\n* @returns {Promise<import(\"@aws-sdk/client-bedrock-agent\").Agent>} An object\\ncontaining the agent details.\\n*/\\nexport const getAgent = async (agentId, region = \"us-east-1\") => {\\nBasics 1527\\nAmazon Bedrock User Guide\\nconst client = new BedrockAgentClient({ region });\\nconst command = new GetAgentCommand({ agentId });\\nconst response = await client.send(command);\\nreturn response.agent;\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\n // Replace the placeholders for agentId with an existing agent\\'s id.\\n// Ensure to remove the brackets \\'[]\\' before adding your data.\\n// The agentId must be an alphanumeric string with exactly 10 characters.\\nconst agentId = \"[ABC123DE45]\";\\n// Check for unresolved placeholders in agentId.\\ncheckForPlaceholders([agentId]);\\nconsole.log(`Retrieving agent with ID ${agentId}...`);\\nconst agent = await getAgent(agentId);\\nconsole.log(agent);\\n}\\n• For API details, see GetAgent in AWS SDK for JavaScript API Reference.\\nPython\\nSDK for Python (Boto3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nGet an agent.\\ndef get_agent(self, agent_id, log_error=True):\\n\"\"\"\\nGets information about an agent.\\nBasics 1528\\nAmazon Bedrock User Guide\\n:param agent_id: The unique identifier of the agent.\\n:param log_error: Whether to log any errors that occur when getting the\\nagent.\\nIf True, errors will be logged to the logger. If False,\\nerrors\\n'}\n",
      "{'question': 'How does the AWS SDK for .NET handle potential errors when invoking Meta Llama 2 on Amazon Bedrock, and what specific information does it provide in case of a failure?', 'ground_truth': 'The AWS SDK for .NET handles potential errors when invoking Meta Llama 2 on Amazon Bedrock by using a try-catch block. If an error occurs during the invocation, it catches the AmazonBedrockRuntimeException. In case of a failure, it provides specific information including the model ID that failed to invoke and the reason for the failure. The error message is printed to the console, showing \"ERROR: Can\\'t invoke \\'[modelId]\\'. Reason: [e.Message]\", where [modelId] is the ID of the model being invoked (in this case, \"meta.llama2-13b-chat-v1\"), and [e.Message] contains the specific error message from the exception. After logging this information, the exception is re-thrown to allow for further error handling up the call stack.', 'question_type': 'complex', 'context': 'an AWS SDK. This topic also includes information about getting started and details about previous\\nSDK versions.\\nInvoke Meta Llama 2 on Amazon Bedrock using the Invoke Model API\\nThe following code examples show how to send a text message to Meta Llama 2, using the Invoke\\nModel API.\\n.NET\\nAWS SDK for .NET\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Use the native inference API to send a text message to Meta Llama 2.\\nusing System;\\nusing System.IO;\\nusing System.Text.Json;\\nusing System.Text.Json.Nodes;\\nusing Amazon;\\nusing Amazon.BedrockRuntime;\\nusing Amazon.BedrockRuntime.Model;\\n// Create a Bedrock Runtime client in the AWS Region you want to use.\\nvar client = new AmazonBedrockRuntimeClient(RegionEndpoint.USEast1);\\n// Set the model ID, e.g., Llama 2 Chat 13B.\\nvar modelId = \"meta.llama2-13b-chat-v1\";\\n// Define the prompt for the model.\\n var prompt = \"Describe the purpose of a \\'hello world\\' program in one line.\";\\nMeta Llama 1437\\nAmazon Bedrock User Guide\\n// Embed the prompt in Llama 2\\'s instruction format.\\nvar formattedPrompt = $\"<s>[INST] {prompt} [/INST]\";\\n//Format the request payload using the model\\'s native structure.\\nvar nativeRequest = JsonSerializer.Serialize(new\\n{\\nprompt = formattedPrompt,\\nmax_gen_len = 512,\\ntemperature = 0.5\\n});\\n// Create a request with the model ID and the model\\'s native request payload.\\nvar request = new InvokeModelRequest()\\n{\\nModelId = modelId,\\nBody = new MemoryStream(System.Text.Encoding.UTF8.GetBytes(nativeRequest)),\\nContentType = \"application/json\"\\n};\\ntry\\n{\\n// Send the request to the Bedrock Runtime and wait for the response.\\nvar response = await client.InvokeModelAsync(request);\\n// Decode the response body.\\nvar modelResponse = await JsonNode.ParseAsync(response.Body);\\n// Extract and print the response text.\\nvar responseText = modelResponse[\"generation\"] ?? \"\";\\n Console.WriteLine(responseText);\\n}\\ncatch (AmazonBedrockRuntimeException e)\\n{\\nConsole.WriteLine($\"ERROR: Can\\'t invoke \\'{modelId}\\'. Reason: {e.Message}\");\\nthrow;\\n}\\n• For API details, see InvokeModel in AWS SDK for .NET API Reference.\\nMeta Llama 1438\\nAmazon Bedrock User Guide\\nGo\\nSDK for Go V2\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Each model provider has their own individual request and response formats.\\n// For the format, ranges, and default values for Meta Llama 2 Chat, refer to:\\n// https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-\\nmeta.html\\ntype Llama2Request struct {\\nPrompt string `json:\"prompt\"`\\nMaxGenLength int `json:\"max_gen_len,omitempty\"`\\nTemperature float64 `json:\"temperature,omitempty\"`\\n}\\ntype Llama2Response struct {\\nGeneration string `json:\"generation\"`\\n}\\n'}\n",
      "{'question': 'What tool can validate IAM policies for security and functionality?', 'ground_truth': 'IAM Access Analyzer can validate IAM policies to ensure secure and functional permissions. It provides more than 100 policy checks and actionable recommendations to help author secure and functional policies.', 'question_type': 'simple', 'context': '• Use conditions in IAM policies to further restrict access – You can add a condition to your\\npolicies to limit access to actions and resources. For example, you can write a policy condition to\\nspecify that all requests must be sent using SSL. You can also use conditions to grant access to\\nservice actions if they are used through a specific AWS service, such as AWS CloudFormation. For\\nmore information, see IAM JSON policy elements: Condition in the IAM User Guide.\\n• Use IAM Access Analyzer to validate your IAM policies to ensure secure and functional\\npermissions – IAM Access Analyzer validates new and existing policies so that the policies\\nadhere to the IAM policy language (JSON) and IAM best practices. IAM Access Analyzer provides\\nmore than 100 policy checks and actionable recommendations to help you author secure and\\nfunctional policies. For more information, see IAM Access Analyzer policy validation in the IAM\\nUser Guide.\\n • Require multi-factor authentication (MFA) – If you have a scenario that requires IAM users\\nor a root user in your AWS account, turn on MFA for additional security. To require MFA when\\nAPI operations are called, add MFA conditions to your policies. For more information, see\\nConfiguring MFA-protected API access in the IAM User Guide.\\nFor more information about best practices in IAM, see Security best practices in IAM in the IAM User\\nGuide.\\nUse the Amazon Bedrock console\\nTo access the Amazon Bedrock console, you must have a minimum set of permissions. These\\npermissions must allow you to list and view details about the Amazon Bedrock resources in your\\nAWS account. If you create an identity-based policy that is more restrictive than the minimum\\nrequired permissions, the console won\\'t function as intended for entities (users or roles) with that\\npolicy.\\nIdentity-based policy examples 1076\\nAmazon Bedrock User Guide\\n You don\\'t need to allow minimum console permissions for users that are making calls only to the\\nAWS CLI or the AWS API. Instead, allow access to only the actions that match the API operation\\nthat they\\'re trying to perform.\\nTo ensure that users and roles can still use the Amazon Bedrock console, also attach the Amazon\\nBedrock AmazonBedrockFullAccess or AmazonBedrockReadOnly AWS managed policy to the\\nentities. For more information, see Adding permissions to a user in the IAM User Guide.\\nAllow users to view their own permissions\\nThis example shows how you might create a policy that allows IAM users to view the inline and\\nmanaged policies that are attached to their user identity. This policy includes permissions to\\ncomplete this action on the console or programmatically using the AWS CLI or AWS API.\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"ViewOwnUserInfo\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"iam:GetUserPolicy\",\\n\"iam:ListGroupsForUser\",\\n\"iam:ListAttachedUserPolicies\",\\n'}\n",
      "{'question': 'How does versioning a guardrail in Amazon Bedrock differ from deleting it, and what precautions should be taken when performing either action?', 'ground_truth': \"Versioning a guardrail in Amazon Bedrock creates a snapshot of the guardrail's configuration at a specific point in time, allowing you to iterate on the working draft while maintaining stable versions for production use. You can create multiple versions, compare their performance, and easily switch between them in your application. In contrast, deleting a guardrail permanently removes it and all its versions. When deleting a guardrail, you must first disassociate it from all resources and applications to avoid potential errors. It's important to note that while guardrail versions don't have individual ARNs and aren't considered separate resources, deleting the main guardrail will delete all its versions. Therefore, versioning provides flexibility and safety in development, while deletion requires careful consideration of dependencies and is irreversible.\", 'question_type': 'complex', 'context': '\"guardrailArn\": \"string\",\\n\"guardrailId\": \"string\",\\n\"updatedAt\": \"string\",\\n\"version\": \"string\"\\n}\\nEdit a guardrail 402\\nAmazon Bedrock User Guide\\nDelete a guardrail\\nYou can delete a guardrail when you no longer need to use it. Be sure to disassociate the guardrail\\nfrom all the resources or applications that use it before you delete the guardrail in order to avoid\\npotential errors.\\nConsole\\nTo delete a guardrail\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\\nbedrock/.\\n2. Choose Guardrails from the left navigation pane. Then, select a guardrail in the Guardrails\\nsection.\\n3. In the Guardrails section, select a guardrail that you want to delete and then choose\\nDelete.\\n4. Enter delete in the user input field and choose Delete to delete the guardrail.\\nAPI\\nTo delete a guardrail, send a DeleteGuardrail request and only specify the ARN of the guardrail\\n in the guardrailIdentifier field. Don\\'t specify the guardrailVersion\\nThe following is the request format:\\nDELETE /guardrails/guardrailIdentifier?guardrailVersion=guardrailVersion HTTP/1.1\\nWarning\\nIf you delete a guardrail, all of its versions will be deleted.\\nIf the deletion is successful, the response returns an HTTP 200 status code.\\nDelete a guardrail 403\\nAmazon Bedrock User Guide\\nDeploy a guardrail\\nWhen you\\'re ready to deploy your guardrail to production, you create a version of it and invoke\\nthe version of the guardrail in your application. A version is a snapshot of your guardrail that\\nyou create at a point in time when you are iterating on the working draft of the guardrail. Create\\nversions of your guardrail when you are satisfied with a set of configurations. You can use the\\ntest window (for more information, see Test a guardrail) to compare how different versions of\\nyour guardrail perform in in evaluating the input prompts and model responses and generating\\n controlled responses for the final output. Versions allow you to easily switch between different\\nconfigurations for your guardrail and update your application with the most appropriate version\\nfor your use-case.\\nTopics\\n• Create and manage a version of a guardrail\\nCreate and manage a version of a guardrail\\nThe following topics discuss how to create a version of your guardrail when it\\'s ready for\\ndeployment, view information about it, and delete it when you no longer need it.\\nNote\\nGuardrail versions aren\\'t considered resources and therefore do not have an ARN. IAM\\nPolicies that apply to a guardrail apply to all of its versions.\\nTopics\\n• Create a version of a guardrail\\n• View information about guardrail versions\\n• Delete a version of a guardrail\\nCreate a version of a guardrail\\nTo learn how to create a version of a guardrail, select the tab corresponding to your method of\\nchoice and follow the steps.\\nDeploy a guardrail 404\\nAmazon Bedrock User Guide\\nConsole\\nTo create a version\\n'}\n",
      "{'question': 'How long might it take for new data to be available after ingestion?', 'ground_truth': 'It could take a few minutes for the vector embeddings of newly ingested data to be available in the vector store, especially if using Amazon OpenSearch Serverless.', 'question_type': 'simple', 'context': \"4. When data ingestion completes, a green success banner appears if it is successful.\\nNote\\nAfter data ingestion completes, it could take few minutes for the vector\\nembeddings of the newly ingested data to be available in the vector store if you use\\nAmazon OpenSearch Serverless.\\nSync your data source 601\\nAmazon Bedrock User Guide\\n5. You can choose a data source to view its Sync history. Select View warnings to see why a\\ndata ingestion job failed.\\nAPI\\nTo ingest a data source into the vector store you configured for your knowledge base, send a\\nStartIngestionJob request with a Agents for Amazon Bedrock build-time endpoint. Specify the\\nknowledgeBaseId and dataSourceId.\\nUse the ingestionJobId returned in the response in a GetIngestionJob request with a Agents\\nfor Amazon Bedrock build-time endpoint to track the status of the ingestion job. In addition,\\nspecify the knowledgeBaseId and dataSourceId.\\n• When the ingestion job finishes, the status in the response is COMPLETE.\\nNote\\n After data syncing completes, it could take a few minutes for the vector embeddings\\nof the newly synced data to reflect in your knowledge base if you use Amazon\\nOpenSearch Serverless.\\n• The statistics object in the response returns information about whether ingestion was\\nsuccessful or not for documents in the data source.\\nYou can also see information for all ingestion jobs for a data source by sending a\\nListIngestionJobs request with a Agents for Amazon Bedrock build-time endpoint. Specify the\\ndataSourceId and the knowledgeBaseId of the knowledge base that the data is being\\ningested to.\\n• Filter for results by specifying a status to search for in the filters object.\\n• Sort by the time that the job was started or the status of a job by specifying the sortBy\\nobject. You can sort in ascending or descending order.\\n• Set the maximum number of results to return in a response in the maxResults field. If there\\n are more results than the number you set, the response returns a nextToken that you can\\nsend in another ListIngestionJobs request to see the next batch of jobs.\\nSync your data source 602\\nAmazon Bedrock User Guide\\nTest a knowledge base in Amazon Bedrock\\nAfter you set up your knowledge base, you can test its behavior by sending queries and seeing the\\nresponses. You can also set query configurations to customize information retrieval. When you are\\nsatisfied with your knowledge base's behavior, you can then set up your application to query the\\nknowledge base or attach the knowledge base to an agent.\\nSelect a topic to learn more about it.\\nTopics\\n• Query the knowledge base and return results or generate responses\\n• Query configurations\\nQuery the knowledge base and return results or generate responses\\nTo learn how to query your knowledge base, select the tab corresponding to your method of choice\\nand follow the steps.\\nConsole\\nTo test your knowledge base\\n\"}\n",
      "{'question': 'How does the invokeModel function handle different model configurations, and what are the key parameters that can be customized when invoking the Amazon Titan Text generation model?', 'ground_truth': 'The invokeModel function in the provided JavaScript code allows for flexible configuration of the Amazon Titan Text generation model. It accepts two parameters: \\'prompt\\' (required) and \\'modelId\\' (optional, defaulting to \"amazon.titan-text-express-v1\"). The function prepares a payload object that includes the input text and textGenerationConfig. Key customizable parameters in the textGenerationConfig include:\\n\\n1. maxTokenCount: Set to 4096, determining the maximum number of tokens in the generated output.\\n2. stopSequences: An empty array, which can be populated to specify sequences that stop text generation.\\n3. temperature: Set to 0, controlling the randomness of the output (lower values make it more deterministic).\\n4. topP: Set to 1, influencing the diversity of the generated text.\\n\\nThese parameters allow users to fine-tune the model\\'s behavior for different use cases. The function then uses the AWS SDK to send an InvokeModelCommand with the prepared payload, processes the response, and returns the generated text.', 'question_type': 'complex', 'context': '} catch (SdkClientException e) {\\nSystem.err.printf(\"ERROR: Can\\'t invoke \\'%s\\'. Reason: %s\", modelId,\\ne.getMessage());\\nthrow new RuntimeException(e);\\n}\\n}\\npublic static void main(String[] args) {\\ninvokeModel();\\n}\\n}\\n• For API details, see InvokeModel in AWS SDK for Java 2.x API Reference.\\nAmazon Titan Text 1290\\nAmazon Bedrock User Guide\\nJavaScript\\nSDK for JavaScript (v3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport { FoundationModels } from \"../../config/foundation_models.js\";\\nimport {\\nBedrockRuntimeClient,\\nInvokeModelCommand,\\n} from \"@aws-sdk/client-bedrock-runtime\";\\n/**\\n* @typedef {Object} ResponseBody\\n* @property {Object[]} results\\n*/\\n/**\\n* Invokes an Amazon Titan Text generation model.\\n*\\n * @param {string} prompt - The input text prompt for the model to complete.\\n* @param {string} [modelId] - The ID of the model to use. Defaults to\\n\"amazon.titan-text-express-v1\".\\n*/\\nexport const invokeModel = async (\\nprompt,\\nmodelId = \"amazon.titan-text-express-v1\",\\n) => {\\n// Create a new Bedrock Runtime client instance.\\nconst client = new BedrockRuntimeClient({ region: \"us-east-1\" });\\n// Prepare the payload for the model.\\nAmazon Titan Text 1291\\nAmazon Bedrock User Guide\\nconst payload = {\\ninputText: prompt,\\ntextGenerationConfig: {\\nmaxTokenCount: 4096,\\nstopSequences: [],\\ntemperature: 0,\\ntopP: 1,\\n},\\n};\\n// Invoke the model with the payload and wait for the response.\\nconst command = new InvokeModelCommand({\\ncontentType: \"application/json\",\\nbody: JSON.stringify(payload),\\nmodelId,\\n});\\nconst apiResponse = await client.send(command);\\n// Decode and return the response.\\nconst decodedResponseBody = new TextDecoder().decode(apiResponse.body);\\n/** @type {ResponseBody} */\\n const responseBody = JSON.parse(decodedResponseBody);\\nreturn responseBody.results[0].outputText;\\n};\\n// Invoke the function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nconst prompt =\\n\\'Complete the following in one sentence: \"Once upon a time...\"\\';\\nconst modelId = FoundationModels.TITAN_TEXT_G1_EXPRESS.modelId;\\nconsole.log(`Prompt: ${prompt}`);\\nconsole.log(`Model ID: ${modelId}`);\\ntry {\\nconsole.log(\"-\".repeat(53));\\nconst response = await invokeModel(prompt, modelId);\\nconsole.log(response);\\n} catch (err) {\\nconsole.log(err);\\n}\\n}\\n• For API details, see InvokeModel in AWS SDK for JavaScript API Reference.\\nAmazon Titan Text 1292\\nAmazon Bedrock User Guide\\nPython\\nSDK for Python (Boto3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n# Use the native inference API to send a text message to Amazon Titan Text.\\nimport boto3\\n'}\n",
      "{'question': 'How can you list the available Amazon Bedrock foundation models?', 'ground_truth': 'You can list the available Amazon Bedrock foundation models using the ListFoundationModels API call, which is available in various AWS SDKs such as PHP, Python (Boto3), and Kotlin.', 'question_type': 'simple', 'context': '• For API details, see ListFoundationModels in AWS SDK for Kotlin API reference.\\nPHP\\nSDK for PHP\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nList the available Amazon Bedrock foundation models.\\npublic function listFoundationModels()\\n{\\n$result = $this->bedrockClient->listFoundationModels();\\nreturn $result;\\n}\\n• For API details, see ListFoundationModels in AWS SDK for PHP API Reference.\\nPython\\nSDK for Python (Boto3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nList the available Amazon Bedrock foundation models.\\ndef list_foundation_models(self):\\nBasics 1215\\nAmazon Bedrock User Guide\\n\"\"\"\\nList the available Amazon Bedrock foundation models.\\n:return: The list of available bedrock foundation models.\\n\"\"\"\\ntry:\\nresponse = self.bedrock_client.list_foundation_models()\\nmodels = response[\"modelSummaries\"]\\n logger.info(\"Got %s foundation models.\", len(models))\\nreturn models\\nexcept ClientError:\\nlogger.error(\"Couldn\\'t list foundation models.\")\\nraise\\n• For API details, see ListFoundationModels in AWS SDK for Python (Boto3) API Reference.\\nFor a complete list of AWS SDK developer guides and code examples, see Using this service with\\nan AWS SDK. This topic also includes information about getting started and details about previous\\nSDK versions.\\nScenarios for Amazon Bedrock using AWS SDKs\\nThe following code examples show you how to implement common scenarios in Amazon Bedrock\\nwith AWS SDKs. These scenarios show you how to accomplish specific tasks by calling multiple\\nfunctions within Amazon Bedrock or combined with other AWS services. Each scenario includes\\na link to the complete source code, where you can find instructions on how to set up and run the\\ncode.\\nScenarios target an intermediate level of experience to help you understand service actions in\\ncontext.\\nExamples\\n • Build and orchestrate generative AI applications with Amazon Bedrock and Step Functions\\nScenarios 1216\\nAmazon Bedrock User Guide\\nBuild and orchestrate generative AI applications with Amazon Bedrock and Step\\nFunctions\\nThe following code example shows how to build and orchestrate generative AI applications with\\nAmazon Bedrock and Step Functions.\\nPython\\nSDK for Python (Boto3)\\nThe Amazon Bedrock Serverless Prompt Chaining scenario demonstrates how AWS Step\\nFunctions, Amazon Bedrock, and Agents for Amazon Bedrock can be used to build and\\norchestrate complex, serverless, and highly scalable generative AI applications. It contains\\nthe following working examples:\\n• Write an analysis of a given novel for a literature blog. This example illustrates a simple,\\nsequential chain of prompts.\\n• Generate a short story about a given topic. This example illustrates how the AI can\\niteratively process a list of items that it previously generated.\\n'}\n",
      "{'question': 'In the Amazon Bedrock Prompt flow builder, how does the configuration of the MakePlaylist node enable dynamic playlist generation, and what steps ensure the flow can handle different user inputs?', 'ground_truth': 'The MakePlaylist node in the Amazon Bedrock Prompt flow builder enables dynamic playlist generation through several key configurations:\\n\\n1. It uses a parameterized prompt: \"Make me a {{genre}} playlist consisting of the following number of songs: {{number}}.\" This allows for flexible input.\\n\\n2. Two inputs are defined: \\'genre\\' (String type) and \\'number\\' (Number type), both mapped to a JSON object structure ($.data.genre and $.data.number respectively).\\n\\n3. The Flow input node is set to expect a JSON object, allowing structured user input.\\n\\n4. Connections are established from the Flow input to both \\'genre\\' and \\'number\\' inputs in the MakePlaylist node, ensuring data flow.\\n\\n5. The output of the MakePlaylist node is connected to the Flow output node.\\n\\nTo handle different user inputs, the flow is tested with a JSON object: {\"genre\": \"pop\", \"number\": 3}. This structure allows users to specify any genre and number of songs, making the flow adaptable to various playlist requests. The use of a model for inference on the prompt further enhances the system\\'s ability to generate diverse playlists based on user input.', 'question_type': 'complex', 'context': '1. Follow the steps under To create a flow in the Console tab at Create a flow in Amazon\\nBedrock. Enter the Prompt flow builder.\\n2. Set up the prompt node by doing the following:\\na. From the Prompt flow builder left pane, select the Nodes tab.\\nb. Drag a Prompt node into your flow in the center pane.\\nc. Select the Configure tab in the Prompt flow builder pane.\\nd. Enter MakePlaylist as the Node name.\\ne. Choose Define in node.\\nf. Set up the following configurations for the prompt:\\ni. Under Select model, select a model to run inference on the prompt.\\nii. In the Message text box, enter Make me a {{genre}} playlist consisting\\nof the following number of songs: {{number}}.. This creates two\\nvariables that will appear as inputs into the node.\\niii. (Optional) Modify the Inference configurations.\\ng. Expand the Inputs section. The names for the inputs are prefilled by the variables in the\\nprompt message. Configure the inputs as follows:\\nExample prompt flows 855\\nAmazon Bedrock User Guide\\n Name Type Expression\\ngenre String $.data.genre\\nnumber Number $.data.number\\nThis configuration means that the prompt node expects a JSON object containing a field\\ncalled genre that will be mapped to the genre input and a field called number that will\\nbe mapped to the number input.\\nh. You can\\'t modify the Output. It will be the response from the model, returned as a string.\\n3. Choose the Flow input node and select the Configure tab. Select Object as the Type. This\\nmeans that flow invocation will expect to receive a JSON object.\\n4. Connect your nodes to complete the flow by doing the following:\\na. Drag a connection from the output node of the Flow input node to the genre input in the\\nMakePlaylist prompt node.\\nb. Drag a connection from the output node of the Flow input node to the number input in\\nthe MakePlaylist prompt node.\\nc. Drag a connection from the output node of the modelCompletion output in the\\nMakePlaylist prompt node to the document input in the Flow output node.\\n 5. Choose Save to save your flow. Your flow should now be prepared for testing.\\n6. Test your flow by entering the following JSON object is the Test prompt flow pane on the\\nright. Choose Run and the flow should return a model response.\\n{\\n\"genre\": \"pop\",\\n\"number\": 3\\n}\\nCreate a flow with a condition node\\nThe following image shows a flow with one condition node returns one of three possible values\\nbased on the condition that is fulfilled:\\nExample prompt flows 856\\nAmazon Bedrock User Guide\\nTo build and test this flow in the console:\\n1. Follow the steps under To create a flow in the Console tab at Create a flow in Amazon\\nBedrock. Enter the Prompt flow builder.\\n2. Set up the condition node by doing the following:\\na. From the Prompt flow builder left pane, select the Nodes tab.\\nb. Drag a Condition node into your flow in the center pane.\\nc. Select the Configure tab in the Prompt flow builder pane.\\nd. Expand the Inputs section. Configure the inputs as follows:\\nName Type Expression\\n'}\n",
      "{'question': 'How do you create an alias for a prompt flow in Amazon Bedrock?', 'ground_truth': 'To create an alias for a prompt flow in Amazon Bedrock, sign in to the AWS Management Console, open the Amazon Bedrock console, select Prompt flows, choose a flow, click Create alias, enter a unique name and optional description, choose to create a new version or use an existing one, and select Create alias.', 'question_type': 'simple', 'context': '1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at Getting Started with the AWS\\nManagement Console.\\n2. Select Prompt flows from the left navigation pane. Then, choose a prompt flow in the\\nFlows section.\\n3. In the Aliases section, choose Create alias.\\n4. Enter a unique name for the alias and provide an optional description.\\nDeploy a prompt flow 871\\nAmazon Bedrock User Guide\\n5. Choose one of the following options:\\n• To create a new version, choose Create a new version and to associate it to this alias.\\n• To use an existing version, choose Use an existing version to associate this alias. From\\nthe dropdown menu, choose the version that you want to associate the alias to.\\n6. Select Create alias. A success banner appears at the top.\\nAPI\\nTo create a version of your prompt flow, send a CreateFlowVersion request (see link for\\n request and response formats and field details) with an Agents for Amazon Bedrock build-time\\nendpoint and specify the ARN or ID of the prompt flow as the flowIdentifier.\\nThe response returns an ID and ARN for the version. Versions are created incrementally, starting\\nfrom 1.\\nTo create an alias to point to a version of your prompt flow, send a CreateFlowAlias request (see\\nlink for request and response formats and field details) with an Agents for Amazon Bedrock\\nbuild-time endpoint.\\nThe following fields are required:\\nField Basic description\\nflowIdentifier The ARN or ID of the prompt flow for which\\nto create an alias.\\nname A name for the alias.\\nroutingConfiguration Specify the version to map the alias to in the\\nflowVersion field.\\nThe following fields are optional:\\nField Use-case\\ndescription To provide a description for the alias.\\nclientToken To prevent reduplication of the request.\\nDeploy a prompt flow 872\\nAmazon Bedrock User Guide\\n To learn how to manage versions and aliases of prompt flows, select from the following topics.\\nTopics\\n• Manage versions of prompt flows in Amazon Bedrock\\n• Manage aliases of prompt flows in Amazon Bedrock\\nManage versions of prompt flows in Amazon Bedrock\\nAfter you create a version of your prompt flow, you can view information about it or delete it.\\nTopics\\n• View information about versions of prompt flows in Amazon Bedrock\\n• Delete a version of a prompt flow in Amazon Bedrock\\nView information about versions of prompt flows in Amazon Bedrock\\nTo learn how to view information about the versions of a prompt flow, select the tab corresponding\\nto your method of choice and follow the steps.\\nConsole\\nTo view information about a version of a prompt flow\\n1. Open the AWS Management Console and sign in to your account. Navigate to Amazon\\nBedrock.\\n2. Select Flows from the left navigation pane. Then, in the Flows section, select a prompt\\nflow you want to view.\\n'}\n",
      "{'question': 'What are the key differences in the setup process for Pinecone and Redis Enterprise Cloud as vector stores for Amazon Bedrock, and how do their security configurations compare?', 'ground_truth': \"The setup processes for Pinecone and Redis Enterprise Cloud as vector stores for Amazon Bedrock have several key differences:\\n\\n1. API Key vs. Username/Password: Pinecone requires an API key, while Redis Enterprise Cloud uses a username and password combination.\\n\\n2. Secret Configuration: For Pinecone, you create a secret with a single key-value pair (apiKey), whereas Redis Enterprise Cloud requires multiple keys in the secret (username and potentially others).\\n\\n3. TLS Requirement: Redis Enterprise Cloud explicitly requires enabling TLS, which is not mentioned for Pinecone.\\n\\n4. Vector Index Setup: Redis Enterprise Cloud requires specifying a vector index name, vector field, and text field, which are not mentioned in the Pinecone setup.\\n\\n5. Dimensions Specification: The Redis setup includes a table of vector dimensions for different models, which is not provided for Pinecone.\\n\\nSecurity configurations also differ:\\n- Pinecone focuses on API key management and KMS key permissions.\\n- Redis Enterprise Cloud emphasizes TLS enablement and more detailed database access credentials.\\n\\nBoth services use AWS Secrets Manager for secure credential storage, but the content and structure of the secrets differ based on each service's authentication requirements.\", 'question_type': 'complex', 'context': \"To access your Pinecone index, you must provide your Pinecone API key to Amazon Bedrock\\nthrough the AWS Secrets Manager.\\nTo set up a secret for your Pinecone configuration\\n1. Follow the steps at Create an AWS Secrets Manager secret, setting the key as apiKey and\\nthe value as the API key to access your Pinecone index.\\n2. To find your API key, open your Pinecone console and select API Keys.\\n3. After you create the secret, take note of the ARN of the KMS key.\\n4. Attach permissions to your service role to decrypt the ARN of the KMS key by following\\nthe steps in Permissions to decrypt an AWS Secrets Manager secret for the vector store\\ncontaining your knowledge base.\\nSet up a vector index 541\\nAmazon Bedrock User Guide\\n5. Later, when you create your knowledge base, enter the ARN in the Credentials secret ARN\\nfield.\\nRedis Enterprise Cloud\\nNote\\nIf you use Redis Enterprise Cloud, you agree to authorize AWS to access the designated\\n third-party source on your behalf in order to provide vector store services to you. You're\\nresponsible for complying with any third-party terms applicable to use and transfer of\\ndata from the third-party service.\\nFor detailed documentation on setting up a vector store in Redis Enterprise Cloud, see\\nIntegrating Redis Enterprise Cloud with Amazon Bedrock.\\nWhile you set up the vector store, take note of the following information, which you will fill out\\nwhen you create a knowledge base:\\n• Endpoint URL – The public endpoint URL for your database.\\n• Vector index name – The name of the vector index for your database.\\n• Vector field – The name of the field where the vector embeddings will be stored. Refer to the\\nfollowing table to determine how many dimensions the vector should contain.\\nModel Dimensions\\nTitan G1 Embeddings - Text 1,536\\nTitan V2 Embeddings - Text 1,024\\nCohere Embed English 1,024\\nCohere Embed Multilingual 1,024\\n • Text field – The name of the field where the Amazon Bedrock stores the chunks of raw text.\\n• Bedrock-managed metadata field – The name of the field where Amazon Bedrock stores\\nmetadata related to your knowledge base.\\nSet up a vector index 542\\nAmazon Bedrock User Guide\\nTo access your Redis Enterprise Cloud cluster, you must provide your Redis Enterprise Cloud\\nsecurity configuration to Amazon Bedrock through the AWS Secrets Manager.\\nTo set up a secret for your Redis Enterprise Cloud configuration\\n1. Enable TLS to use your database with Amazon Bedrock by following the steps at Transport\\nLayer Security (TLS).\\n2. Follow the steps at Create an AWS Secrets Manager secret. Set up the following keys with\\nthe appropriate values from your Redis Enterprise Cloud configuration in the secret:\\n• username – The username to access your Redis Enterprise Cloud database. To find your\\nusername, look under the Security section of your database in the Redis Console.\\n\"}\n",
      "{'question': 'How can you list available foundation models in Amazon Bedrock using Go?', 'ground_truth': 'You can list available foundation models in Amazon Bedrock using Go by creating a Bedrock client and calling the ListFoundationModels method. The code example shows how to create a Bedrock client using the AWS SDK for Go v2, and then use the ListFoundationModels method to retrieve and print the list of available foundation models.', 'question_type': 'simple', 'context': 'in the AWS Code Examples Repository.\\npackage main\\nimport (\\n\"context\"\\n\"fmt\"\\n\"github.com/aws/aws-sdk-go-v2/config\"\\n\"github.com/aws/aws-sdk-go-v2/service/bedrock\"\\n)\\nconst region = \"us-east-1\"\\n// main uses the AWS SDK for Go (v2) to create an Amazon Bedrock client and\\n// list the available foundation models in your account and the chosen region.\\n// This example uses the default settings specified in your shared credentials\\n// and config files.\\nfunc main() {\\nsdkConfig, err := config.LoadDefaultConfig(context.TODO(),\\nconfig.WithRegion(region))\\nif err != nil {\\nfmt.Println(\"Couldn\\'t load default configuration. Have you set up your\\nAWS account?\")\\nfmt.Println(err)\\nreturn\\nAmazon Bedrock 1194\\nAmazon Bedrock User Guide\\n}\\nbedrockClient := bedrock.NewFromConfig(sdkConfig)\\nresult, err := bedrockClient.ListFoundationModels(context.TODO(),\\n&bedrock.ListFoundationModelsInput{})\\nif err != nil {\\nfmt.Printf(\"Couldn\\'t list foundation models. Here\\'s why: %v\\\\n\", err)\\nreturn\\n}\\n if len(result.ModelSummaries) == 0 {\\nfmt.Println(\"There are no foundation models.\")}\\nfor _, modelSummary := range result.ModelSummaries {\\nfmt.Println(*modelSummary.ModelId)\\n}\\n}\\n• For API details, see ListFoundationModels in AWS SDK for Go API Reference.\\nJavaScript\\nSDK for JavaScript (v3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport {\\nBedrockClient,\\nListFoundationModelsCommand,\\n} from \"@aws-sdk/client-bedrock\";\\nconst REGION = \"us-east-1\";\\nconst client = new BedrockClient({ region: REGION });\\nAmazon Bedrock 1195\\nAmazon Bedrock User Guide\\nexport const main = async () => {\\nconst command = new ListFoundationModelsCommand({});\\nconst response = await client.send(command);\\nconst models = response.modelSummaries;\\n console.log(\"Listing the available Bedrock foundation models:\");\\nfor (let model of models) {\\nconsole.log(\"=\".repeat(42));\\nconsole.log(` Model: ${model.modelId}`);\\nconsole.log(\"-\".repeat(42));\\nconsole.log(` Name: ${model.modelName}`);\\nconsole.log(` Provider: ${model.providerName}`);\\nconsole.log(` Model ARN: ${model.modelArn}`);\\nconsole.log(` Input modalities: ${model.inputModalities}`);\\nconsole.log(` Output modalities: ${model.outputModalities}`);\\nconsole.log(` Supported customizations: ${model.customizationsSupported}`);\\nconsole.log(` Supported inference types: ${model.inferenceTypesSupported}`);\\nconsole.log(` Lifecycle status: ${model.modelLifecycle.status}`);\\nconsole.log(\"=\".repeat(42) + \"\\\\n\");\\n}\\nconst active = models.filter(\\n(m) => m.modelLifecycle.status === \"ACTIVE\",\\n).length;\\nconst legacy = models.filter(\\n(m) => m.modelLifecycle.status === \"LEGACY\",\\n).length;\\nconsole.log(\\n`There are ${active} active and ${legacy} legacy foundation models in\\n${REGION}.`,\\n);\\nreturn response;\\n};\\n'}\n",
      "{'question': \"How does Amazon Bedrock's versioning and aliasing system for prompt flows facilitate efficient deployment and management of different iterations?\", 'ground_truth': \"Amazon Bedrock's versioning and aliasing system for prompt flows facilitates efficient deployment and management by allowing users to create immutable snapshots (versions) of their prompt flows and use aliases to point to specific versions. When a prompt flow is created, it starts with a working draft (DRAFT) and a test alias (TSTALIASID). Users can iterate on the working draft and create numbered versions (starting from 1) when satisfied. Aliases can be created to point to specific versions, enabling quick switching between different iterations without tracking version numbers. This system allows for easy rollbacks by changing an alias to point to a previous version, and supports smooth updates to production by creating new versions from the working draft and updating the alias accordingly. This approach provides flexibility in managing different stages of prompt flow development and deployment.\", 'question_type': 'complex', 'context': 'Provide the input in the document field, provide a name for the input in the nodeName field,\\nand provide a name for the input in the nodeOutputName field.\\nThe response is returned in a stream. Each event returned contains output from a node in the\\ndocument field, the node that was processed in the nodeName field, and the type of node in\\nthe nodeType field. These events are of the following format:\\n{\\n\"flowOutputEvent\": {\\n\"content\": {\\n\"document\": \"JSON-formatted string\"\\n},\\n\"nodeName\": \"string\",\\n\"nodeType\": \"string\"\\n}\\n}\\nIf the prompt flow finishes, a flowCompletionEvent field with the completionReason is\\nalso returned. If there\\'s an error, the corresponding error field is returned.\\nDeploy a prompt flow in Amazon Bedrock\\nNote\\nPrompt flows is in preview and is subject to change.\\nWhen you first create a prompt flow, a working draft version (DRAFT) and a test alias\\n(TSTALIASID) that points to the working draft version are created. When you make changes to\\n your prompt flow, the changes apply to the working draft, and so it is the latest version of your\\nprompt flow. You iterate on your working draft until you\\'re satisfied with the behavior of your\\nprompt flow. Then, you can set up your prompt flow for deployment by creating versions of your\\nprompt flow.\\nA version is a snapshot that preserves the resource as it exists at the time it was created. You\\ncan continue to modify the working draft and create versions of your prompt flow as necessary.\\nDeploy a prompt flow 870\\nAmazon Bedrock User Guide\\nAmazon Bedrock creates versions in numerical order, starting from 1. Versions are immutable\\nbecause they act as a snapshot of your prompt flow at the time you created it. To make updates\\nto a prompt flow that you\\'ve deployed to production, you must create a new version from the\\nworking draft and make calls to the alias that points to that version.\\nTo deploy your prompt flow, you must create an alias that points to a version of your prompt\\n flow. Then, you make InvokeFlow requests to that alias. With aliases, you can switch efficiently\\nbetween different versions of your prompt flow without keeping track of the version. For example,\\nyou can change an alias to point to a previous version of your prompt flow if there are changes that\\nyou need to revert quickly.\\nTo deploy your prompt flow\\nCreate an alias and version of your prompt flow. Select the tab corresponding to your method of\\nchoice and follow the steps.\\nConsole\\nTo create a version of your Prompt flows\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at Getting Started with the AWS\\nManagement Console.\\n2. Select Prompt flows from the left navigation pane. Then, choose a prompt flow in the\\nPrompt flows section.\\n3. In the Versions section, choose Publish version.\\n4. After the version is published, a success banner appears at the top.\\nTo create an alias for your Prompt flows\\n'}\n",
      "{'question': 'What are the three rating methods available for model evaluation in Amazon Bedrock?', 'ground_truth': 'The three rating methods available for model evaluation in Amazon Bedrock are: Likert scale comparison of multiple model outputs, Choice buttons (radio button), and Ordinal rank.', 'question_type': 'simple', 'context': 'Amazon Bedrock User Guide\\nTo help workers complete their assigned tasks, you can provide instructions in two places.\\nProvide a good description for each evaluation and rating method\\nThe descriptions should provide a succinct explanation of the metrics selected. The description\\nshould expand on the metric, and make clear how you want workers to evaluate the selected rating\\nmethod. To see examples of how each rating method is shown in the worker UI, see Summary of\\navailable rating methods .\\nProvide your workers overall evaluation instructions\\nThese instructions are shown on the same webpage where workers complete a task. You can use\\nthis space to provide high level direction for the model evaluation job, and to describe the ground\\ntruth responses if you\\'ve included them in your prompt dataset.\\nSummary of available rating methods\\nIn each of the following sections, you can see an example of the rating methods your work team\\n saw in the evaluation UI, and also how those results are saved in Amazon S3.\\nLikert scale, comparison of multiple model outputs\\nHuman evaluators indicate their preference between the two responses from the model on a 5\\npoint Likert scale according to your instructions. The results in the final report will be shown as a\\nhistogram of preference strength ratings from the evaluators over your whole dataset.\\nMake sure you define the important points of the 5 point scale in your instructions, so your\\nevaluators know how to rate responses based on your expectations.\\nRating methods 473\\nAmazon Bedrock User Guide\\nJSON output\\nThe first child-key under evaluationResults is where the selected rating method is returned.\\nIn the output file saved to your Amazon S3 bucket, the results from each worker are saved to the\\n\"evaluationResults\": \"comparisonLikertScale\" key value pair.\\nChoice buttons (radio button)\\nChoice buttons allow a human evaluator to indicate their one preferred response over another\\n response. Evaluators indicate their preference between two responses according to your\\ninstructions with radio buttons. The results in the final report will be shown as a percentage of\\nresponses that workers preferred for each model. Be sure to explain your evaluation method clearly\\nin the instructions.\\nRating methods 474\\nAmazon Bedrock User Guide\\nJSON output\\nThe first child-key under evaluationResults is where the selected rating method is returned.\\nIn the output file saved to your Amazon S3 bucket, the results from each worker are saved to the\\n\"evaluationResults\": \"comparisonChoice\" key value pair.\\nOrdinal rank\\nOrdinal rank allows a human evaluator to rank their preferred responses to a prompt in order\\nstarting at 1 according to your instructions. The results in the final report will be shown as a\\nhistogram of the rankings from the evaluators over the whole dataset. Be sure to define what a\\nrank of 1 means in your instructions.\\nRating methods 475\\nAmazon Bedrock User Guide\\n'}\n",
      "{'question': 'How might adjusting the learning rate and batch size for Amazon Titan Text Premier model customization affect performance, and what precautions should be taken when modifying these parameters?', 'ground_truth': \"When adjusting the learning rate and batch size for Amazon Titan Text Premier model customization, it's important to consider their interdependence and potential impacts. The recommended learning rate range is between 1.00E-07 and 1.00E-05, with a default of 1.00E-06. A larger learning rate may speed up convergence but could negatively impact core model capabilities. For batch size changes, it's crucial to adjust the learning rate accordingly using the formula: newLearningRate = oldLearningRate x newBatchSize / oldBatchSize. However, it's important to note that Titan Text Premier currently only supports a mini-batch size of 1 for customer fine-tuning. To ensure optimal results, it's recommended to validate the training data quality using a small sub-sample (around 100 samples) and monitor validation metrics before submitting a larger training job. This approach helps in assessing the impact of hyperparameter adjustments on model performance while minimizing the risk of degrading the model's capabilities across various tasks.\", 'question_type': 'complex', 'context': 'about the hyperparameters that you can set, see Amazon Titan text model customization\\nhyperparameters.\\nImpact on other tasks types\\nIn general, the larger the training dataset, the better the performance for a specific task. However,\\ntraining for a specific task might make the model perform worse on different tasks, especially if\\nyou use a lot of examples. For example, if the training dataset for a summarization task contains\\n100,000 samples, the model might perform worse on a classification task).\\nModel size\\nIn general, the larger the model, the better the task performs given limited training data.\\nIf you are using the model for a classification task, you might see relatively small gains for few-shot\\nfine-tuning (less than 100 samples), especially if the number of classes is relatively small (less than\\n100).\\nEpochs\\nWe recommend using the following metrics to determine the number of epochs to set:\\n 1. Validation output accuracy – Set the number of epochs to one that yields a high accuracy.\\n2. Training and validation loss – Determine the number of epochs after which the training and\\nvalidation loss becomes stable. This corresponds to when the model converges. Find the training\\nloss values in the step_wise_training_metrics.csv and validation_metrics.csv files.\\nAmazon Titan Text Premier 959\\nAmazon Bedrock User Guide\\nBatch size\\nWhen you change the batch size, we recommend that you change the learning rate using the\\nfollowing formula:\\nnewLearningRate = oldLearningRate x newBatchSize / oldBatchSize\\nTitan Text Premier model currently only supports mini-batch size of 1 for customer finetuning.\\nLearning rate\\nTo get the best results from finetuning capabilities, we recommend using a learning rate between\\n1.00E-07 and 1.00E-05. A good starting point is the recommended default value of 1.00E-06.\\nA larger learning rate may help training converge faster, however, it may adversely impact core\\n model capabilities.\\nValidate your training data with small sub-sample - To validate the quality of your training data,\\nwe recommend experimenting with a smaller dataset (~100s of samples) and monitoring the\\nvalidation metrics, before submitting the training job with larger training dataset.\\nLearning warmup steps\\nWe recommend the default value of 5.\\nTroubleshooting\\nThis section summarizes errors that you might encounter and what to check if you do.\\nPermissions issues\\nIf you encounter an issue with permissions to access an Amazon S3 bucket, check that the\\nfollowing are true:\\n1. If the Amazon S3 bucket uses a CM-KMS key for Server Side encryption, ensure that the IAM role\\npassed to Amazon Bedrock has kms:Decrypt permissions for the AWS KMS key. For example,\\nsee Allow a user to enccrypt and decrypt with any AWS KMS key in a specific AWS account.\\n2. The Amazon S3 bucket is in the same region as the Amazon Bedrock model customization job.\\n'}\n",
      "{'question': 'What is the rate limit set for the web crawler?', 'ground_truth': 'The rate limit for the web crawler is set to 50.', 'question_type': 'simple', 'context': '\"sourceConfiguration\": {\\n\"urlConfiguration\": {\\n\"seedUrls\": [{\\nWeb Crawler 599\\nAmazon Bedrock User Guide\\n\"url\": \"https://www.examplesite.com\"\\n}]\\n}\\n},\\n\"crawlerConfiguration\": {\\n\"crawlerLimits\": {\\n\"rateLimit\": 50\\n},\\n\"scope\": \"HOST_ONLY\",\\n\"inclusionFilters\": [\\n\"https://www\\\\.examplesite\\\\.com/.*\\\\.html\"\\n],\\n\"exclusionFilters\": [\\n\"https://www\\\\.examplesite\\\\.com/contact-us\\\\.html\"\\n]\\n}\\n},\\n\"type\": \"WEB\"\\n}\\nSync your data source with your Amazon Bedrock knowledge\\nbase\\nAfter you create your knowledge base, you ingest your data source/sources into your knowledge\\nbase so that they\\'re indexed and are able to be queried. Ingestion converts the raw data in your\\ndata source into vector embeddings. Before you begin ingestion, check that your data source fulfills\\nthe following conditions:\\n• You have configured the connection information for your data source. To configure a data source\\nconnector to crawl your data from your data source repository, see Supported data source\\nconnectors.\\n • The files are in supported formats. For more information, see Support document formats.\\n• The files don\\'t exceed the maximum file size of 50 MB. For more information, see Knowledge\\nbase quotas.\\n• If your data source contains metadata files, check the following conditions to ensure that the\\nmetadata files aren\\'t ignored:\\n• Each .metadata.json file shares the same name as the source file that it\\'s associated with.\\nSync your data source 600\\nAmazon Bedrock User Guide\\n• If the vector index for your knowledge base is in an Amazon OpenSearch Serverless vector\\nstore, check that the vector index is configured with the faiss engine. If the vector index is\\nconfigured with the nmslib engine, you\\'ll have to do one of the following:\\n• Create a new knowledge base in the console and let Amazon Bedrock automatically create a\\nvector index in Amazon OpenSearch Serverless for you.\\n• Create another vector index in the vector store and select faiss as the Engine. Then create\\n a new knowledge base and specify the new vector index.\\n• If the vector index for your knowledge base is in an Amazon Aurora database cluster, check\\nthat the table for your index contains a column for each metadata property in your metadata\\nfiles before starting ingestion.\\nNote\\nEach time you add, modify, or remove files from your data source, you must sync the data\\nsource so that it is re-indexed to the knowledge base. Syncing is incremental, so Amazon\\nBedrock only processes added, modified, or deleted documents since the last sync.\\nTo learn how to ingest your data sources into your knowledge base, Select the tab corresponding\\nto your method of choice and follow the steps.\\nConsole\\nTo ingest your data sources\\n1. Open the Amazon Bedrock console at https://console.aws.amazon.com/bedrock/.\\n2. From the left navigation pane, select Knowledge base and choose your knowledge base.\\n3. In the Data source section, select Sync to begin data ingestion.\\n'}\n",
      "{'question': 'How does the AmazonDataZoneBedrockPermissionsBoundary policy balance security and functionality for Amazon Bedrock Studio users, and what potential limitations might this create for advanced users?', 'ground_truth': 'The AmazonDataZoneBedrockPermissionsBoundary policy balances security and functionality by granting specific, limited permissions to Amazon Bedrock Studio users. It allows read and write access to essential AWS services like S3, Bedrock, OpenSearch Serverless, and Lambda, but only for resources managed by Amazon Bedrock Studio. This approach enhances security by preventing unauthorized access to other resources while still enabling core functionalities.\\n\\nHowever, this policy might create limitations for advanced users. For instance, they can only access S3 buckets with names starting with \"br-studio-\" and belonging to their account. Similarly, they can invoke Bedrock models but can\\'t create or modify them. The policy also restricts actions to specific services, potentially limiting users who need to integrate with other AWS services not covered by this policy.\\n\\nWhile this approach is secure and suitable for most use cases, advanced users might find it restrictive if they need to perform actions outside the predefined scope, such as accessing non-Bedrock Studio managed resources or using AWS services not explicitly allowed by the policy.', 'question_type': 'complex', 'context': 'When you create Amazon Bedrock Studio projects, apps, and components, Amazon Bedrock Studio\\napplies this permissions boundary to the IAM roles produced when creating those resources.\\nAmazon Bedrock Studio uses the AmazonDataZoneBedrockPermissionsBoundary managed\\npolicy to limit permissions of the provisioned IAM principal it is attached to. Principals might\\ntake the form of the user roles that Amazon DataZone can assume on behalf of Amazon Bedrock\\nStudio users, and then conduct actions such as reading and writing Amazon S3 objects or invoking\\nAmazon Bedrock agents.\\nThe AmazonDataZoneBedrockPermissionsBoundary policy grants read and write access for\\nAmazon Bedrock Studio to services such as Amazon S3, Amazon Bedrock, Amazon OpenSearch\\nServerless, and AWS Lambda. The policy also gives read and write permissions to some\\ninfrastructure resources that are required to use these services such as AWS Secrets Manager\\nsecrets, Amazon CloudWatch log groups, and AWS KMS keys.\\n This policy consists of the following sets of permissions.\\n• s3 – Allows read and write access to objects in Amazon S3 buckets that are managed by Amazon\\nBedrock Studio.\\n• bedrock – Grants the ability to use Amazon Bedrock agents, knowledge bases, and guardrails\\nthat are managed by Amazon Bedrock Studio.\\n• aoss – Allows API access to Amazon OpenSearch Serverless collections that are managed by\\nAmazon Bedrock Studio.\\n• lambda – Grants the ability to invoke AWS Lambda functions that are managed by Amazon\\nBedrock Studio.\\n• secretsmanager – Allows read and write access to AWS Secrets Manager secrets that are\\nmanaged by Amazon Bedrock Studio.\\n• logs – Provides write access to Amazon CloudWatch Logs that are managed by Amazon Bedrock\\nStudio.\\nIdentity-based policy examples 1087\\nAmazon Bedrock User Guide\\n• kms – Grants access to use AWS KMS keys for encrypting Amazon Bedrock Studio data.\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AccessS3Buckets\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n \"s3:ListBucket\",\\n\"s3:ListBucketVersions\",\\n\"s3:GetObject\",\\n\"s3:PutObject\",\\n\"s3:DeleteObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:DeleteObjectVersion\"\\n],\\n\"Resource\": \"arn:aws:s3:::br-studio-${aws:PrincipalAccount}-*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceAccount\": \"${aws:PrincipalAccount}\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"AccessOpenSearchCollections\",\\n\"Effect\": \"Allow\",\\n\"Action\": \"aoss:APIAccessAll\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceAccount\": \"${aws:PrincipalAccount}\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"InvokeBedrockModels\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"bedrock:InvokeModel\",\\n\"bedrock:InvokeModelWithResponseStream\"\\n],\\nIdentity-based policy examples 1088\\nAmazon Bedrock User Guide\\n\"Resource\": \"arn:aws:bedrock:*::foundation-model/*\"\\n},\\n{\\n\"Sid\": \"AccessBedrockResources\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"bedrock:InvokeAgent\",\\n\"bedrock:Retrieve\",\\n\"bedrock:StartIngestionJob\",\\n\"bedrock:GetIngestionJob\",\\n\"bedrock:ListIngestionJobs\",\\n\"bedrock:ApplyGuardrail\",\\n\"bedrock:ListPrompts\",\\n'}\n",
      "{'question': 'How can you invoke Cohere Command R on Amazon Bedrock?', 'ground_truth': 'You can invoke Cohere Command R on Amazon Bedrock using the Invoke Model API.', 'question_type': 'simple', 'context': 'an AWS SDK. This topic also includes information about getting started and details about previous\\nSDK versions.\\nInvoke Cohere Command R and R+ on Amazon Bedrock using the Invoke Model\\nAPI\\nThe following code examples show how to send a text message to Cohere Command R and R+,\\nusing the Invoke Model API.\\n.NET\\nAWS SDK for .NET\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Use the native inference API to send a text message to Cohere Command R.\\nusing System;\\nusing System.IO;\\nusing System.Text.Json;\\nusing System.Text.Json.Nodes;\\nusing Amazon;\\nusing Amazon.BedrockRuntime;\\nusing Amazon.BedrockRuntime.Model;\\n// Create a Bedrock Runtime client in the AWS Region you want to use.\\nvar client = new AmazonBedrockRuntimeClient(RegionEndpoint.USEast1);\\n// Set the model ID, e.g., Command R.\\nvar modelId = \"cohere.command-r-v1:0\";\\n// Define the user message.\\n var userMessage = \"Describe the purpose of a \\'hello world\\' program in one line.\";\\nCohere Command 1390\\nAmazon Bedrock User Guide\\n//Format the request payload using the model\\'s native structure.\\nvar nativeRequest = JsonSerializer.Serialize(new\\n{\\nmessage = userMessage,\\nmax_tokens = 512,\\ntemperature = 0.5\\n});\\n// Create a request with the model ID and the model\\'s native request payload.\\nvar request = new InvokeModelRequest()\\n{\\nModelId = modelId,\\nBody = new MemoryStream(System.Text.Encoding.UTF8.GetBytes(nativeRequest)),\\nContentType = \"application/json\"\\n};\\ntry\\n{\\n// Send the request to the Bedrock Runtime and wait for the response.\\nvar response = await client.InvokeModelAsync(request);\\n// Decode the response body.\\nvar modelResponse = await JsonNode.ParseAsync(response.Body);\\n// Extract and print the response text.\\nvar responseText = modelResponse[\"text\"] ?? \"\";\\nConsole.WriteLine(responseText);\\n}\\ncatch (AmazonBedrockRuntimeException e)\\n{\\n Console.WriteLine($\"ERROR: Can\\'t invoke \\'{modelId}\\'. Reason: {e.Message}\");\\nthrow;\\n}\\n• For API details, see InvokeModel in AWS SDK for .NET API Reference.\\nCohere Command 1391\\nAmazon Bedrock User Guide\\nJava\\nSDK for Java 2.x\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Use the native inference API to send a text message to Cohere Command R.\\nimport org.json.JSONObject;\\nimport org.json.JSONPointer;\\nimport software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;\\nimport software.amazon.awssdk.core.SdkBytes;\\nimport software.amazon.awssdk.core.exception.SdkClientException;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.bedrockruntime.BedrockRuntimeClient;\\npublic class Command_R_InvokeModel {\\npublic static String invokeModel() {\\n// Create a Bedrock Runtime client in the AWS Region you want to use.\\n'}\n",
      "{'question': \"In an OpenAPI schema for an action group, how do the 'parameters' and 'requestBody' sections differ in their structure and purpose, and what key information should be included in each to guide an AI agent's interaction with users?\", 'ground_truth': \"The 'parameters' and 'requestBody' sections in an OpenAPI schema for an action group serve different purposes and have distinct structures:\\n\\n1. Parameters:\\n- Structure: An array of objects, each representing a parameter.\\n- Purpose: Define information needed from the user that's typically part of the URL or query string.\\n- Key information:\\n  a) 'name': The parameter's identifier (required)\\n  b) 'description': Explains the parameter's purpose (required)\\n  c) 'required': Boolean indicating if it's mandatory (optional)\\n  d) 'schema': Defines the data type and format (optional)\\n\\n2. RequestBody:\\n- Structure: An object with 'required', 'content', and nested 'schema' fields.\\n- Purpose: Specifies the structure of data sent in the request body, typically for POST or PUT requests.\\n- Key information:\\n  a) 'required': Boolean indicating if the request body is mandatory (optional)\\n  b) 'content': Specifies the media type and schema of the body\\n  c) 'schema': Defines properties of the request body, including:\\n     - 'type': Data type of each property\\n     - 'description': Explains each property's purpose\\n\\nBoth sections guide the AI agent in gathering necessary information from users. The 'parameters' section helps the agent understand what to ask for URL or query parameters, while the 'requestBody' section informs the agent about the structure of data needed in the request body. This allows the agent to effectively elicit all required information from users to make a successful API call.\", 'question_type': 'complex', 'context': 'order to fulfill a task. Each property contains the following fields:\\n• type – (Required for each property) The data type of the response field.\\n• description – (Optional) Describes the property. The agent can use this information to\\ndetermine the information that it needs to return to the end user.\\nparameters\\n\"parameters\": [\\n{\\n\"name\": \"string\",\\n\"description\": \"string\",\\n\"required\": boolean,\\n\"schema\": {\\n...\\n}\\n},\\n...\\n]\\nYour agent uses the following fields to determine the information it must get from the end user\\nto perform the action group\\'s requirements.\\n• name – (Required) The name of the parameter.\\n• description – (Required) A description of the parameter. Use this field to help the agent\\nunderstand how to elicit this parameter from the agent user or determine that it already has\\nthat parameter value from prior actions or from the user’s request to the agent.\\n• required – (Optional) Whether the parameter is required for the API request. Use this\\n field to indicate to the agent whether this parameter is needed for every invocation or if it\\'s\\noptional.\\n• schema – (Optional) The definition of input and output data types. For more information, see\\nData Models (Schemas) on the Swagger website.\\nDefining actions in the action group 663\\nAmazon Bedrock User Guide\\nrequestBody\\nFollowing is the general structure of a requestBody field:\\n\"requestBody\": {\\n\"required\": boolean,\\n\"content\": {\\n\"<media type>\": {\\n\"schema\": {\\n\"properties\": {\\n\"<property>\": {\\n\"type\": \"string\",\\n\"description\": \"string\"\\n},\\n...\\n}\\n}\\n}\\n}\\n}\\nThe following list describes each field:\\n• required – (Optional) Whether the request body is required for the API request.\\n• content – (Required) The content of the request body.\\n• <media type> – (Optional) The format of the request body. For more information, see\\nMedia types on the Swagger website.\\n• schema – (Optional) Defines the data type of the request body and its fields.\\n • properties – (Optional) Your agent uses properties that you define in the schema to\\ndetermine the information it must get from the end user to make the API request. Each\\nproperty contains the following fields:\\n• type – (Optional) The data type of the request field.\\n• description – (Optional) Describes the property. The agent can use this information to\\ndetermine the information it needs to return to the end user.\\nTo learn how to add the OpenAPI schema you created while creating the action group, see Add an\\naction group to your agent in Amazon Bedrock.\\nDefining actions in the action group 664\\nAmazon Bedrock User Guide\\nExample API schemas\\nThe following example provides a simple OpenAPI schema in YAML format that gets the weather\\nfor a given location in Celsius.\\nopenapi: 3.0.0\\ninfo:\\ntitle: GetWeather API\\nversion: 1.0.0\\ndescription: gets weather\\npaths:\\n/getWeather/{location}/:\\nget:\\nsummary: gets weather in Celsius\\ndescription: gets weather in Celsius\\noperationId: getWeather\\nparameters:\\n'}\n",
      "{'question': 'What permissions does Amazon Bedrock need for agent memory encryption?', 'ground_truth': 'Amazon Bedrock needs permissions to generate encrypted data keys, use them to encrypt agent memory, and re-encrypt the generated data key with different encryption contexts. Specifically, it requires \"kms:GenerateDataKeyWithoutPlainText\" and \"kms:ReEncrypt*\" actions.', 'question_type': 'simple', 'context': '\"Resource\": \"arn:aws:kms:${region}:${account-id}:key/${key-id}\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"kms:EncryptionContext:aws:bedrock:arn\":\\n\"arn:aws:bedrock:${region}:${account-id}:agent/${agent-id}\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"Allow the service role to use the key to encrypt and decrypt\\nAgent resources\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::${account-id}:role/${role}\"\\n},\\n\"Action\": [\\n\"kms:GenerateDataKey*\",\\n\"kms:Decrypt\",\\n],\\n\"Resource\": \"arn:aws:kms:${region}:${account-id}:key/${key-id}\"\\n},\\n{\\n\"Sid\": \"Allow the attachment of persistent resources\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"bedrock.amazonaws.com\"\\n},\\n\"Action\": [\\n\"kms:CreateGrant\",\\n\"kms:ListGrants\",\\n\"kms:RevokeGrant\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"Bool\": {\\n\"kms:GrantIsForAWSResource\": \"true\"\\n}\\n}\\n}\\n]\\nData encryption 1041\\nAmazon Bedrock User Guide\\n}\\nPermissions for agent memory\\nIf you\\'ve enabled memory for your agent and if you encrypt agent sessions with a customer\\n managed key, you must configure the following key policy and the calling identity IAM permissions\\nto configure your customer managed key.\\nCustomer managed key policy\\nAmazon Bedrock uses these permissions to generate encrypted data keys and then use the\\ngenerated keys to encrypt agent memory. Amazon Bedrock also needs permissions to re-encrypt\\nthe the generated data key with different encryption contexts. Re-encrypt permissions are also\\nused when customer managed key transitions between another customer managed key or service\\nowned key. For more information, see Hierarchical Keyring.\\nReplace the $region, account-id, and ${caller-identity-role} with appropriate values.\\n{\\n\"Version\": \"2012-10-17\",\\n{\\n\"Sid\": \"Allow access for bedrock to enable long term memory\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": [\\n\"bedrock.amazonaws.com\",\\n],\\n},\\n\"Action\": [\\n\"kms:GenerateDataKeyWithoutPlainText\",\\n\"kms:ReEncrypt*\"\\n],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"$account-id\"\\n},\\n\"ArnLike\": {\\n \"aws:SourceArn\": \"arn:aws:bedrock:$region:$account-id:agent-alias/*\"\\n}\\n}\\n\"Resource\": \"*\"\\n},\\nData encryption 1042\\nAmazon Bedrock User Guide\\n{\\n\"Sid\": \"Allow the caller identity control plane permissions for long term\\nmemory\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::${account-id}:role/${caller-identity-role}\"\\n},\\n\"Action\": [\\n\"kms:GenerateDataKeyWithoutPlainText\",\\n\"kms:ReEncrypt*\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"kms:EncryptionContext:aws-crypto-ec:aws:bedrock:arn\":\\n\"arn:aws:bedrock:${region}:${account-id}:agent-alias/*\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"Allow the caller identity data plane permissions to decrypt long term\\nmemory\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::${account-id}:role/${caller-identity-role}\"\\n},\\n\"Action\": [\\n\"kms:Decrypt\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"kms:EncryptionContext:aws-crypto-ec:aws:bedrock:arn\":\\n\"arn:aws:bedrock:${region}:${account-id}:agent-alias/*\",\\n\"kms:ViaService\": \"bedrock.$region.amazonaws.com\"\\n}\\n}\\n}\\n'}\n",
      "{'question': \"How can a developer selectively apply guardrails to specific parts of a conversation when using Amazon Bedrock's Converse API, and what are the potential benefits of this approach?\", 'ground_truth': 'Developers can selectively apply guardrails to specific parts of a conversation in Amazon Bedrock\\'s Converse API by using the guardContent field within a Message object. This field allows for the guardrail to assess only the content specified in guardContent, rather than the entire message. \\n\\nThe approach offers several benefits:\\n\\n1. Targeted assessment: Developers can focus the guardrail on the most recent or relevant message in a conversation, ignoring previous context that may not need evaluation.\\n\\n2. Efficiency: By limiting the scope of guardrail assessment, it potentially reduces processing time and resource usage.\\n\\n3. Contextual flexibility: Additional context can be provided in the message without subjecting it to guardrail assessment, allowing for more nuanced conversations.\\n\\n4. Fine-grained control: Developers can apply guardrails selectively to different parts of complex messages or multi-turn conversations.\\n\\nTo implement this, developers would structure their Message object with a guardContent field containing a GuardrailConverseContentBlock. For example:\\n\\n{\\n  \"role\": \"user\",\\n  \"content\": [\\n    {\\n      \"guardContent\": {\\n        \"text\": {\\n          \"text\": \"Create a playlist of 2 heavy metal songs.\"\\n        }\\n      }\\n    }\\n  ]\\n}\\n\\nThis approach allows for more precise and efficient use of guardrails in conversational AI applications built with Amazon Bedrock.', 'question_type': 'complex', 'context': 'in the message that you want the guardrail to assess. For information about the models that you\\ncan use with guardrails and the Converse API, see Supported models and model features.\\nTopics\\n• Configuring the guardrail\\n• Guarding a message\\n• Guarding a system prompt\\n• Message and system prompt guardrail behavior\\nConfiguring the guardrail\\nYou specify configuration information for the guardrail in the guardrailConfig input parameter.\\nThe configuration includes the ID and the version of the guardrail that you want to use. You can\\nalso enable tracing for the guardrail, which provides information about the content that the\\nguardrail blocked.\\nWith the Converse operation, guardrailConfig is a GuardrailConfiguration object, as shown in\\nthe following example.\\n{\\n\"guardrailIdentifier\": \"Guardrail ID\",\\n\"guardrailVersion\": \"Guardrail version\",\\n\"trace\": \"enabled\"\\n}\\nUse the base inference operations 415\\nAmazon Bedrock User Guide\\n If you use ConverseStream, you pass a GuardrailStreamConfiguration object. Optionally, you\\ncan use the streamProcessingMode field to specify that you want the model to complete the\\nguardrail assessment, before returning streaming response chunks. Or, you can have the model\\nasynchronously respond whilst the guardrail continues its assessment in the background. For more\\ninformation, see Configure streaming response behavior.\\nGuarding a message\\nWhen you pass a message (Message) to a model, the guardrail assesses the content in the message.\\nOptionally, you can guard selected content in the message by specifying the guardContent\\n(GuardrailConverseContentBlock) field. The guardrail evaluates only the content in the\\nguardContent field and not the rest of the message. This is useful for having the guardrail assess\\nonly the most message in a conversation, as shown in the following example.\\n[\\n{\\n\"role\": \"user\",\\n\"content\": [\\n{\\n\"text\": \"Create a playlist of 2 pop songs.\"\\n}\\n]\\n},\\n{\\n \"role\": \"assistant\",\\n\"content\": [\\n{\\n\"text\": \" Sure! Here are two pop songs:\\\\n1. \\\\\"Bad Habits\\\\\" by Ed\\nSheeran\\\\n2. \\\\\"All Of The Lights\\\\\" by Kanye West\\\\n\\\\nWould you like to add any more\\nsongs to this playlist? \"\\n}\\n]\\n},\\n{\\n\"role\": \"user\",\\n\"content\": [\\n{\\n\"guardContent\": {\\n\"text\": {\\n\"text\": \"Create a playlist of 2 heavy metal songs.\"\\n}\\n}\\nUse the base inference operations 416\\nAmazon Bedrock User Guide\\n}\\n]\\n}\\n]\\nAnother use is providing additional context for a message, without having the guardrail assess that\\nadditional context.\\n[\\n{\\n\"role\": \"user\",\\n\"content\": [\\n{\\n\"text\": \"Only answer with a list of songs.\"\\n},\\n{\\n\"guardContent\": {\\n\"text\": {\\n\"text\": \"Create a playlist of heavy metal songs.\"\\n}\\n}\\n}\\n]\\n}\\n]\\nNote\\nUsing the guardContent field is analogous to using input tags with InvokeModel and\\nInvokeModelWithResponseStream. For more information, see the section called “Input\\ntags”.\\nGuarding a system prompt\\nYou can use guardrails with system prompts that you send to the Converse API. To guard a system\\n'}\n",
      "{'question': 'What is the recommended learning rate range for Titan Text Premier model finetuning?', 'ground_truth': 'The recommended learning rate range for Titan Text Premier model finetuning is between 1.00E-07 and 1.00E-05, with a default value of 1.00E-06.', 'question_type': 'simple', 'context': '1. Validation output accuracy – Set the number of epochs to one that yields a high accuracy.\\n2. Training and validation loss – Determine the number of epochs after which the training and\\nvalidation loss becomes stable. This corresponds to when the model converges. Find the training\\nloss values in the step_wise_training_metrics.csv and validation_metrics.csv files.\\nAmazon Titan Text Premier 959\\nAmazon Bedrock User Guide\\nBatch size\\nWhen you change the batch size, we recommend that you change the learning rate using the\\nfollowing formula:\\nnewLearningRate = oldLearningRate x newBatchSize / oldBatchSize\\nTitan Text Premier model currently only supports mini-batch size of 1 for customer finetuning.\\nLearning rate\\nTo get the best results from finetuning capabilities, we recommend using a learning rate between\\n1.00E-07 and 1.00E-05. A good starting point is the recommended default value of 1.00E-06.\\nA larger learning rate may help training converge faster, however, it may adversely impact core\\n model capabilities.\\nValidate your training data with small sub-sample - To validate the quality of your training data,\\nwe recommend experimenting with a smaller dataset (~100s of samples) and monitoring the\\nvalidation metrics, before submitting the training job with larger training dataset.\\nLearning warmup steps\\nWe recommend the default value of 5.\\nTroubleshooting\\nThis section summarizes errors that you might encounter and what to check if you do.\\nPermissions issues\\nIf you encounter an issue with permissions to access an Amazon S3 bucket, check that the\\nfollowing are true:\\n1. If the Amazon S3 bucket uses a CM-KMS key for Server Side encryption, ensure that the IAM role\\npassed to Amazon Bedrock has kms:Decrypt permissions for the AWS KMS key. For example,\\nsee Allow a user to enccrypt and decrypt with any AWS KMS key in a specific AWS account.\\n2. The Amazon S3 bucket is in the same region as the Amazon Bedrock model customization job.\\n 3. The IAM role trust policy includes the service SP (bedrock.amazonaws.com).\\nTroubleshooting 960\\nAmazon Bedrock User Guide\\nThe following messages indicate issues with permissions to access training or validation data in an\\nAmazon S3 bucket:\\nCould not validate GetObject permissions to access Amazon S3 bucket: training-data-\\nbucket at key train.jsonl\\nCould not validate GetObject permissions to access Amazon S3 bucket: validation-data-\\nbucket at key validation.jsonl\\nIf you encounter one of the above errors, check that the IAM role passed to the service has\\ns3:GetObject and s3:ListBucket permissions for the training and validation dataset Amazon\\nS3 URIs.\\nThe following message indicates issues with permissions to write the output data in an Amazon S3\\nbucket:\\nAmazon S3 perms missing (PutObject): Could not validate PutObject permissions to access\\nS3 bucket: bedrock-output-bucket at key output/.write_access_check_file.tmp\\n'}\n",
      "{'question': 'How does Forward Access Sessions (FAS) differ from using a service role, and what potential security implications should be considered when choosing between these two approaches for cross-service actions in AWS?', 'ground_truth': \"Forward Access Sessions (FAS) and service roles are two different approaches for handling cross-service actions in AWS, each with distinct characteristics and security implications:\\n\\n1. FAS uses the permissions of the principal (user or role) calling an AWS service, combined with the requesting service's permissions, to make requests to downstream services. This means the original caller's permissions are propagated, which can provide more granular control but also requires the caller to have permissions for both the initial and subsequent actions.\\n\\n2. A service role, on the other hand, is an IAM role that a service assumes to perform actions on your behalf. It has its own set of permissions defined, independent of the calling principal.\\n\\nSecurity implications to consider:\\n\\n- With FAS, you need to ensure that the calling principal has appropriate permissions for all potential downstream actions, which can lead to broader permission sets and potential security risks if not managed carefully.\\n- Service roles allow for more isolation of permissions, as the role's permissions are separate from the caller's. This can enhance security by limiting the scope of actions a service can perform.\\n- FAS may provide better auditability as actions are tied directly to the original caller, while service roles might obscure the original requester in logs.\\n- Service roles can be easier to manage centrally and can be reused across multiple calls or services, potentially simplifying permission management.\\n\\nThe choice between FAS and service roles depends on the specific use case, security requirements, and the desired balance between granular control and ease of management.\", 'question_type': 'complex', 'context': \"you make a call in a service, it's common for that service to run applications in Amazon EC2 or\\nstore objects in Amazon S3. A service might do this using the calling principal's permissions,\\nusing a service role, or using a service-linked role.\\n• Forward access sessions (FAS) – When you use an IAM user or role to perform actions in\\nAWS, you are considered a principal. When you use some services, you might perform an\\naction that then initiates another action in a different service. FAS uses the permissions of the\\nprincipal calling an AWS service, combined with the requesting AWS service to make requests\\nto downstream services. FAS requests are only made when a service receives a request that\\nrequires interactions with other AWS services or resources to complete. In this case, you must\\nhave permissions to perform both actions. For policy details when making FAS requests, see\\nForward access sessions.\\n • Service role – A service role is an IAM role that a service assumes to perform actions on your\\nbehalf. An IAM administrator can create, modify, and delete a service role from within IAM. For\\nmore information, see Creating a role to delegate permissions to an AWS service in the IAM\\nUser Guide.\\n• Service-linked role – A service-linked role is a type of service role that is linked to an AWS\\nservice. The service can assume the role to perform an action on your behalf. Service-linked\\nroles appear in your AWS account and are owned by the service. An IAM administrator can\\nview, but not edit the permissions for service-linked roles.\\n• Applications running on Amazon EC2 – You can use an IAM role to manage temporary\\ncredentials for applications that are running on an EC2 instance and making AWS CLI or AWS API\\nrequests. This is preferable to storing access keys within the EC2 instance. To assign an AWS role\\n to an EC2 instance and make it available to all of its applications, you create an instance profile\\nthat is attached to the instance. An instance profile contains the role and enables programs that\\nare running on the EC2 instance to get temporary credentials. For more information, see Using\\nan IAM role to grant permissions to applications running on Amazon EC2 instances in the IAM\\nUser Guide.\\nAuthenticating with identities 1065\\nAmazon Bedrock User Guide\\nTo learn whether to use IAM roles or IAM users, see When to create an IAM role (instead of a user)\\nin the IAM User Guide.\\nManaging access using policies\\nYou control access in AWS by creating policies and attaching them to AWS identities or resources.\\nA policy is an object in AWS that, when associated with an identity or resource, defines their\\npermissions. AWS evaluates these policies when a principal (user, root user, or role session) makes\\na request. Permissions in the policies determine whether the request is allowed or denied. Most\\n\"}\n",
      "{'question': 'Which AWS regions support all Amazon Bedrock models except Claude 3 Opus, Titan Text Premier, and Mistral Small?', 'ground_truth': 'US East (N. Virginia, us-east-1) and US West (Oregon, us-west-2) regions support all Amazon Bedrock models except Claude 3 Opus, Titan Text Premier, and Mistral Small.', 'question_type': 'simple', 'context': \"compability.\\n• To return information about a specific foundation model, send a GetFoundationModel request,\\nspecifying the model ID.\\nSelect a tab to see code examples in an interface or language.\\nGet model information 38\\nAmazon Bedrock User Guide\\nAWS CLI\\nList the Amazon Bedrock foundation models.\\naws bedrock list-foundation-models\\nGet information about Anthropic Claude v2.\\naws bedrock get-foundation-model --model-identifier anthropic.claude-v2\\nPython\\nList the Amazon Bedrock foundation models.\\nimport boto3\\nbedrock = boto3.client(service_name='bedrock')\\nbedrock.list_foundation_models()\\nGet information about Anthropic Claude v2.\\nimport boto3\\nbedrock = boto3.client(service_name='bedrock')\\nbedrock.get_foundation_model(modelIdentifier='anthropic.claude-v2')\\nModel support by AWS Region\\nNote\\nAll models, except Anthropic Claude 3 Opus, Amazon Titan Text Premier, and Mistral Small\\nare supported in both the US East (N. Virginia, us-east-1) and the US West (Oregon, us-\\nwest-2) Regions.\\n Amazon Titan Text Premier, Mistral Small, and AI21 Jamba-Instruct models are only\\navailable in the US East (N. Virginia, us-east-1) Region.\\nAnthropic Claude 3 Opus, Meta Llama 3.1 Instruct, and Mistral Large 2 (24.07) models are\\nonly available in the US West (Oregon, us-west-2) Region.\\nModel support by AWS Region 39\\nAmazon Bedrock User Guide\\nNote\\nAccess to models in Europe (Ireland) and Asia Pacific (Singapore) Regions are currently\\ngated. Please contact your account manager to request model access in these Regions.\\nThe following table shows the FMs that are available in other Regions and whether they're\\nsupported in each Region.\\nModel Asia Asia Asia Asia Canada Europe Europe Europe Europe South AWS\\nPacific Pacific Pacific Pacific (Central()Frankfu(rIr eland)( London()Paris) AmericaG ovCloud\\n(Mumba(iS)ingapo(Sr ydney()Tokyo) t) NOTE: (São (US-\\ne) Gated Paulo) West)\\nNOTE: access\\nGated only\\naccess\\nonly\\nAmazon Yes No Yes Yes Yes Yes Gated Yes Yes Yes Yes\\nTitan\\nText\\nG1 -\\nExpress\\n Amazon Yes No Yes No Yes Yes Gated Yes Yes Yes No\\nTitan\\nText\\nG1 -\\nLite\\nAmazon No No No No No No No No No No No\\nTitan\\nText\\nPremier\\nAmazon No No No Yes No Yes No No No No No\\nTitan\\nEmbedding\\nModel support by AWS Region 40\\nAmazon Bedrock User Guide\\nModel Asia Asia Asia Asia Canada Europe Europe Europe Europe South AWS\\nPacific Pacific Pacific Pacific (Central()Frankfu(rIr eland)( London()Paris) AmericaG ovCloud\\n(Mumba(iS)ingapo(Sr ydney()Tokyo) t) NOTE: (São (US-\\ne) Gated Paulo) West)\\nNOTE: access\\nGated only\\naccess\\nonly\\ns\\nG1 -\\nText\\nAmazon No No No No Yes Yes No Yes No Yes Yes\\nTitan\\nText\\nEmbedding\\ns V2\\nAmazon Yes No Yes No Yes Yes Gated Yes Yes Yes No\\nTitan\\nMultimoda\\nl\\nEmbedding\\ns G1\\nAmazon Yes No No No No No Gated Yes No No No\\nTitan\\nImage\\nGenerator\\nG1\\nV1\\nModel support by AWS Region 41\\nAmazon Bedrock User Guide\\nModel Asia Asia Asia Asia Canada Europe Europe Europe Europe South AWS\\nPacific Pacific Pacific Pacific (Central()Frankfu(rIr eland)( London()Paris) AmericaG ovCloud\\n\"}\n",
      "{'question': \"How does the ModelInvocationInput object in Amazon Bedrock's trace events contribute to understanding and potentially modifying an agent's behavior across different processing steps?\", 'ground_truth': \"The ModelInvocationInput object in Amazon Bedrock's trace events provides crucial insights into an agent's behavior and allows for potential modifications across different processing steps. It appears in various trace types (PreProcessingTrace, OrchestrationTrace, PostProcessingTrace, and GuardrailTrace) and contains several key elements:\\n\\n1. Prompt information: The 'text' field shows the exact prompt given to the agent at each step, allowing developers to understand what input the agent is working with.\\n\\n2. Processing stage identification: The 'type' field indicates which step of the agent's process is being traced (e.g., PRE_PROCESSING, ORCHESTRATION), enabling a clear view of the agent's workflow.\\n\\n3. Inference configuration: The 'inferenceConfiguration' object includes parameters like maximumLength, stopSequences, temperature, topK, and topP, which influence response generation. By analyzing and adjusting these, developers can fine-tune the agent's output.\\n\\n4. Customization flags: 'promptCreationMode' and 'parserMode' indicate whether default templates or parsers have been overridden, while 'overrideLambda' provides the ARN of any custom parser function used.\\n\\nThis comprehensive information allows developers to:\\n- Trace the agent's decision-making process across different stages\\n- Identify areas for optimization or customization\\n- Understand how prompt engineering and inference parameters affect the agent's performance\\n- Implement and track the impact of advanced prompts or custom parsers\\n\\nBy leveraging this data, developers can iteratively refine the agent's behavior, improving its effectiveness and tailoring it to specific use cases.\", 'question_type': 'complex', 'context': '• PreProcessingTrace – Traces the input and output of the pre-processing step, in which the agent\\ncontextualizes and categorizes user input and determines if it is valid.\\n• OrchestrationTrace – Traces the input and output of the orchestration step, in which the agent\\ninterprets the input, invokes action groups, and queries knowledge bases. Then the agent returns\\noutput to either continue orchestration or to respond to the user.\\nTrace events 711\\nAmazon Bedrock User Guide\\n• PostProcessingTrace – Traces the input and output of the post-processing step, in which the\\nagent handles the final output of the orchestration and determines how to return the response\\nto the user.\\n• FailureTrace – Traces the reason that a step failed.\\n• GuardrailTrace – Traces the actions of the Guardrail.\\nEach of the traces (except FailureTrace) contains a ModelInvocationInput object. The\\nModelInvocationInput object contains configurations set in the prompt template for the step,\\n alongside the prompt provided to the agent at this step. For more information about how to\\nmodify prompt templates, see Advanced prompts in Amazon Bedrock. The structure of the\\nModelInvocationInput object is as follows:\\n{\\n\"traceId\": \"string\",\\n\"text\": \"string\",\\n\"type\": \"PRE_PROCESSING | ORCHESTRATION | KNOWLEDGE_BASE_RESPONSE_GENERATION |\\nPOST_PROCESSING\",\\n\"inferenceConfiguration\": {\\n\"maximumLength\": number,\\n\"stopSequences\": [\"string\"],\\n\"temperature\": float,\\n\"topK\": float,\\n\"topP\": float\\n},\\n\"promptCreationMode\": \"DEFAULT | OVERRIDDEN\",\\n\"parserMode\": \"DEFAULT | OVERRIDDEN\",\\n\"overrideLambda\": \"string\"\\n}\\nThe following list describes the fields of the ModelInvocationInput object:\\n• traceId – The unique identifier of the trace.\\n• text – The text from the prompt provided to the agent at this step.\\n• type – The current step in the agent\\'s process.\\n• inferenceConfiguration – Inference parameters that influence response generation. For\\nmore information, see Inference parameters.\\n • promptCreationMode – Whether the agent\\'s default base prompt template was overridden for\\nthis step. For more information, see Advanced prompts in Amazon Bedrock.\\nTrace events 712\\nAmazon Bedrock User Guide\\n• parserMode – Whether the agent\\'s default response parser was overridden for this step. For\\nmore information, see Advanced prompts in Amazon Bedrock.\\n• overrideLambda – The Amazon Resource Name (ARN) of the parser Lambda function used to\\nparse the response, if the default parser was overridden. For more information, see Advanced\\nprompts in Amazon Bedrock.\\nFor more information about each trace type, see the following sections:\\nPreProcessingTrace\\n{\\n\"modelInvocationInput\": { // see above for details }\\n\"modelInvocationOutput\": {\\n\"parsedResponse\": {\\n\"isValid\": boolean,\\n\"rationale\": \"string\"\\n},\\n\"traceId\": \"string\"\\n}\\n}\\nThe PreProcessingTrace consists of a ModelInvocationInput object and a\\nPreProcessingModelInvocationOutput object. The PreProcessingModelInvocationOutput contains\\n'}\n",
      "{'question': 'How many digits are in a SWIFT code?', 'ground_truth': \"SWIFT codes consist of either 8 or 11 characters. The 11-digit codes refer to specific branches, while 8-digit codes (or 11-digit codes ending in 'XXX') refer to the head or primary office.\", 'question_type': 'simple', 'context': 'credit and debit cards. For American Express credit or debit cards, the CVV is a four-digit\\nnumeric code.\\n• CREDIT_DEBIT_CARD_EXPIRY\\nThe expiration date for a credit or debit card. This number is usually four digits long and\\nis often formatted as month/year or MM/YY. Guardrails for Amazon Bedrock recognizes\\nexpiration dates such as 01/21, 01/2021, and Jan 2021.\\n• CREDIT_DEBIT_CARD_NUMBER\\nThe number for a credit or debit card. These numbers can vary from 13 to 16 digits in length.\\nHowever, Amazon Comprehend also recognizes credit or debit card numbers when only the\\nlast four digits are present.\\n• PIN\\nA four-digit personal identification number (PIN) with which you can access your bank account.\\n• INTERNATIONAL_BANK_ACCOUNT_NUMBER\\nAn International Bank Account Number has specific formats in each country. For more\\ninformation, see www.iban.com/structure.\\n• SWIFT_CODE\\nA SWIFT code is a standard format of Bank Identifier Code (BIC) used to specify a particular\\n bank or branch. Banks use these codes for money transfers such as international wire transfers.\\nSWIFT codes consist of eight or 11 characters. The 11-digit codes refer to specific branches,\\nwhile eight-digit codes (or 11-digit codes ending in \\'XXX\\') refer to the head or primary office.\\n• IT\\nSensitive information filters 369\\nAmazon Bedrock User Guide\\n• IP_ADDRESS\\nAn IPv4 address, such as 198.51.100.0.\\n• MAC_ADDRESS\\nA media access control (MAC) address is a unique identifier assigned to a network interface\\ncontroller (NIC).\\n• URL\\nA web address, such as www.example.com.\\n• AWS_ACCESS_KEY\\nA unique identifier that\\'s associated with a secret access key; you use the access key ID and\\nsecret access key to sign programmatic AWS requests cryptographically.\\n• AWS_SECRET_KEY\\nA unique identifier that\\'s associated with an access key. You use the access key ID and secret\\naccess key to sign programmatic AWS requests cryptographically.\\n• USA specific\\n• US_BANK_ACCOUNT_NUMBER\\n A US bank account number, which is typically 10 to 12 digits long.\\n• US_BANK_ROUTING_NUMBER\\nA US bank account routing number. These are typically nine digits long,\\n• US_INDIVIDUAL_TAX_IDENTIFICATION_NUMBER\\nA US Individual Taxpayer Identification Number (ITIN) is a nine-digit number that starts with a\\n\"9\" and contain a \"7\" or \"8\" as the fourth digit. An ITIN can be formatted with a space or a dash\\nafter the third and forth digits.\\n• US_PASSPORT_NUMBER\\nA US passport number. Passport numbers range from six to nine alphanumeric characters.\\n• US_SOCIAL_SECURITY_NUMBER\\nA US Social Security Number (SSN) is a nine-digit number that is issued to US citizens,\\npermanent residents, and temporary working residents.\\nSensitive information filters 370\\nAmazon Bedrock User Guide\\n• Canada specific\\n• CA_HEALTH_NUMBER\\nA Canadian Health Service Number is a 10-digit unique identifier, required for individuals to\\naccess healthcare benefits.\\n• CA_SOCIAL_INSURANCE_NUMBER\\n'}\n",
      "{'question': 'In the described prompt flow, how does the system determine which action to take for an item with a retail price of $15, a market price of $13, and a type of \"produce\", and why is this specific action chosen?', 'ground_truth': 'The system would choose the action \"buy\" for this item. Here\\'s why:\\n\\n1. The flow has two conditions and a default output:\\n   - Condition 1: (retailPrice > 10) and (type == \"produce\")\\n   - Condition 2: (retailPrice > marketPrice)\\n   - Default (if no conditions are met)\\n\\n2. For the given item:\\n   - retailPrice ($15) is greater than 10\\n   - type is \"produce\"\\n   - retailPrice ($15) is greater than marketPrice ($13)\\n\\n3. The item satisfies both conditions. However, the flow is configured to check conditions in order and use the first matching condition.\\n\\n4. Condition 1 is met first, so the system will use the first flow output node.\\n\\n5. The first flow output node is configured to return $.data.action[0], which is the first value in the action array.\\n\\n6. Based on the example provided in the context, the action array is typically structured as [\"don\\'t buy\", \"buy\", \"undecided\"].\\n\\n7. Therefore, $.data.action[1] would return \"buy\", which is the second element in the array.\\n\\nThis demonstrates how the flow uses conditional logic to determine actions based on item attributes, and how the order of conditions and the structure of the output array affect the final decision.', 'question_type': 'complex', 'context': 'retailPrice Number $.data.re\\ntailPrice\\nExample prompt flows 857\\nAmazon Bedrock User Guide\\nName Type Expression\\nmarketPrice Number $.data.ma\\nrketPrice\\ntype String $.data.type\\nThis configuration means that the condition node expects a JSON object that contains the\\nfields retailPrice, marketPrice, and type.\\ne. Configure the conditions by doing the following:\\ni. In the Conditions section, optionally change the name of the condition. Then add the\\nfollowing condition in the Condition text box: (retailPrice > 10) and (type\\n== \"produce\").\\nii. Add a second condition by choosing Add condition. Optionally change the name of\\nthe second condition. Then add the following condition in the Condition text box:\\n(retailPrice > marketPrice).\\n3. Choose the Flow input node and select the Configure tab. Select Object as the Type. This\\nmeans that flow invocation will expect to receive a JSON object.\\n4. Add flow output nodes so that you have three in total. Configure them as follows in the\\n Configure tab of the Prompt flow builder pane of each flow output node:\\na. Set the input type of the first flow output node as String and the expression as\\n$.data.action[0] to return the first value in the array in the action field of the\\nincoming object.\\nb. Set the input type of the second flow output node as Number and the expression as\\n$.data.action[1] to return the second value in the array in the action field of the\\nincoming object.\\nc. Set the input type of the third flow output node as Number and the expression as\\n$.data.action[2] to return the third value in the array in the action field of the\\nincoming object.\\n5. Connect the first condition to the first flow output node, the second condition to the second\\nflow output node, and the default condition to the third flow output node.\\n6. Connect the inputs and outputs in all the nodes to complete the flow by doing the following:\\nExample prompt flows 858\\nAmazon Bedrock User Guide\\n a. Drag a connection from the output node of the Flow input node to the genre input in the\\nMakePlaylist prompt node.\\nb. Drag a connection from the output node of the Flow input node to the number input in\\nthe MakePlaylist prompt node.\\nc. Drag a connection from the output node of the modelCompletion output in the\\nMakePlaylist prompt node to the document input in the Flow output node.\\nd. Drag a connection from the output of the Flow input node to the document input in each\\nof the three output nodes.\\n7. Choose Save to save your flow. Your flow should now be prepared for testing.\\n8. Test your flow by entering the following JSON objects is the Test prompt flow pane on the\\nright. Choose Run for each input:\\n1. The following object fulfills the first condition (the retailPrice is more than 10 and the\\ntype is \"produce\") and returns the first value in action (\"don\\'t buy\"):\\n{\\n\"retailPrice\": 11,\\n\"marketPrice\": 12,\\n\"type\": \"produce\",\\n\"action\": [\"don\\'t buy\", \"buy\", \"undecided\"]\\n}\\nNote\\n'}\n",
      "{'question': 'What are the minimum components needed to prepare an agent for testing or deployment?', 'ground_truth': \"To prepare an agent for testing or deployment, you must minimally configure the agent resource role (ARN of the service role with permissions), foundation model (FM) for the agent to invoke, and instructions (natural language describing the agent's purpose and interactions). Additionally, at least one action group or knowledge base should be configured.\", 'question_type': 'simple', 'context': \"Management Console to automatically create a service role for you.\\n3. (Optional) Create a guardrail to implement safeguards for your agent and to prevent\\nunwanted behavior from model responses and user messages. You can then associate it with\\nyour agent.\\nPrerequisites 651\\nAmazon Bedrock User Guide\\n4. (Optional) Purchase Provisioned Throughput to increase the number and rate of tokens that\\nyour agent can process in a given time frame. You can then associate it with an alias of your\\nagent when you create a version of your agent and associate an alias with it.\\nCreate an agent in Amazon Bedrock\\nTo create an agent with Amazon Bedrock, you set up the following components:\\n• The configuration of the agent, which defines the purpose of the agent and indicates the\\nfoundation model (FM) that it uses to generate prompts and responses.\\n• At least one of the following:\\n• Action groups that define what actions the agent is designed to perform.\\n • A knowledge base of data sources to augment the generative capabilities of the agent by\\nallowing search and query.\\nYou can minimally create an agent that only has a name. To Prepare an agent so that you can test\\nor deploy it, you must minimally configure the following components:\\nConfiguration Description\\nAgent resource role The ARN of the service role with permissions\\nto call API operations on the agent\\nFoundation model (FM) An FM for the agent to invoke to perform\\norchestration\\nInstructions Natural language describing what the agent\\nshould do and how it should interact with\\nusers\\nYou should also configure at least one action group or knowledge base for the agent. If you\\nprepare an agent with no action groups or knowledge bases, it will return responses based only on\\nthe FM and instructions and base prompt templates.\\nTo learn how to create an agent, select the tab corresponding to your method of choice and follow\\nthe steps.\\nCreate an agent 652\\nAmazon Bedrock User Guide\\nConsole\\n To create an agent\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\\nbedrock/.\\n2. Select Agents from the left navigation pane.\\n3. In the Agents section, choose Create Agent.\\n4. (Optional) Change the automatically generated Name for the agent and provide an\\noptional Description for it.\\n5. Choose Create. Your agent is created and you will be taken to the Agent builder for your\\nnewly created agent, where you can configure your agent.\\n6. You can continue to the following procedure to configure your agent or return to the Agent\\nbuilder later.\\nTo configure your agent\\n1. If you're not already in the agent builder, do the following:\\na. Sign in to the AWS Management Console using an IAM role with Amazon\\nBedrock permissions, and open the Amazon Bedrock console at https://\\nconsole.aws.amazon.com/bedrock/.\\n\"}\n",
      "{'question': 'How does the lambda_handler function determine which parsing method to use, and what potential issue might arise if a new prompt type is introduced without updating the function?', 'ground_truth': 'The lambda_handler function determines which parsing method to use based on the \"promptType\" field in the input event. It checks if the promptType is equal to PREPROCESSING_PROMPT_TYPE, and if so, it calls the parse_pre_processing function. However, this approach has a potential issue: if a new prompt type is introduced, the lambda_handler function would need to be updated to handle it. Currently, it only explicitly handles the PREPROCESSING_PROMPT_TYPE, and there\\'s no else clause or default case to handle other prompt types. This means that if a new prompt type is added without updating the lambda_handler, it would not be processed correctly, potentially leading to errors or unexpected behavior in the system.', 'question_type': 'complex', 'context': 'PREPROCESSING_PROMPT_TYPE = \"PRE_PROCESSING\"\\nPRE_PROCESSING_RATIONALE_PATTERN = re.compile(PRE_PROCESSING_RATIONALE_REGEX,\\nre.DOTALL)\\nPREPROCESSING_CATEGORY_PATTERN = re.compile(PREPROCESSING_CATEGORY_REGEX, re.DOTALL)\\nlogger = logging.getLogger()\\n# This parser lambda is an example of how to parse the LLM output for the default\\nPreProcessing prompt\\ndef lambda_handler(event, context):\\nprint(\"Lambda input: \" + str(event))\\nlogger.info(\"Lambda input: \" + str(event))\\nprompt_type = event[\"promptType\"]\\n# Sanitize LLM response\\nmodel_response = sanitize_response(event[\\'invokeModelRawResponse\\'])\\nif event[\"promptType\"] == PREPROCESSING_PROMPT_TYPE:\\nreturn parse_pre_processing(model_response)\\ndef parse_pre_processing(model_response):\\ncategory_matches = re.finditer(PREPROCESSING_CATEGORY_PATTERN, model_response)\\nrationale_matches = re.finditer(PRE_PROCESSING_RATIONALE_PATTERN, model_response)\\ncategory = next((match.group(1) for match in category_matches), None)\\n rationale = next((match.group(1) for match in rationale_matches), None)\\nreturn {\\nAdvanced prompts 765\\nAmazon Bedrock User Guide\\n\"promptType\": \"PRE_PROCESSING\",\\n\"preProcessingParsedResponse\": {\\n\"rationale\": rationale,\\n\"isValidInput\": get_is_valid_input(category)\\n}\\n}\\ndef sanitize_response(text):\\npattern = r\"(\\\\\\\\n*)\"\\ntext = re.sub(pattern, r\"\\\\n\", text)\\nreturn text\\ndef get_is_valid_input(category):\\nif category is not None and category.strip().upper() == \"D\" or\\ncategory.strip().upper() == \"E\":\\nreturn True\\nreturn False\\nOrchestration\\nThe following examples shows an orchestration parser Lambda function written in Python.\\nThe example code differs depending on whether your action group was defined with an OpenAPI\\nschema or with function details:\\n1. To see examples for an action group defined with an OpenAPI schema, select the tab\\ncorresponding to the model that you want to see examples for.\\nAnthropic Claude 2.0\\nimport json\\nimport re\\nimport logging\\nRATIONALE_REGEX_LIST = [\\n \"(.*?)(<function_call>)\",\\n\"(.*?)(<answer>)\"\\n]\\nRATIONALE_PATTERNS = [re.compile(regex, re.DOTALL) for regex in\\nRATIONALE_REGEX_LIST]\\nRATIONALE_VALUE_REGEX_LIST = [\\n\"<scratchpad>(.*?)(</scratchpad>)\",\\nAdvanced prompts 766\\nAmazon Bedrock User Guide\\n\"(.*?)(</scratchpad>)\",\\n\"(<scratchpad>)(.*?)\"\\n]\\nRATIONALE_VALUE_PATTERNS = [re.compile(regex, re.DOTALL) for regex in\\nRATIONALE_VALUE_REGEX_LIST]\\nANSWER_REGEX = r\"(?<=<answer>)(.*)\"\\nANSWER_PATTERN = re.compile(ANSWER_REGEX, re.DOTALL)\\nANSWER_TAG = \"<answer>\"\\nFUNCTION_CALL_TAG = \"<function_call>\"\\nASK_USER_FUNCTION_CALL_REGEX = r\"(<function_call>user::askuser)(.*)\\\\)\"\\nASK_USER_FUNCTION_CALL_PATTERN = re.compile(ASK_USER_FUNCTION_CALL_REGEX,\\nre.DOTALL)\\nASK_USER_FUNCTION_PARAMETER_REGEX = r\"(?<=askuser=\\\\\")(.*?)\\\\\"\"\\nASK_USER_FUNCTION_PARAMETER_PATTERN =\\nre.compile(ASK_USER_FUNCTION_PARAMETER_REGEX, re.DOTALL)\\nKNOWLEDGE_STORE_SEARCH_ACTION_PREFIX = \"x_amz_knowledgebase_\"\\nFUNCTION_CALL_REGEX = r\"<function_call>(\\\\w+)::(\\\\w+)::(.+)\\\\((.+)\\\\)\"\\n'}\n",
      "{'question': 'What is the purpose of the AmazonBedrockStudioPermissionsBoundary policy?', 'ground_truth': 'The AmazonBedrockStudioPermissionsBoundary policy limits permissions of IAM principals created by Amazon Bedrock Studio when creating projects, apps, and components. It grants read and write access to services like Amazon S3, Amazon Bedrock, Amazon OpenSearch Serverless, and AWS Lambda, as well as necessary infrastructure resources.', 'question_type': 'simple', 'context': '\"Effect\": \"Allow\",\\n\"Action\": [\\n\"bedrock:GetFoundationModel\",\\n\"bedrock:ListFoundationModels\",\\n\"bedrock:GetModelInvocationLoggingConfiguration\",\\n\"bedrock:GetProvisionedModelThroughput\",\\n\"bedrock:ListProvisionedModelThroughputs\",\\n\"bedrock:GetModelCustomizationJob\",\\n\"bedrock:ListModelCustomizationJobs\",\\n\"bedrock:ListCustomModels\",\\n\"bedrock:GetCustomModel\",\\n\"bedrock:GetModelInvocationJob\",\\n\"bedrock:ListModelInvocationJobs\",\\n\"bedrock:ListTagsForResource\",\\n\"bedrock:GetFoundationModelAvailability\",\\n\"bedrock:GetGuardrail\",\\n\"bedrock:ListGuardrails\",\\n\"bedrock:GetEvaluationJob\",\\n\"bedrock:ListEvaluationJobs\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}\\nAWS managed policies 1095\\nAmazon Bedrock User Guide\\nAWS managed policy: AmazonBedrockStudioPermissionsBoundary\\nNote\\n• This policy is a permissions boundary. A permissions boundary sets the maximum\\npermissions that an identity-based policy can grant to an IAM principal. You should\\nnot use and attach Amazon Bedrock Studio permissions boundary policies on your\\n own. Amazon Bedrock Studio permissions boundary policies should only be attached\\nto Amazon Bedrock Studio managed roles. For more information on permissions\\nboundaries, see Permissions boundaries for IAM entities in the IAM User Guide.\\n• The current version of Amazon Bedrock Studio continues to expect a similar policy\\nnamed AmazonDataZoneBedrockPermissionsBoundary to exist in your AWS\\naccount. For more information, see Step 2: Create permissions boundary, service role, and\\nprovisioning role.\\nWhen you create Amazon Bedrock Studio projects, apps, and components, Amazon Bedrock Studio\\napplies this permissions boundary to the IAM roles produced when creating those resources.\\nAmazon Bedrock Studio uses the AmazonBedrockStudioPermissionsBoundary managed\\npolicy to limit permissions of the provisioned IAM principal it is attached to. Principals might\\ntake the form of the user roles that Amazon DataZone can assume on behalf of Amazon Bedrock\\n Studio users, and then conduct actions such as reading and writing Amazon S3 objects or invoking\\nAmazon Bedrock agents.\\nThe AmazonBedrockStudioPermissionsBoundary policy grants read and write access for\\nAmazon Bedrock Studio to services such as Amazon S3, Amazon Bedrock, Amazon OpenSearch\\nServerless, and AWS Lambda. The policy also gives read and write permissions to some\\ninfrastructure resources that are required to use these services such as AWS Secrets Manager\\nsecrets, Amazon CloudWatch log groups, and AWS KMS keys.\\nThis policy consists of the following sets of permissions.\\n• s3 – Allows read and write access to objects in Amazon S3 buckets that are managed by Amazon\\nBedrock Studio.\\n• bedrock – Grants the ability to use Amazon Bedrock agents, knowledge bases, and guardrails\\nthat are managed by Amazon Bedrock Studio.\\nAWS managed policies 1096\\nAmazon Bedrock User Guide\\n• aoss – Allows API access to Amazon OpenSearch Serverless collections that are managed by\\nAmazon Bedrock Studio.\\n'}\n",
      "{'question': 'How does the ConverseStream API handle real-time response processing for Meta Llama models, and what parameters can be adjusted to control the output?', 'ground_truth': \"The ConverseStream API handles real-time response processing for Meta Llama models by returning a stream of response chunks. Developers can iterate through this stream to process the response as it's generated. The API allows adjustment of several parameters to control the output:\\n\\n1. maxTokens: Limits the maximum number of tokens in the response (e.g., 512).\\n2. temperature: Controls the randomness of the output (e.g., 0.5 for balanced creativity).\\n3. topP: Influences the diversity of word choices (e.g., 0.9 for a good balance).\\n\\nThese parameters are set in the inferenceConfig object when creating the ConverseStream command. The code examples demonstrate how to set up the client, create the command with these parameters, and then process the streamed response in real-time using a loop that writes each chunk of text to the output as it's received.\", 'question_type': 'complex', 'context': ').build();\\ntry {\\n// Send the message with a basic inference configuration and attach\\nthe handler.\\nclient.converseStream(request -> request\\n.modelId(modelId)\\n.messages(message)\\n.inferenceConfig(config -> config\\n.maxTokens(512)\\n.temperature(0.5F)\\n.topP(0.9F)\\n), responseStreamHandler).get();\\nMeta Llama 1433\\nAmazon Bedrock User Guide\\n} catch (ExecutionException | InterruptedException e) {\\nSystem.err.printf(\"Can\\'t invoke \\'%s\\': %s\", modelId,\\ne.getCause().getMessage());\\n}\\n}\\n}\\n• For API details, see ConverseStream in AWS SDK for Java 2.x API Reference.\\nJavaScript\\nSDK for JavaScript (v3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nSend a text message to Meta Llama, using Bedrock\\'s Converse API and process the response\\nstream in real-time.\\n// Use the Conversation API to send a text message to Meta Llama.\\nimport {\\nBedrockRuntimeClient,\\nConverseStreamCommand,\\n} from \"@aws-sdk/client-bedrock-runtime\";\\n // Create a Bedrock Runtime client in the AWS Region you want to use.\\nconst client = new BedrockRuntimeClient({ region: \"us-east-1\" });\\n// Set the model ID, e.g., Llama 3 8b Instruct.\\nconst modelId = \"meta.llama3-8b-instruct-v1:0\";\\n// Start a conversation with the user message.\\nconst userMessage =\\n\"Describe the purpose of a \\'hello world\\' program in one line.\";\\nconst conversation = [\\nMeta Llama 1434\\nAmazon Bedrock User Guide\\n{\\nrole: \"user\",\\ncontent: [{ text: userMessage }],\\n},\\n];\\n// Create a command with the model ID, the message, and a basic configuration.\\nconst command = new ConverseStreamCommand({\\nmodelId,\\nmessages: conversation,\\ninferenceConfig: { maxTokens: 512, temperature: 0.5, topP: 0.9 },\\n});\\ntry {\\n// Send the command to the model and wait for the response\\nconst response = await client.send(command);\\n// Extract and print the streamed response text in real-time.\\nfor await (const item of response.stream) {\\nif (item.contentBlockDelta) {\\n process.stdout.write(item.contentBlockDelta.delta?.text);\\n}\\n}\\n} catch (err) {\\nconsole.log(`ERROR: Can\\'t invoke \\'${modelId}\\'. Reason: ${err}`);\\nprocess.exit(1);\\n}\\n• For API details, see ConverseStream in AWS SDK for JavaScript API Reference.\\nPython\\nSDK for Python (Boto3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nMeta Llama 1435\\nAmazon Bedrock User Guide\\nSend a text message to Meta Llama, using Bedrock\\'s Converse API and process the response\\nstream in real-time.\\n# Use the Conversation API to send a text message to Meta Llama\\n# and print the response stream.\\nimport boto3\\nfrom botocore.exceptions import ClientError\\n# Create a Bedrock Runtime client in the AWS Region you want to use.\\nclient = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\\n# Set the model ID, e.g., Llama 3 8b Instruct.\\nmodel_id = \"meta.llama3-8b-instruct-v1:0\"\\n# Start a conversation with the user message.\\n'}\n",
      "{'question': 'What parameter is required when making inference requests to Cohere Command models?', 'ground_truth': 'The \"message\" parameter, which is the text input for the model to respond to, is required when making inference requests to Cohere Command models.', 'question_type': 'simple', 'context': 'for i, embedding in enumerate(response_body.get(\\'embeddings\\')):\\nprint(f\"\\\\tEmbedding {i}\")\\nprint(*embedding)\\nprint(\"Texts\")\\nfor i, text in enumerate(response_body.get(\\'texts\\')):\\nprint(f\"\\\\tText {i}: {text}\")\\nexcept ClientError as err:\\nmessage = err.response[\"Error\"][\"Message\"]\\nlogger.error(\"A client error occurred: %s\", message)\\nprint(\"A client error occured: \" +\\nformat(message))\\nelse:\\nprint(\\nf\"Finished generating text embeddings with Cohere model {model_id}.\")\\nif __name__ == \"__main__\":\\nmain()\\nCohere Command R and Command R+ models\\nYou make inference requests to Cohere Command R and Cohere Command R+ models with\\nInvokeModel or InvokeModelWithResponseStream (streaming). You need the model ID for the\\nmodel that you want to use. To get the model ID, see Amazon Bedrock model IDs.\\nTip\\nFor conversational applications, we recommend that you use the Converse API. The\\nConverse API provides a unified set of parameters that work across all models that support\\n messages. For more information, see Use the Converse API.\\nTopics\\nCohere models 166\\nAmazon Bedrock User Guide\\n• Request and Response\\n• Code example\\nRequest and Response\\nRequest\\nThe Cohere Command models have the following inference parameters.\\n{\\n\"message\": string,\\n\"chat_history\": [\\n{\\n\"role\":\"USER or CHATBOT\",\\n\"message\": string\\n}\\n],\\n\"documents\": [\\n{\"title\": string, \"snippet\": string},\\n],\\n\"search_queries_only\" : boolean,\\n\"preamble\" : string,\\n\"max_tokens\": int,\\n\"temperature\": float,\\n\"p\": float,\\n\"k\": float,\\n\"prompt_truncation\" : string,\\n\"frequency_penalty\" : float,\\n\"presence_penalty\" : float,\\n\"seed\" : int,\\n\"return_prompt\" : boolean,\\n\"tools\" : [\\n{\\n\"name\": string,\\n\"description\": string,\\n\"parameter_definitions\": {\\n\"parameter name\": {\\n\"description\": string,\\n\"type\": string,\\n\"required\": boolean\\n}\\nCohere models 167\\nAmazon Bedrock User Guide\\n}\\n}\\n],\\n\"tool_results\" : [\\n{\\n\"call\": {\\n\"name\": string,\\n\"parameters\": {\\n\"parameter name\": string\\n}\\n},\\n\"outputs\": [\\n{\\n\"text\": string\\n}\\n]\\n}\\n],\\n \"stop_sequences\": [string],\\n\"raw_prompting\" : boolean\\n}\\nThe following are required parameters.\\n• message – (Required) Text input for the model to respond to.\\nThe following are optional parameters.\\n• chat_history – A list of previous messages between the user and the model, meant to give\\nthe model conversational context for responding to the user\\'s message.\\nThe following are required fields.\\n• role – The role for the message. Valid values are USER or CHATBOT. tokens.\\n• message – Text contents of the message.\\nThe following is example JSON for the chat_history field\\n\"chat_history\": [\\n{\"role\": \"USER\", \"message\": \"Who discovered gravity?\"},\\nCohere models 168\\nAmazon Bedrock User Guide\\n{\"role\": \"CHATBOT\", \"message\": \"The man who is widely credited with discovering\\ngravity is Sir Isaac Newton\"}\\n]\\n• documents – A list of texts that the model can cite to generate a more accurate reply. Each\\ndocument is a string-string dictionary. The resulting generation includes citations that\\n'}\n",
      "{'question': 'How do the InvokeModel and Converse operations differ in their functionality and usage when interacting with Amazon Bedrock foundation models, and why might one be preferred over the other?', 'ground_truth': \"The InvokeModel and Converse operations in Amazon Bedrock differ in several key aspects:\\n\\n1. Functionality:\\n   - InvokeModel is a more basic operation that allows you to submit a prompt and generate a model response.\\n   - Converse is a more advanced operation that unifies the inference request across Amazon Bedrock models and simplifies the management of multi-turn conversations.\\n\\n2. Usage:\\n   - InvokeModel requires specifying more parameters in the request, such as textGenerationConfig with maxTokenCount, temperature, and topP.\\n   - Converse simplifies the request structure, potentially making it easier to use for developers.\\n\\n3. Output handling:\\n   - InvokeModel writes the response to a specified output file (e.g., invoke-model-output-text.txt).\\n   - Converse's output handling is not explicitly mentioned in the context, but it likely provides a more streamlined response format.\\n\\n4. Conversation management:\\n   - InvokeModel doesn't inherently support multi-turn conversations.\\n   - Converse is designed to simplify the management of multi-turn conversations, making it more suitable for interactive applications.\\n\\n5. Model compatibility:\\n   - While both can be used with Amazon Bedrock models, Converse is recommended when supported, suggesting it may not be available for all models.\\n\\nConverse is generally preferred over InvokeModel when supported because it offers a more unified approach across different Amazon Bedrock models and provides better support for conversational applications. However, InvokeModel might still be necessary for certain models or specific use cases that require fine-grained control over generation parameters.\", 'question_type': 'complex', 'context': 'steps at Request access to an Amazon Bedrock foundation model.\\n• You\\'ve received access keys for your IAM user and configured a profile with them. Otherwise,\\nfollow the steps that are applicable to your use case at Get credentials to grant programmatic\\naccess to a user.\\nTest that your permissions and access keys are set up properly for Amazon Bedrock, using the\\nAmazon Bedrock role that you created. These examples assume that you have configured a default\\nprofile with your access keys. Note the following:\\n• Minimally, you must configure a profile containing an AWS access key ID and an AWS secret\\naccess key.\\n• If you\\'re using temporary credentials, you must also include an AWS session token.\\nTopics\\n• List the foundation models that Amazon Bedrock has to offer\\n• Submit a text prompt to a model and generate a text response with InvokeModel\\n• Submit a text prompt to a model and generate a text response with Converse\\nList the foundation models that Amazon Bedrock has to offer\\n The following example runs the ListFoundationModels operation using an Amazon Bedrock\\nendpoint. ListFoundationModels lists the foundation models (FMs) that are available in\\nAmazon Bedrock in your region. In a terminal, run the following command:\\nRun examples with the AWS CLI 17\\nAmazon Bedrock User Guide\\naws bedrock list-foundation-models --region us-east-1\\nIf the command is successful, the response returns a list of foundation models that are available in\\nAmazon Bedrock.\\nSubmit a text prompt to a model and generate a text response with InvokeModel\\nThe following example runs the InvokeModel operation using an Amazon Bedrock runtime\\nendpoint. InvokeModel lets you submit a prompt to generate a model response. In a terminal, run\\nthe following command:\\naws bedrock-runtime invoke-model \\\\\\n--model-id amazon.titan-text-express-v1 \\\\\\n--body \\'{\"inputText\": \"Describe the purpose of a \\\\\"hello world\\\\\" program in one line.\",\\n \"textGenerationConfig\" : {\"maxTokenCount\": 512, \"temperature\": 0.5, \"topP\": 0.9}}\\' \\\\\\n--cli-binary-format raw-in-base64-out \\\\\\n--outfile invoke-model-output-text.txt\\nIf the command is successful, the response generated by the model is written to the invoke-\\nmodel-output-text.txt file. The text response is returned in the outputText field, alongside\\naccompanying information.\\nSubmit a text prompt to a model and generate a text response with Converse\\nThe following example runs the Converse operation using an Amazon Bedrock runtime endpoint.\\nConverse lets you submit a prompt to generate a model response. We recommend using\\nConverse operation over InvokeModel when supported, because it unifies the inference request\\nacross Amazon Bedrock models and simplifies the management of multi-turn conversations. In a\\nterminal, run the following command:\\naws bedrock-runtime converse \\\\\\n--model-id amazon.titan-text-express-v1 \\\\\\n'}\n",
      "{'question': 'What are two main versions of Amazon Titan Image Generator G1?', 'ground_truth': 'The two main versions of Amazon Titan Image Generator G1 are v1 and v2.', 'question_type': 'simple', 'context': 'Amazon Bedrock to access the data by attaching an IAM policy to your Amazon Bedrock service\\nrole. For more information on granting an IAM policies for training data, see Grant custom jobs\\naccess to your training data.\\nHyperparameters\\nThese values can be adjusted for the Multimodal Embeddings model hyperparameters. The default\\nvalues will work well for most use cases.\\n• Learning rate - (min/max learning rate) – default: 5.00E-05, min: 5.00E-08, max: 1\\n• Batch size - Effective batch size – default: 576, min: 256, max: 9,216\\n• Max epochs – default: \"auto\", min: 1, max: 100\\nAmazon Titan Image Generator G1 models\\nAmazon Titan Image Generator G1 is an image generation model. It comes in two versions v1 and\\nv2.\\nAmazon Titan Image Generator v1 enables users to generate and edit images in versatile ways.\\nUsers can create images that match their text-based descriptions by simply inputting natural\\nlanguage prompts. Furthermore, they can upload and edit existing images, including applying\\n text-based prompts without the need for a mask, or editing specific parts of an image using an\\nimage mask. The model also supports outpainting, which extends the boundaries of an image, and\\ninpainting, which fills in missing areas. It offers the ability to generate variations of an image based\\non an optional text prompt, as well as instant customization options that allow users to transfer\\nstyles using reference images or combine styles from multiple references, all without requiring any\\nfine-tuning.\\nTitan Image Generator v2 supports all the existing features of Titan Image Generator v1 and adds\\nseveral new capabilities. It allows users to leverage reference images to guide image generation,\\nwhere the output image aligns with the layout and composition of the reference image while still\\nHyperparameters 993\\nAmazon Bedrock User Guide\\nfollowing the textual prompt. It also includes an automatic background removal feature, which can\\n remove backgrounds from images containing multiple objects without any user input. The model\\nprovides precise control over the color palette of generated images, allowing users to preserve a\\nbrand\\'s visual identity without the requirement for additional fine-tuning. Additionally, the subject\\nconsistency feature enables users to fine-tune the model with reference images to preserve the\\nchosen subject (e.g., pet, shoe or handbag) in generated images. This comprehensive suite of\\nfeatures empowers users to unleash their creative potential and bring their imaginative visions to\\nlife.\\nFor more information on Amazon Titan Image Generator G1 models prompt engineering\\nguidelines, see Amazon Titan Image Generator Prompt Engineering Best Practices.\\nTo continue supporting best practices in the responsible use of AI, Titan Foundation Models\\n(FMs) are built to detect and remove harmful content in the data, reject inappropriate content\\n'}\n",
      "{'question': 'How can an Amazon Bedrock agent be customized to enhance its functionality, and what are the key considerations when configuring these advanced features?', 'ground_truth': \"An Amazon Bedrock agent can be customized through several advanced features to enhance its functionality:\\n\\n1. Knowledge bases: You can associate knowledge groups with the agent to expand its information sources.\\n\\n2. Guardrails: You can add a guardrail to block and filter out harmful content, selecting a specific guardrail and version.\\n\\n3. Advanced prompts: You can customize the prompts sent to the foundation model (FM) at each orchestration step.\\n\\n4. Agent resource role: You must specify an ARN for a service role with permissions to call API operations on the agent.\\n\\n5. Foundation model selection: You need to specify which FM the agent will orchestrate with.\\n\\n6. Instructions: You provide instructions to guide the agent's behavior, used in the $instructions$ placeholder of the orchestration prompt template.\\n\\n7. Idle session timeout: You can set a duration after which the agent ends the session and deletes stored information.\\n\\n8. Encryption: You have the option to specify a KMS key ARN for encrypting agent resources.\\n\\nWhen configuring these features, key considerations include:\\n- Ensuring the selected knowledge bases are relevant and up-to-date\\n- Choosing appropriate guardrails to maintain safety and compliance\\n- Crafting effective custom prompts that align with the agent's purpose\\n- Granting the correct permissions in the agent's resource role\\n- Selecting a suitable foundation model for the agent's tasks\\n- Providing clear and comprehensive instructions\\n- Setting an appropriate idle session timeout to balance resource usage and user experience\\n- Implementing proper encryption for sensitive data\\n\\nThese customizations allow for a highly tailored agent that can better meet specific use cases and requirements.\", 'question_type': 'complex', 'context': '4. In the Knowledge bases section, you can choose Add to associate knowledge groups with\\nyour agent. For more information on setting up knowledge bases, see Knowledge bases for\\nAmazon Bedrock. To learn how to associate knowledge bases with your agent, see Associate\\na knowledge base with an Amazon Bedrock agent.\\n5. In the Guardrails details section, you can choose Edit to associate a guardrail with\\nyour agent to block and filter out harmful content. Select a guardrail you want to use\\nfrom the drop down menu under Select guardrail and then choose the version to use\\nunder Guardrail version. You can select View to see your Guardrail settings. For more\\ninformation, see Guardrails for Amazon Bedrock.\\n6. In the Advanced prompts section, you can choose Edit to customize the prompts that\\nare sent to the FM by your agent in each step of orchestration. For more information\\nabout the prompt templates that you can use for customization, see Advanced prompts in\\n Amazon Bedrock. To learn how to configure advanced prompts, see Configure the prompt\\ntemplates.\\n7. When you finish configuring your agent, select one of the following options:\\nCreate an agent 655\\nAmazon Bedrock User Guide\\n• To stay in the Agent builder, choose Save. You can then Prepare the agent in order to\\ntest it with your updated configurations in the test window. To learn how to test your\\nagent, see Test an Amazon Bedrock agent.\\n• To return to the Agent Details page, choose Save and exit.\\nAPI\\nTo create an agent, send a CreateAgent request (see link for request and response formats and\\nfield details) with an Agents for Amazon Bedrock build-time endpoint.\\nSee code examples\\nTo prepare your agent and test or deploy it, so that you can test or deploy it, you must\\nminimally include the following fields (if you prefer, you can skip these configurations and\\nconfigure them later by sending an UpdateAgent request):\\nField Use case\\n agentResourceRoleArn To specify an ARN of the service role with\\npermissions to call API operations on the\\nagent\\nfoundationModel To specify a foundation model (FM) for the\\nagent to orchestrate with\\ninstruction To provide instructions to tell the agent\\nwhat to do. Used in the $instructions$\\nplaceholder of the orchestration prompt\\ntemplate.\\nThe following fields are optional:\\nField Use case\\ndescription Describes what the agent does\\nCreate an agent 656\\nAmazon Bedrock User Guide\\nField Use case\\nidleSessionTTLInSeconds Duration after which the agent ends the\\nsession and deletes any stored information.\\ncustomerEncryptionKeyArn ARN of a KMS key to encrypt agent resources\\ntags To associate tags with your agent.\\npromptOverrideConfiguration To customize the prompts sent to the FM at\\neach step of orchestration.\\nguardrailConfiguration To add a guardrail to the agent. Specify the\\nID or ARN of the guardrail and the version to\\nuse.\\nclientToken Identifier to ensure the API request\\ncompletes only once.\\n'}\n",
      "{'question': \"How can you view details of an agent's orchestration process?\", 'ground_truth': 'To view details for each step of the agent\\'s orchestration process, select \"Show trace\" in the Test window. This displays the prompt, inference configurations, and agent\\'s reasoning process for each step, including usage of action groups and knowledge bases.', 'question_type': 'simple', 'context': \"generate or after it is generated, you have the following options:\\n• To view details for each step of the agent's orchestration process, including the prompt,\\ninference configurations, and agent's reasoning process for each step and usage of its\\naction groups and knowledge bases, select Show trace. The trace is updated in real-time\\nso you can view it before the response is returned. To expand or collapse the trace for a\\nstep, select an arrow next to a step. For more information about the Trace window and\\ndetails that appear, see Trace events in Amazon Bedrock.\\n• If the agent invokes a knowledge base, the response contains footnotes. To view the\\nlink to the S3 object containing the cited information for a specific part of the response,\\nselect the relevant footnote.\\nTest an agent 705\\nAmazon Bedrock User Guide\\n• If you set your agent to return control rather than using a Lambda function to handle the\\naction group, the response contains the predicted action and its parameters. Provide an\\n example output value from the API or function for the action and then choose Submit to\\ngenerate an agent response. See the following image for an example:\\nTest an agent 706\\nAmazon Bedrock User Guide\\nYou can perform the following actions in the Test window:\\nTest an agent 707\\nAmazon Bedrock User Guide\\n• To start a new conversation with the agent, select the refresh icon.\\n• To view the Trace window, select the expand icon. To close the Trace window, select the\\nshrink icon.\\n• To close the Test window, select the right arrow icon.\\nYou can enable or disable action groups and knowledge bases. Use this feature to troubleshoot\\nyour agent by isolating which action groups or knowledge bases need to be updated by\\nassessing its behavior with different settings.\\nTo enable an action group or knowledge base\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\\nbedrock/.\\n 2. Select Agents from the left navigation pane. Then, choose an agent in the Agents section.\\n3. In the Agents section. select the link for the agent that you want to test from the list of\\nagents.\\n4. On the agent's details page, in the Working draft section, select the link for the Working\\ndraft.\\n5. In the Action groups or Knowledge bases section, hover over the State of the action group\\nor knowledge base whose state you want to change.\\n6. An edit button appears. Select the edit icon and then choose from the dropdown menu\\nwhether the action group or knowledge base is Enabled or Disabled.\\n7. If an action group is Disabled, the agent doesn't use the action group. If a knowledge base\\nis Disabled, the agent doesn't use the knowledge base. Enable or disable action groups or\\nknowledge bases and then use the Test window to troubleshoot your agent.\\n8. Choose Prepare to apply the changes that you have made to the agent before testing it.\\nAPI\\n\"}\n",
      "{'question': 'How does the structure of the action response differ when an action group is defined with an API schema versus function details, and what unique fields are available in each case?', 'ground_truth': \"The structure of the action response differs significantly between action groups defined with an API schema and those defined with function details:\\n\\n1. API schema-defined action groups:\\n   - Include fields like 'apiPath', 'httpMethod', 'httpStatusCode', and 'responseBody'\\n   - 'responseBody' contains the response as defined in the OpenAPI schema\\n   - Focuses on RESTful API-style interactions\\n\\n2. Function details-defined action groups:\\n   - Include fields like 'responseState' (optional) and 'responseBody'\\n   - 'responseState' can be set to 'FAILURE' or 'REPROMPT' to control agent behavior\\n   - 'responseBody' contains an object with a content type (currently only TEXT) and the response body\\n\\nBoth types can optionally include 'sessionAttributes', 'promptSessionAttributes', and 'knowledgeBasesConfiguration'.\\n\\nThe key difference is that API schema responses are structured around HTTP concepts, while function details responses offer more control over the agent's behavior and are more flexible in terms of response formatting.\", 'question_type': 'complex', 'context': '• If you defined the action group with an API schema, the following fields can be in the\\nresponse:\\n• apiPath – The path to the API operation, as defined in the OpenAPI schema.\\n• httpMethod – The method of the API operation, as defined in the OpenAPI schema.\\n• httpStatusCode – The HTTP status code returned from the API operation.\\n• responseBody – Contains the response body, as defined in the OpenAPI schema.\\nHandling fulfillment of the action 676\\nAmazon Bedrock User Guide\\n• If you defined the action group with function details, the following fields can be in the\\nresponse:\\n• responseState (Optional) – Set to one of the following states to define the agent\\'s\\nbehavior after processing the action:\\n• FAILURE – The agent throws a DependencyFailedException for the current session.\\nApplies when the function execution fails because of a dependency failure.\\n• REPROMPT – The agent passes a response string to the model to reprompt it. Applies\\n when the function execution fails because of invalid input.\\n• responseBody – Contains an object that defines the response from execution of the\\nfunction. The key is the content type (currently only TEXT is supported) and the value is an\\nobject containing the body of the response.\\n• (Optional) sessionAttributes – Contains session attributes and their values. For more\\ninformation, see Session and prompt session attributes.\\n• (Optional) promptSessionAttributes – Contains prompt attributes and their values. For\\nmore information, see Session and prompt session attributes.\\n• (Optional) knowledgeBasesConfiguration – Contains a list of query configurations for\\nknowledge bases attached to the agent. For more information, see Knowledge base retrieval\\nconfigurations.\\nAction group Lambda function example\\nThe following is an minimal example of how the Lambda function can be defined in Python. Select\\nthe tab corresponding to whether you defined the action group with an OpenAPI schema or with\\n function details:\\nOpenAPI schema\\ndef lambda_handler(event, context):\\nagent = event[\\'agent\\']\\nactionGroup = event[\\'actionGroup\\']\\napi_path = event[\\'apiPath\\']\\n# get parameters\\nget_parameters = event.get(\\'parameters\\', [])\\n# post parameters\\npost_parameters = event[\\'requestBody\\'][\\'content\\'][\\'application/json\\']\\n[\\'properties\\']\\nHandling fulfillment of the action 677\\nAmazon Bedrock User Guide\\nresponse_body = {\\n\\'application/json\\': {\\n\\'body\\': \"sample response\"\\n}\\n}\\naction_response = {\\n\\'actionGroup\\': event[\\'actionGroup\\'],\\n\\'apiPath\\': event[\\'apiPath\\'],\\n\\'httpMethod\\': event[\\'httpMethod\\'],\\n\\'httpStatusCode\\': 200,\\n\\'responseBody\\': response_body\\n}\\nsession_attributes = event[\\'sessionAttributes\\']\\nprompt_session_attributes = event[\\'promptSessionAttributes\\']\\napi_response = {\\n\\'messageVersion\\': \\'1.0\\',\\n\\'response\\': action_response,\\n\\'sessionAttributes\\': session_attributes,\\n\\'promptSessionAttributes\\': prompt_session_attributes\\n}\\nreturn api_response\\nFunction details\\ndef lambda_handler(event, context):\\n'}\n",
      "{'question': 'What is the primary purpose of Knowledge bases for Amazon Bedrock?', 'ground_truth': 'Knowledge bases for Amazon Bedrock help you leverage Retrieval Augmented Generation (RAG), allowing you to augment responses generated by Large Language Models with information drawn from your own data store.', 'question_type': 'simple', 'context': \"that a response is based on and also check that the response makes sense and is factually correct.\\nYou take the following steps to set up and use your knowledge base.\\n1. Gather source documents to add to your knowledge base.\\n2. Store your source documents in a supported data source and configure the connection\\ninformation to connect to and crawl your data.\\n3. (Optional if using Amazon S3 to store your source documents) Create a metadata file for each\\nsource document to allow for filtering of results during knowledge base query.\\n4. (Optional) Set up a vector index in a supported vector store to index your data. You can use the\\nAmazon Bedrock console to create an Amazon OpenSearch Serverless vector database for you.\\n5. Create and configure your knowledge base. You must enable model access to use a model\\nthat's supported for knowledge bases.\\nIf you use the Amazon Bedrock API, take note of your model Amazon Resource Name (ARN)\\n that's required as part of the configuration for knowledge base retrieval and generation and\\nfor converting your data into vector embeddings. Copy the model ID for your chosen model for\\n512\\nAmazon Bedrock User Guide\\nknowledge bases and construct the model ARN using the model (resource) ID, following the\\nprovided ARN examples for your model resource type.\\nIf you use the Amazon Bedrock console, you are not required to construct a model ARN, as you\\ncan select an available model as part of the steps for creating a knowledge base.\\n6. Ingest your data by letting knowledge bases generate embeddings with an embeddings model\\nand storing them in a supported vector store.\\n7. Set up your application or agent to query the knowledge base and return augmented\\nresponses.\\nTopics\\n• How it works\\n• Supported regions and models for Knowledge bases for Amazon Bedrock\\n• Prerequisites for Knowledge bases for Amazon Bedrock\\n• Create a knowledge base\\n• Chat with your document data using the knowledge base\\n • Data source connectors\\n• Sync your data source with your Amazon Bedrock knowledge base\\n• Test a knowledge base in Amazon Bedrock\\n• Manage a knowledge base\\n• Manage a data source\\n• Deploy a knowledge base\\nHow it works\\nKnowledge bases for Amazon Bedrock help you take advantage of Retrieval Augmented\\nGeneration (RAG), a popular technique that involves drawing information from a data store\\nto augment the responses generated by Large Language Models (LLMs). When you set up a\\nknowledge base with your data sources, your application can query the knowledge base to return\\ninformation to answer the query either with direct quotations from sources or with natural\\nresponses generated from the query results.\\nWith knowledge bases, you can build applications that are enriched by the context that is received\\nfrom querying a knowledge base. It enables a faster time to market by abstracting from the heavy\\nHow it works 513\\nAmazon Bedrock User Guide\\n\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': \"How do temperature, Top K, and Top P parameters interact in Amazon Bedrock's foundation models, and how might adjusting these affect the output when generating text about different types of equines?\",\n",
       "  'ground_truth': 'Temperature, Top K, and Top P are parameters that work together to control the randomness and diversity of the model\\'s output in Amazon Bedrock\\'s foundation models. Using the equine example:\\n\\n1. Temperature: A higher temperature flattens the probability distribution, increasing the chance of selecting less probable options like \"unicorns\" while decreasing the likelihood of more common choices like \"horses\".\\n\\n2. Top K: This parameter limits the selection to the K most likely candidates. Setting Top K to 2 would only consider \"horses\" and \"zebras\", excluding less probable options like \"unicorns\".\\n\\n3. Top P: This sets a cumulative probability threshold. With Top P at 0.7, only \"horses\" would be considered as it\\'s the only option within the top 70% of the probability distribution. Increasing Top P to 0.9 would include both \"horses\" and \"zebras\".\\n\\nThe interaction of these parameters can significantly impact the output. For instance, a high temperature with a low Top K might still produce varied results within the limited selection, while a low temperature with a high Top P could lead to more predictable, common outputs. Balancing these parameters allows for fine-tuning between creativity and coherence in the generated text about equines or any other topic.',\n",
       "  'question_type': 'complex',\n",
       "  'context': '• If you set a high temperature, the probability distribution is flattened and the probabilities\\nbecome less different, which would increase the probability of choosing \"unicorns\" and decrease\\nthe probability of choosing \"horses\".\\nRandomness and diversity 239\\nAmazon Bedrock User Guide\\n• If you set Top K as 2, the model only considers the top 2 most likely candidates: \"horses\" and\\n\"zebras.\"\\n• If you set Top P as 0.7, the model only considers \"horses\" because it is the only candidate that\\nlies in the top 70% of the probability distribution. If you set Top P as 0.9, the model considers\\n\"horses\" and \"zebras\" as they are in the top 90% of probability distribution.\\nLength\\nFoundation models typically support parameters that limit the length of the response. Examples of\\nthese parameters are provided below.\\n• Response length – An exact value to specify the minimum or maximum number of tokens to\\nreturn in the generated response.\\n • Penalties – Specify the degree to which to penalize outputs in a response. Examples include the\\nfollowing.\\n• The length of the response.\\n• Repeated tokens in a response.\\n• Frequency of tokens in a response.\\n• Types of tokens in a response.\\n• Stop sequences – Specify sequences of characters that stop the model from generating further\\ntokens. If the model generates a stop sequence that you specify, it will stop generating after that\\nsequence.\\nPlaygrounds\\nImportant\\nBefore you can use any of the foundation models, you must request access to that model\\nthrough the Amazon Bedrock console. You can manage model access only through the\\nconsole. If you try to use the model (with the API or within the console) before you have\\nrequested access to it, you\\'ll receive an error message. For more information, see Manage\\naccess to Amazon Bedrock foundation models.\\nLength 240\\nAmazon Bedrock User Guide\\nThe Amazon Bedrock playgrounds provide you a console environment to experiment with running\\n inference on different models and with different configurations, before deciding to use them in an\\napplication. In the console, access the playgrounds by choosing Playgrounds in the left navigation\\npane. You can also navigate directly to the playground when you choose a model from a model\\ndetails page or the examples page.\\nThere are playgrounds for text, chat, and image models.\\nWithin each playground you can enter prompts and experiment with inference parameters.\\nPrompts are usually one or more sentences of text that set up a scenario, question, or task for a\\nmodel. For information about creating prompts, see Prompt engineering guidelines.\\nInference parameters influence the response generated by a model, such as the randomness of\\ngenerated text. When you load a model into a playground, the playground configures the model\\nwith its default inference settings. You can change and reset the settings as you experiment\\n'},\n",
       " {'question': 'How long will Amazon Bedrock support base models after launch in a region?',\n",
       "  'ground_truth': 'Amazon Bedrock will support base models for a minimum of 12 months from the launch in a region.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"• EOL: This version is no longer available for use. Any requests made to this version will fail.\\nWe will support base models for a minimum of 12 months from the launch in a region. We will\\nalways give customers 6 months of notice before we mark the model EOL. Model will be marked\\nLegacy on the date when the EOL notice is sent out. The console marks a model version's state as\\nActive or Legacy. When you make a GetFoundationModelor ListFoundationModels call, you can\\nfind the state of the model in the modelLifecycle field in the response. While you can continue\\nto use a Legacy version, you should plan to transition to an Active version before the EOL date.\\nOn-Demand, Provisioned Throughput, and model customization\\nYou specify the version of a model when you use it in On-Demand mode (for example,\\nanthropic.claude-v2, anthropic.claude-v2:1, etc.).\\nWhen you configure Provisioned Throughput, you must specify a model version that will remain\\nunchanged for the entire term.\\nModel lifecycle 53\\n Amazon Bedrock User Guide\\nIf you customized a model, you can continue to use it until the EOL date of the base model version\\nthat you used for customization. You can also customize a legacy model version, but you should\\nplan to migrate before it reaches its EOL date.\\nNote\\nService quotas are shared among model minor versions.\\nLegacy versions\\nThe following table shows the legacy versions of models available on Amazon Bedrock.\\nModel version Legacy date EOL date Recommended Recommended\\nmodel version model ID\\nreplacement\\nStable Diffusion February 2, April 30, 2024 Stable Diffusion stability.stable-d\\nXL 0.8 2024 XL 1.x iffusion-xl-v1\\nClaude v1.3 November 28, February 28, Claude v2.1 anthropic\\n2023 2024 .claude-v2:1\\nTitan November 7, February 15, Titan amazon.titan-\\nEmbeddings - 2023 2024 Embeddings - embed-text-v1\\nText v1.1 Text v1.2\\nMeta Llama 2 May 12, 2024 October 30, Meta Llama3 meta.llam\\n13b-chat-v1, 2024 and Meta a3-1-8b-i\\nMeta Llama 2 Llama3.1 nstruct-v1,\\n70b-chat-v1, meta.llam\\n Meta Llama a3-1-70b-\\n2-13b, Meta instruct-v1,\\nLlama 2-70b meta.llam\\na3-1-405b-\\ninstruct-v1,\\nmeta.llama3-8b-\\ninstruct-v1,\\nLegacy versions 54\\nAmazon Bedrock User Guide\\nModel version Legacy date EOL date Recommended Recommended\\nmodel version model ID\\nreplacement\\nand meta.llam\\na3-70b-instruct-\\nv1\\nAi21 J2 Mid- April 30, 2024 August 31, 2024 N/A N/A\\nv1 and Ai21 J2 (only in us- (only in us-\\nUltra-v1 west-2) west-2)\\nAmazon Bedrock model IDs\\nMany Amazon Bedrock API operations require the use of a model ID. Refer to the following table to\\ndetermine where to find the model ID that you need to use.\\nUse case How to find the model ID\\nUse a base model Look up the ID in the base model IDs chart\\nPurchase Provisioned Throughput for a base Look up the ID in the model IDs for Provision\\nmodel ed Throughput chart and use it as the\\nmodelId in the CreateProvisionedModelThrou\\nghput request.\\nPurchase Provisioned Throughput for a Use the name of the custom model or its ARN\\n\"},\n",
       " {'question': \"How does the system handle a scenario where a transaction status changes from 'Pending' to 'Paid', and what functions would be involved in updating and retrieving this information?\",\n",
       "  'ground_truth': \"The system doesn't explicitly show a function for updating transaction statuses, but it does provide functions for retrieving payment status and date. If a transaction status changes from 'Pending' to 'Paid', the underlying data in the DataFrame 'df' would need to be updated first. Then, to reflect and retrieve this change, two functions would be involved:\\n\\n1. retrieve_payment_status(df, transaction_id): This function would return the updated status 'Paid' for the given transaction_id.\\n\\n2. retrieve_payment_date(df, transaction_id): This function would return the date when the payment was made, which would likely be updated when the status changes to 'Paid'.\\n\\nThese functions are wrapped in a tool-calling system that uses Amazon Bedrock with a Mistral AI model. The invoke_bedrock_mistral_tool() function would be used to call these functions through the AI model, interpreting user queries and returning the appropriate information. The system uses JSON for data exchange and includes error handling for cases where a transaction ID is not found.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': '\\'payment_date\\': [\\'2021-10-05\\', \\'2021-10-06\\', \\'2021-10-07\\', \\'2021-10-05\\',\\n\\'2021-10-08\\'],\\n\\'payment_status\\': [\\'Paid\\', \\'Unpaid\\', \\'Paid\\', \\'Paid\\', \\'Pending\\']\\n}\\n# Create DataFrame\\ndf = pd.DataFrame(data)\\ndef retrieve_payment_status(df: data, transaction_id: str) -> str:\\nif transaction_id in df.transaction_id.values:\\nreturn json.dumps({\\'status\\': df[df.transaction_id ==\\ntransaction_id].payment_status.item()})\\nreturn json.dumps({\\'error\\': \\'transaction id not found.\\'})\\nMistral AI models 198\\nAmazon Bedrock User Guide\\ndef retrieve_payment_date(df: data, transaction_id: str) -> str:\\nif transaction_id in df.transaction_id.values:\\nreturn json.dumps({\\'date\\': df[df.transaction_id ==\\ntransaction_id].payment_date.item()})\\nreturn json.dumps({\\'error\\': \\'transaction id not found.\\'})\\ntools = [\\n{\\n\"type\": \"function\",\\n\"function\": {\\n\"name\": \"retrieve_payment_status\",\\n\"description\": \"Get payment status of a transaction\",\\n\"parameters\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"transaction_id\": {\\n\"type\": \"string\",\\n \"description\": \"The transaction id.\",\\n}\\n},\\n\"required\": [\"transaction_id\"],\\n},\\n},\\n},\\n{\\n\"type\": \"function\",\\n\"function\": {\\n\"name\": \"retrieve_payment_date\",\\n\"description\": \"Get payment date of a transaction\",\\n\"parameters\": {\\n\"type\": \"object\",\\n\"properties\": {\\n\"transaction_id\": {\\n\"type\": \"string\",\\n\"description\": \"The transaction id.\",\\n}\\n},\\n\"required\": [\"transaction_id\"],\\n},\\n},\\n}\\n]\\nnames_to_functions = {\\nMistral AI models 199\\nAmazon Bedrock User Guide\\n\\'retrieve_payment_status\\': functools.partial(retrieve_payment_status, df=df),\\n\\'retrieve_payment_date\\': functools.partial(retrieve_payment_date, df=df)\\n}\\ntest_tool_input = \"What\\'s the status of my transaction T1001?\"\\nmessage = [{\"role\": \"user\", \"content\": test_tool_input}]\\ndef invoke_bedrock_mistral_tool():\\nmistral_params = {\\n\"body\": json.dumps({\\n\"messages\": message,\\n\"tools\": tools\\n}),\\n\"modelId\":\"mistral.mistral-large-2407-v1:0\",\\n}\\nresponse = bedrock.invoke_model(**mistral_params)\\nbody = response.get(\\'body\\').read().decode(\\'utf-8\\')\\n body = json.loads(body)\\nchoices = body.get(\"choices\")\\nmessage.append(choices[0].get(\"message\"))\\ntool_call = choices[0].get(\"message\").get(\"tool_calls\")[0]\\nfunction_name = tool_call.get(\"function\").get(\"name\")\\nfunction_params = json.loads(tool_call.get(\"function\").get(\"arguments\"))\\nprint(\"\\\\nfunction_name: \", function_name, \"\\\\nfunction_params: \",\\nfunction_params)\\nfunction_result = names_to_functions[function_name](**function_params)\\nmessage.append({\"role\": \"tool\", \"content\": function_result,\\n\"tool_call_id\":tool_call.get(\"id\")})\\nnew_mistral_params = {\\n\"body\": json.dumps({\\n\"messages\": message,\\n\"tools\": tools\\n}),\\n\"modelId\":\"mistral.mistral-large-2407-v1:0\",\\n}\\nresponse = bedrock.invoke_model(**new_mistral_params)\\nbody = response.get(\\'body\\').read().decode(\\'utf-8\\')\\nMistral AI models 200\\nAmazon Bedrock User Guide\\nbody = json.loads(body)\\nprint(body)\\ninvoke_bedrock_mistral_tool()\\nStability.ai Diffusion models\\n'},\n",
       " {'question': 'What is the purpose of an S3 retrieval node in a prompt flow?',\n",
       "  'ground_truth': 'An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. It takes an object key as input and returns the content of the S3 location as output.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. In\\nthe configuration, you specify the S3 bucket from which to retrieve data. The input into the node is\\nthe object key. The node returns the content in the S3 location as the output.\\nNote\\nCurrently, the data in the S3 location must be a UTF-8 encoded string.\\nThe following shows the general structure of an S3 retrieval FlowNode object:\\n{\\n\"name\": \"string\",\\n\"type\": \"Retrieval\",\\n\"inputs\": [\\n{\\n\"name\": \"objectKey\",\\n\"type\": \"String\",\\n\"expression\": \"string\"\\n}\\n],\\n\"outputs\": [\\n{\\n\"name\": \"s3Content\",\\n\"type\": \"String\"\\n}\\n],\\n\"configuration\": {\\n\"retrieval\": {\\n\"serviceConfiguration\": {\\n\"s3\": {\\n\"bucketName\": \"string\"\\n}\\n}\\n}\\n}\\n}\\nNode types in prompt flow 846\\nAmazon Bedrock User Guide\\nLambda function node\\nA Lambda function node lets you call a Lambda function in which you can define code to carry out\\nbusiness logic. When you include a Lambda node in a prompt flow, Amazon Bedrock sends an input\\n event to the Lambda function that you specify.\\nIn the configuration, specify the Amazon Resource Name (ARN) of the Lambda function. Define\\ninputs to send in the Lambda input event. You can write code based on these inputs and define\\nwhat the function returns. The function response is returned in the output.\\nThe following shows the general structure of a Λ function FlowNode object:\\n{\\n\"name\": \"string\",\\n\"type\": \"LambdaFunction\",\\n\"inputs\": [\\n{\\n\"name\": \"string\",\\n\"type\": \"String | Number | Boolean | Object | Array\",\\n\"expression\": \"string\"\\n},\\n...\\n],\\n\"outputs\": [\\n{\\n\"name\": \"functionResponse\",\\n\"type\": \"String | Number | Boolean | Object | Array\"\\n}\\n],\\n\"configuration\": {\\n\"lambdaFunction\": {\\n\"lambdaArn\": \"string\"\\n}\\n}\\n}\\nLambda input event for a prompt flow\\nThe input event sent to a Lambda function in a Lambda node is of the following format:\\n{\\n\"messageVersion\": \"1.0\",\\nNode types in prompt flow 847\\nAmazon Bedrock User Guide\\n\"flow\": {\\n\"flowArn\": \"string\",\\n\"flowAliasArn\": \"string\"\\n},\\n\"node\": {\\n \"name\": \"string\",\\n\"nodeInputs\": [\\n{\\n\"name\": \"string\",\\n\"type\": \"String | Number | Boolean | Object | Array\",\\n\"expression\": \"string\",\\n\"value\": ...\\n},\\n...\\n]\\n}\\n}\\nThe fields for each input match the fields that you specify when defining the Lambda node, while\\nthe value of the value field is populated with the whole input into the node after being resolved\\nby the expression. For example, if the whole input into the node is [1, 2, 3] and the expression\\nis $.data[1], the value sent in the input event to the Lambda function would be 2.\\nFor more information about events in Lambda, see Lambda concepts in the AWS Lambda\\nDeveloper Guide.\\nLambda response for a prompt flow\\nWhen you write a Lambda function, you define the response returned by it. This response is\\nreturned to your prompt flow as the output of the Lambda node.\\nLex node\\nA Lex node lets you call a Amazon Lex bot to process an utterance using natural language\\n'},\n",
       " {'question': \"How can a developer create a new prompt version, retrieve its information, and incorporate it into a prompt flow using Amazon Bedrock's Python SDK?\",\n",
       "  'ground_truth': 'To create a new prompt version, retrieve its information, and incorporate it into a prompt flow using Amazon Bedrock\\'s Python SDK, a developer should follow these steps:\\n\\n1. Create a new prompt version using the create_prompt_version() method, specifying the prompt_id.\\n2. Retrieve the version number and ARN from the response.\\n3. List all versions of the prompt using list_prompts() with the prompt_id.\\n4. Get detailed information about the specific version using get_prompt() with prompt_id and prompt_version.\\n5. To incorporate the prompt into a flow:\\n   a. Create a boto3 client for \\'bedrock-agent\\'.\\n   b. Define the flow\\'s structure using Python dictionaries for input, prompt, and output nodes.\\n   c. In the prompt node configuration, set the promptArn to the ARN of the created prompt version.\\n   d. Ensure the prompt node\\'s inputs match the expected variables (e.g., \"genre\" and \"number\").\\n   e. Set up the output node to return the model\\'s completion as a string.\\n\\nThis process allows for version control of prompts and their seamless integration into prompt flows.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'make a CreatePromptVersion Agents for Amazon Bedrock build-time endpoint:\\n# Create a version of the prompt that you created\\nresponse = client.create_prompt_version(promptIdentifier=prompt_id)\\nprompt_version = response.get(\"version\")\\nprompt_version_arn = response.get(\"arn\")\\n5. View information about the prompt version that you just created, alongside information\\nabout the draft version, by running the following code snippet to make a ListPrompts\\nAgents for Amazon Bedrock build-time endpoint:\\n# List versions of the prompt that you just created\\nclient.list_prompts(promptIdentifier=prompt_id)\\n6. View information for the prompt version that you just created by running the following\\ncode snippet to make a GetPrompt Agents for Amazon Bedrock build-time endpoint:\\n# Get information about the prompt version that you created\\nclient.get_prompt(\\npromptIdentifier=prompt_id,\\npromptVersion=prompt_version\\n)\\n7. Test the prompt by adding it to a prompt flow by following the steps at Run Prompt flows\\n code samples. In the first step when you create the flow, run the following code snippet\\ninstead to use the prompt that you created instead of defining an inline prompt in the flow\\n(replace the ARN of the prompt version in the promptARN field with the ARN of the version\\nof the prompt that you created):\\n# Import Python SDK and create client\\nimport boto3\\nclient = boto3.client(service_name=\\'bedrock-agent\\')\\nFLOWS_SERVICE_ROLE = \"arn:aws:iam::123456789012:role/MyPromptFlowsRole\" #\\nPrompt flows service role that you created. For more information, see https://\\ndocs.aws.amazon.com/bedrock/latest/userguide/flows-permissions.html\\nPROMPT_ARN = prompt_version_arn # ARN of the prompt that you created, retrieved\\nprogramatically during creation.\\nRun code samples 351\\nAmazon Bedrock User Guide\\n# Define each node\\n# The input node validates that the content of the InvokeFlow request is a JSON\\nobject.\\ninput_node = {\\n\"type\": \"Input\",\\n\"name\": \"FlowInput\",\\n\"outputs\": [\\n{\\n\"name\": \"document\",\\n\"type\": \"Object\"\\n }\\n]\\n}\\n# This prompt node contains a prompt that you defined in Prompt management.\\n# It validates that the input is a JSON object that minimally contains the\\nfields \"genre\" and \"number\", which it will map to the prompt variables.\\n# The output must be named \"modelCompletion\" and be of the type \"String\".\\nprompt_node = {\\n\"type\": \"Prompt\",\\n\"name\": \"MakePlaylist\",\\n\"configuration\": {\\n\"prompt\": {\\n\"sourceConfiguration\": {\\n\"resource\": {\\n\"promptArn\": \"\"\\n}\\n}\\n}\\n},\\n\"inputs\": [\\n{\\n\"name\": \"genre\",\\n\"type\": \"String\",\\n\"expression\": \"$.data.genre\"\\n},\\n{\\n\"name\": \"number\",\\n\"type\": \"Number\",\\n\"expression\": \"$.data.number\"\\n}\\n],\\nRun code samples 352\\nAmazon Bedrock User Guide\\n\"outputs\": [\\n{\\n\"name\": \"modelCompletion\",\\n\"type\": \"String\"\\n}\\n]\\n}\\n# The output node validates that the output from the last node is a string and\\nreturns it as is. The name must be \"document\".\\noutput_node = {\\n\"type\": \"Output\",\\n\"name\": \"FlowOutput\",\\n\"inputs\": [\\n{\\n\"name\": \"document\",\\n\"type\": \"String\",\\n\"expression\": \"$.data\"\\n}\\n]\\n}\\n'},\n",
       " {'question': 'What metric is used to evaluate toxicity in general text generation tasks?',\n",
       "  'ground_truth': 'Toxicity is used as a metric to evaluate toxicity in general text generation tasks, using the RealToxicPrompts dataset.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"2. In the navigation pane, choose Model evaluation.\\n3. In the Model Evaluation Jobs card, you can find a table that lists the model evaluation jobs\\nyou have already created.\\n4. Select the radio button next to your job's name.\\n5. Then, choose Stop evaluation.\\nAWS CLI\\nIn the AWS CLI, you can use the help command to view parameters are required, and which\\nparameters are optional when using list-evaluation-jobs.\\nList model evaluation jobs 457\\nAmazon Bedrock User Guide\\naws bedrock list-evaluation-jobs help\\nThe follow is an example of using list-evaluation-jobs and specifying that maximum of\\n5 jobs be returned. By default jobs are returned in descending order from the time when they\\nwhere started.\\naws bedrock list-evaluation-jobs --max-items 5\\nSDK for Python\\nThe following examples show how to use the AWS SDK for Python to find a model evaluation\\njob you have previously created.\\nimport boto3\\nclient = boto3.client('bedrock')\\njob_request = client.list_evaluation_jobs(maxResults=20)\\n print (job_request)\\nModel evaluation tasks\\nIn a model evaluation job, an evaluation task ( taskType ) is a task you want the model\\nto perform based on information in your prompts. You can choose one task type per model\\nevaluation job.\\nThe following topics to learn more about each task type. Each topic also includes a list of available\\nbuilt-in datasets and their corresponding metrics that can be used only in automatic model\\nevaluation jobs.\\nThe following table summarizes available tasks types, built-in datasets, and computer metrics for\\neach task type.\\nModel evaluation tasks 458\\nAmazon Bedrock User Guide\\nAvailable built-in datasets for automatic model evaluation jobs in Amazon Bedrock\\nTask type MetricBuilt-in Computed metric\\ndatasets\\nGeneral text AccuraTcRyEX Real world knowledge (RWK) score\\ngeneration\\nRobusBtnOeLsD Word error rate\\ns\\nTREX\\nWikiText2\\nToxicitRyealToxic Toxicity\\nityPrompts\\nBOLD\\nText AccuraGciygaword BERTScore\\nsummarization\\nToxicitGyigaword Toxicity\\n RobusGtnigeas word BERTScore and deltaBERTScore\\ns\\nQuestion and AccuraBcoyolQ NLP-F1\\nanswer\\nNaturalQu\\nestions\\nTriviaQA\\nRobusBtnoeosl Q F1 and deltaF1\\ns\\nNaturalQu\\nestions\\nTriviaQA\\nToxicitByoolQ Toxicity\\nModel evaluation tasks 459\\nAmazon Bedrock User Guide\\nTask type MetricBuilt-in Computed metric\\ndatasets\\nNaturalQu\\nestions\\nTriviaQA\\nText classific AccuraWcyomen's Accuracy (Binary accuracy from classification_accuracy_s\\nation Ecommerce core)\\nClothing\\nReviews\\nRobusWtnoems en's classification_accuracy_score and delta_classifica\\ns Ecommerce tion_accuracy_score\\nClothing\\nReviews\\nTopics\\n• General text generation\\n• Text summarization\\n• Question and answer\\n• Text classification\\nGeneral text generation\\nImportant\\nFor general text generation, there is a known system issue that prevents Cohere models\\nfrom completing the toxicity evaluation successfully.\\nGeneral text generation is a task used by applications that include chatbots. The responses\\n\"},\n",
       " {'question': 'How does the lambda_handler function handle different types of responses from the LLM, and what are the potential outcomes of its parsing process?',\n",
       "  'ground_truth': 'The lambda_handler function in the given code snippet handles different types of responses from the LLM through a series of parsing steps and error checks:\\n\\n1. It first sanitizes the LLM response and extracts any rationale.\\n\\n2. Then, it attempts to parse for a final answer. If successful, it constructs a response with \\'invocationType\\': \\'FINISH\\' and includes the final answer and any generated response parts (citations).\\n\\n3. If no final answer is found, it tries to parse for an \"ask user\" response. If found, it constructs a response with \\'invocationType\\': \\'ASK_USER\\' and includes the question to ask the user.\\n\\n4. If neither a final answer nor an \"ask user\" response is found, it attempts to parse for an agent action (though this part is not fully shown in the snippet).\\n\\n5. Throughout the process, if any parsing errors occur (e.g., incorrect function call format), it adds a reprompt response to guide the LLM to provide the correct format.\\n\\nThe potential outcomes of this parsing process are:\\na) A successful \\'FINISH\\' response with a final answer and possible citations\\nb) A successful \\'ASK_USER\\' response with a question to be asked to the user\\nc) A reprompt response if there are formatting errors in the LLM output\\nd) Potentially, an agent action response (not fully shown in the snippet)\\n\\nThis structured approach allows the function to handle various LLM output scenarios and provide appropriate responses or error handling for each case.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'ANSWER_PART_REGEX = \"<answer_part\\\\\\\\s?>(.+?)</answer_part\\\\\\\\s?>\"\\nANSWER_TEXT_PART_REGEX = \"<text\\\\\\\\s?>(.+?)</text\\\\\\\\s?>\"\\nANSWER_REFERENCE_PART_REGEX = \"<source\\\\\\\\s?>(.+?)</source\\\\\\\\s?>\"\\nANSWER_PART_PATTERN = re.compile(ANSWER_PART_REGEX, re.DOTALL)\\nANSWER_TEXT_PART_PATTERN = re.compile(ANSWER_TEXT_PART_REGEX, re.DOTALL)\\nANSWER_REFERENCE_PART_PATTERN = re.compile(ANSWER_REFERENCE_PART_REGEX, re.DOTALL)\\n# You can provide messages to reprompt the LLM in case the LLM output is not in\\nthe expected format\\nMISSING_API_INPUT_FOR_USER_REPROMPT_MESSAGE = \"Missing the argument askuser for\\nuser::askuser function call. Please try again with the correct argument added\"\\nASK_USER_FUNCTION_CALL_STRUCTURE_REMPROMPT_MESSAGE = \"The function call format\\nis incorrect. The format for function calls to the askuser function must be:\\n<function_call>user::askuser(askuser=\\\\\"$ASK_USER_INPUT\\\\\")</function_call>.\"\\nFUNCTION_CALL_STRUCTURE_REPROMPT_MESSAGE = \\'The function call format\\n is incorrect. The format for function calls must be: <function_call>\\n$FUNCTION_NAME($FUNCTION_ARGUMENT_NAME=\"\"$FUNCTION_ARGUMENT_NAME\"\")</\\nfunction_call>.\\'\\nlogger = logging.getLogger()\\nAdvanced prompts 767\\nAmazon Bedrock User Guide\\n# This parser lambda is an example of how to parse the LLM output for the default\\norchestration prompt\\ndef lambda_handler(event, context):\\nlogger.info(\"Lambda input: \" + str(event))\\n# Sanitize LLM response\\nsanitized_response = sanitize_response(event[\\'invokeModelRawResponse\\'])\\n# Parse LLM response for any rationale\\nrationale = parse_rationale(sanitized_response)\\n# Construct response fields common to all invocation types\\nparsed_response = {\\n\\'promptType\\': \"ORCHESTRATION\",\\n\\'orchestrationParsedResponse\\': {\\n\\'rationale\\': rationale\\n}\\n}\\n# Check if there is a final answer\\ntry:\\nfinal_answer, generated_response_parts = parse_answer(sanitized_response)\\nexcept ValueError as e:\\naddRepromptResponse(parsed_response, e)\\nreturn parsed_response\\nif final_answer:\\n parsed_response[\\'orchestrationParsedResponse\\'][\\'responseDetails\\'] = {\\n\\'invocationType\\': \\'FINISH\\',\\n\\'agentFinalResponse\\': {\\n\\'responseText\\': final_answer\\n}\\n}\\nif generated_response_parts:\\nparsed_response[\\'orchestrationParsedResponse\\'][\\'responseDetails\\']\\n[\\'agentFinalResponse\\'][\\'citations\\'] = {\\n\\'generatedResponseParts\\': generated_response_parts\\n}\\nlogger.info(\"Final answer parsed response: \" + str(parsed_response))\\nreturn parsed_response\\nAdvanced prompts 768\\nAmazon Bedrock User Guide\\n# Check if there is an ask user\\ntry:\\nask_user = parse_ask_user(sanitized_response)\\nif ask_user:\\nparsed_response[\\'orchestrationParsedResponse\\'][\\'responseDetails\\'] = {\\n\\'invocationType\\': \\'ASK_USER\\',\\n\\'agentAskUser\\': {\\n\\'responseText\\': ask_user\\n}\\n}\\nlogger.info(\"Ask user parsed response: \" + str(parsed_response))\\nreturn parsed_response\\nexcept ValueError as e:\\naddRepromptResponse(parsed_response, e)\\nreturn parsed_response\\n# Check if there is an agent action\\ntry:\\n'},\n",
       " {'question': 'How can users download third-party audit reports for AWS services?',\n",
       "  'ground_truth': 'Users can download third-party audit reports using AWS Artifact.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"• To learn the difference between using roles and resource-based policies for cross-account access,\\nsee Cross account resource access in IAM in the IAM User Guide.\\nCompliance validation for Amazon Bedrock\\nTo learn whether an AWS service is within the scope of specific compliance programs, see AWS\\nservices in Scope by Compliance Program and choose the compliance program that you are\\ninterested in. For general information, see AWS Compliance Programs.\\nCompliance validation 1155\\nAmazon Bedrock User Guide\\nYou can download third-party audit reports using AWS Artifact. For more information, see\\nDownloading Reports in AWS Artifact.\\nYour compliance responsibility when using AWS services is determined by the sensitivity of your\\ndata, your company's compliance objectives, and applicable laws and regulations. AWS provides the\\nfollowing resources to help with compliance:\\n• Security and Compliance Quick Start Guides – These deployment guides discuss architectural\\n considerations and provide steps for deploying baseline environments on AWS that are security\\nand compliance focused.\\n• Architecting for HIPAA Security and Compliance on Amazon Web Services – This whitepaper\\ndescribes how companies can use AWS to create HIPAA-eligible applications.\\nNote\\nNot all AWS services are HIPAA eligible. For more information, see the HIPAA Eligible\\nServices Reference.\\n• AWS Compliance Resources – This collection of workbooks and guides might apply to your\\nindustry and location.\\n• AWS Customer Compliance Guides – Understand the shared responsibility model through the\\nlens of compliance. The guides summarize the best practices for securing AWS services and map\\nthe guidance to security controls across multiple frameworks (including National Institute of\\nStandards and Technology (NIST), Payment Card Industry Security Standards Council (PCI), and\\nInternational Organization for Standardization (ISO)).\\n • Evaluating Resources with Rules in the AWS Config Developer Guide – The AWS Config service\\nassesses how well your resource configurations comply with internal practices, industry\\nguidelines, and regulations.\\n• AWS Security Hub – This AWS service provides a comprehensive view of your security state within\\nAWS. Security Hub uses security controls to evaluate your AWS resources and to check your\\ncompliance against security industry standards and best practices. For a list of supported services\\nand controls, see Security Hub controls reference.\\n• Amazon GuardDuty – This AWS service detects potential threats to your AWS accounts,\\nworkloads, containers, and data by monitoring your environment for suspicious and malicious\\nactivities. GuardDuty can help you address various compliance requirements, like PCI DSS, by\\nmeeting intrusion detection requirements mandated by certain compliance frameworks.\\nCompliance validation 1156\\nAmazon Bedrock User Guide\\n\"},\n",
       " {'question': 'How can an administrator troubleshoot a failed data source sync event and optimize future sync operations in Amazon Bedrock, considering both console and API methods?',\n",
       "  'ground_truth': \"To troubleshoot a failed data source sync event and optimize future operations in Amazon Bedrock, an administrator can follow these steps:\\n\\n1. Console method:\\n   a. Navigate to the Amazon Bedrock console and select 'Knowledge bases' from the left navigation pane.\\n   b. In the Data source section, select the relevant data source.\\n   c. Check the Sync history for details about sync events.\\n   d. For failed syncs, select the event and choose 'View warnings' to see specific reasons for failure.\\n\\n2. API method:\\n   a. Use the GetIngestionJob API request to retrieve detailed information about a specific sync event. Provide the dataSourceId, knowledgeBaseId, and ingestionJobId.\\n   b. Use the ListIngestionJobs API request to review the sync history, filtering results by status and sorting by start time or job status.\\n\\n3. Optimization strategies:\\n   a. Analyze patterns in failed sync events to identify common issues.\\n   b. Update the data source configurations if necessary, ensuring the IAM role has the required permissions.\\n   c. If using a custom endpoint, verify the Secrets Manager secret is up-to-date.\\n   d. Consider adjusting the maxResults parameter in API calls to balance between performance and completeness of information.\\n   e. Implement pagination using the nextToken in API responses for comprehensive reviews of large sync histories.\\n\\nBy combining console and API methods for troubleshooting and implementing these optimization strategies, administrators can effectively manage and improve data source sync operations in Amazon Bedrock.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': 'To view information about a data source\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\\nbedrock/.\\n2. From the left navigation pane, select Knowledge bases.\\n3. In the Data source section, select the data source for which you want to view details.\\n4. The Data source overview contains details about the data source.\\n5. The Sync history contains details about when the data source was synced. To see reasons\\nfor why a sync event failed, select a sync event and choose View warnings.\\nAPI\\nTo get information about a data source, send a GetDataSource request with a Agents\\nfor Amazon Bedrock build-time endpoint and specify the dataSourceId and the\\nknowledgeBaseId of the knowledge base that it belongs to.\\nTo list information about a knowledge base\\'s data sources, send a ListDataSources request with\\n a Agents for Amazon Bedrock build-time endpoint and specify the ID of the knowledge base.\\n• To set the maximum number of results to return in a response, use the maxResults field.\\n• If there are more results than the number you set, the response returns a nextToken. You\\ncan use this value in another ListDataSources request to see the next batch of results.\\nManage a data source 636\\nAmazon Bedrock User Guide\\nTo get information a sync event for a data source, send a GetIngestionJob request with a Agents\\nfor Amazon Bedrock build-time endpoint. Specify the dataSourceId, knowledgeBaseId, and\\ningestionJobId.\\nTo list the sync history for a data source in a knowledge base, send a ListIngestionJobs request\\nwith a Agents for Amazon Bedrock build-time endpoint. Specify the ID of the knowledge base\\nand data source. You can set the following specifications.\\n• Filter for results by specifying a status to search for in the filters object.\\n • Sort by the time that the job was started or the status of a job by specifying the sortBy\\nobject. You can sort in ascending or descending order.\\n• Set the maximum number of results to return in a response in the maxResults field. If there\\nare more results than the number you set, the response returns a nextToken that you can\\nsend in another ListIngestionJobs request to see the next batch of jobs.\\nUpdate a data source\\nYou can update a data source in the following ways:\\n• Add, change, or remove files or content from the the data source.\\n• Change the data source configurations, or the KMS key to use for encrypting transient data\\nduring data ingestion. If you change the source or endpoint configuration details, you should\\nupdate or create a new IAM role with the required access permissions and Secrets Manager secret\\n(if applicable).\\n• Set your data source deletion policy is to either \"Delete\" or \"Retain\". You can delete all data from\\n'},\n",
       " {'question': 'How do you enable Prompt flows and Prompt management in Bedrock Studio?',\n",
       "  'ground_truth': 'To enable Prompt flows and Prompt management in Bedrock Studio, update the workspace by modifying the provisioning role, updating the permissions boundary, and adding Amazon DataZone blueprints.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://\\nconsole.aws.amazon.com/bedrock/.\\n2. In the left navigation pane, choose Bedrock Studio.\\n3. In Bedrock Studio workspaces, select the workspace that you want to update.\\n4. Choose the Overview tab. If the workspace needs an update to support Prompt flows and\\nPrompt management, you will see an alert banner with steps for enabling Prompt flows and\\nPrompt management.\\n5. In Workspace details, choose the provisioning role ARN in Provisioning role. The IAM console\\nopens with the provisioning role.\\n6. In the IAM console, choose the Permissions tab.\\n7. In Permission policies select the policy to open the policy editor.\\n8. In the Policy editor, choose JSON, if it is not already chosen.\\n9. Replace the current policy with the policy at Permissions to manage Amazon Bedrock Studio\\nuser resources.\\n10. Choose Next.\\nUpdate the provisioning role 1010\\nAmazon Bedrock User Guide\\n11. Choose Save changes.\\n 12. Next step: Update the permissions boundary.\\nUpdate the permissions boundary\\nIn this procedure, you update the permissions boundary for a Amazon Bedrock Studio workspace.\\nUpdating the permissions boundary helps enable Prompt flows and Prompt management.\\nTo update the permission boundaries\\n1. Sign in to the AWS Management Console and open the IAM console at https://\\nconsole.aws.amazon.com/iam/.\\n2. On the left navigation pane, choose Policies.\\n3. Open the AmazonDataZoneBedrockPermissionsBoundary policy that you created in Step\\n2: Create permissions boundary, service role, and provisioning role.\\n4. On the Permissions tab, choose Edit.\\n5. In the Policy editor, choose JSON, if it is not already chosen.\\n6. Replace the current policy with the policy at Permission boundaries.\\n7. Choose Next.\\n8. Choose Save changes.\\n9. Next step: Add the Amazon DataZone blueprints.\\nAdd the Amazon DataZone blueprints\\nIn this procedure, you add the Amazon DataZone blueprints that an Amazon Bedrock Studio\\n workspace needs to enable Prompt flows and Prompt management.\\nTo add the Amazon DataZone blueprints\\n1. Sign in to the AWS Management Console and open the Amazon Bedrock console at https://\\nconsole.aws.amazon.com/bedrock/.\\n2. In the left navigation pane, choose Bedrock Studio.\\n3. In Bedrock Studio workspaces, select the workspace that you want to add the blueprints to.\\n4. Choose the Overview tab.\\nUpdate the permissions boundary 1011\\nAmazon Bedrock User Guide\\n5. In Workspace details, note the alert banner for Prompt management and Prompt flows. Make\\nsure you have completed step one.\\n6. In the alert banner, choose the Enable hyperlink to add the blueprints.\\nUpdate a workspace for app export\\nAmazon Bedrock Studio is in preview release for Amazon Bedrock and is subject to change.\\nIf you created an Amazon Bedrock Studio workspace before the introduction of the app export\\nfeature, you need to update the permissions boundary for the workspace. You don't need to\\n\"},\n",
       " {'question': 'How might the choice between using the RetrieveAndGenerate API and the Retrieve API in Amazon Bedrock affect the flexibility of model selection for response generation?',\n",
       "  'ground_truth': 'The choice between RetrieveAndGenerate API and Retrieve API in Amazon Bedrock significantly affects model selection flexibility. RetrieveAndGenerate API queries the knowledge base and generates responses using only supported Amazon Bedrock knowledge base models, limiting options to specific models like Amazon Titan Text Premier and Anthropic Claude variants. In contrast, the Retrieve API only queries the knowledge base without generating responses, allowing users to subsequently use the retrieved results with any Amazon Bedrock or SageMaker model in an InvokeModel request. This approach offers greater flexibility in choosing models for response generation, potentially enabling the use of specialized or custom models beyond the predefined set supported by RetrieveAndGenerate.',\n",
       "  'question_type': 'complex',\n",
       "  'context': \"Cohere Embed (Multilingual) cohere.embed-multilingual-v3\\nYou can use the following models to generate responses after retrieving information from\\nknowledge bases:\\nNote\\nThe RetrieveAndGenerate API queries the knowledge base and uses supported Amazon\\nBedrock knowledge base models to generate responses from the information it retrieves.\\nThe Retrieve API only queries the knowledge base; it doesn't generate responses.\\nTherefore, after retrieving results with the Retrieve API, you could use the results in\\nan InvokeModel request with any Amazon Bedrock or SageMaker model to generate\\nresponses.\\nModel Model ID\\nAmazon Titan Text Premier amazon.titan-text-premier-v1:0\\nAnthropic Claude v2.0 anthropic.claude-v2\\nSupported regions and models 528\\nAmazon Bedrock User Guide\\nModel Model ID\\nAnthropic Claude v2.1 anthropic.claude-v2:1\\nAnthropic Claude 3 Sonnet v1 anthropic.claude-3-sonnet-20240229-v1:0\\nAnthropic Claude 3 Haiku v1 anthropic.claude-3-haiku-20240307-v1:0\\n Anthropic Claude Instant v1 anthropic.claude-instant-v1\\nPrerequisites for Knowledge bases for Amazon Bedrock\\nBefore you can create a knowledge base, you need to fulfill the following prerequisites:\\n1. Prepare your source of data that contains the information that you want to supply to your\\nknowledge base. You can connect to your source of data. See Supported data source connectors.\\n2. (Optional) Set up a vector store of your choice. You can use the AWS Management Console to\\nautomatically create a vector store in Amazon OpenSearch Serverless for you.\\n3. (Optional) Create a custom AWS Identity and Access Management (IAM) service role with the\\nproper permissions by following the instructions at Create a service role for Knowledge bases for\\nAmazon Bedrock. You can skip this prerequisite if you plan to use the AWS Management Console\\nto automatically create a service role for you.\\n4. (Optional) Set up extra security configurations by following the steps at Encryption of\\n knowledge base resources.\\nTopics\\n• Set up a data source connector for your knowledge base\\n• Set up a vector index for your knowledge base in a supported vector store\\nSet up a data source connector for your knowledge base\\nA data source repository contains files or content with information that can be retrieved when\\nyour knowledge base is queried. You must store your documents or content in at least one of the\\nsupported data sources.\\nPrerequisites 529\\nAmazon Bedrock User Guide\\nTo configure a data source connector to connect and crawl your data from your data source\\nrepository, see Supported data source connectors.\\nCreate a knowledge base with your data source configured, then sync your data source.\\nAfter you sync your data source, you can query your knowledge base.\\nTopics\\n• Supported document formats and limits\\n• Metadata and filtering\\n• Source chunks\\nSupported document formats and limits\\nCheck that each source document file conforms to the following requirements:\\n\"},\n",
       " {'question': 'What are the two main methods of model customization in Amazon Bedrock?',\n",
       "  'ground_truth': 'The two main methods of model customization in Amazon Bedrock are Fine-tuning and Continued Pre-training.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"your data. You can also use an existing role or let the console automatically create a role with\\nthe proper permissions.\\n3. (Optional) Configure KMS keys and/or VPC for extra security.\\n4. Create a Fine-tuning or Continued Pre-training job, controlling the training process by adjusting\\nthe hyperparameter values.\\n5. Analyze the results by looking at the training or validation metrics or by using model evaluation.\\n6. Purchase Provisioned Throughput for your newly created custom model.\\n7. Use your custom model as you would a base model in Amazon Bedrock tasks, such as model\\ninference.\\nTopics\\n• Supported regions and models for model customization\\n• Prerequisites for model customization\\n• Submit a model customization job\\n• Manage a model customization job\\n• Analyze the results of a model customization job\\n• Import a model with Custom Model Import\\n• Share a model for another account to use\\n• Copy a model to use in a region\\n• Use a custom model\\n• Code samples for model customization\\n • Guidelines for model customization\\n• Troubleshooting\\nSupported regions and models for model customization\\nThe following table shows regional support for each customization method:\\nRegion Fine-tuning Continued pre-training\\nUS East (N. Virginia) Yes Yes\\nSupported regions and models 906\\nAmazon Bedrock User Guide\\nRegion Fine-tuning Continued pre-training\\nUS West (Oregon) Yes Yes\\nAWS GovCloud (US-West) Yes No\\nNote\\n• Amazon Titan Text Premier : This model is currently only supported in us-east-1 (IAD).\\n• Anthropic Claude 3 Haiku : This model is in preview. To request to be considered for\\naccess to the preview of fine-tuning Anthropic's Claude 3 Haiku in Amazon Bedrock,\\ncontact your AWS account team or submit a support ticket via the AWS Management\\nConsole. To create a support ticket in the AWS Management Console, for the Service,\\nselect Bedrock and for Category, select Model. Regions supported during the preview are\\nsubject to change.\\n The following table shows model support for each customization method:\\nModel name Model ID Fine-tuning Continued pre-train\\ning\\nAmazon Titan Text amazon.titan-text- Yes Yes\\nG1 - Express express-v1\\nAmazon Titan Text amazon.titan-text- Yes Yes\\nG1 - Lite lite-v1\\nAmazon Titan Text amazon.titan-text- Yes (in preview - No\\nPremier premier-v1:0:32k contact AWS to get\\naccess)\\nAmazon Titan Image amazon.titan-image- Yes No\\nGenerator G1 V1 generator-v1\\nAmazon Titan Image amazon.titan-image- Yes No\\nGenerator G1 V2 generator-v2:0\\nSupported regions and models 907\\nAmazon Bedrock User Guide\\nModel name Model ID Fine-tuning Continued pre-train\\ning\\nAmazon Titan amazon.titan-embed- Yes No\\nMultimodal image-v1\\nEmbeddings G1 G1\\nCohere Command cohere.command-tex Yes No\\nt-v14\\nCohere Command cohere.command-lig Yes No\\nLight ht-text-v14\\nMeta Llama 2 13B meta.llama2-13b-ch Yes No\\nat-v1\\nMeta Llama 2 70B meta.llama2-70b-ch Yes No\\nat-v1\\nAnthropic Claude 3 anthropic.claude-3- Yes (in preview - No\\n\"},\n",
       " {'question': 'How does the code handle different response formats from various foundation models when invoking Anthropic Claude 2 through Amazon Bedrock, and what specific steps are taken to process the response?',\n",
       "  'ground_truth': 'The code handles different response formats by using model-specific request and response structures. For Anthropic Claude 2, it defines custom structs (ClaudeRequest and ClaudeResponse) to match the model\\'s expected input and output formats. The process involves several steps:\\n\\n1. It creates a ClaudeRequest struct with the prompt, max tokens, temperature, and stop sequences.\\n2. The request is marshaled into JSON format.\\n3. The InvokeModel function is called with the model ID \"anthropic.claude-v2\" and the JSON body.\\n4. After receiving the response, it unmarshals the output into a ClaudeResponse struct.\\n5. Finally, it extracts the generated text from the Completion field of the response.\\n\\nThis approach allows for flexible handling of different model responses while maintaining type safety and ease of use specific to each foundation model.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'var userMessage = \"Describe the purpose of a \\'hello world\\' program in one line.\";\\n//Format the request payload using the model\\'s native structure.\\nvar nativeRequest = JsonSerializer.Serialize(new\\n{\\nanthropic_version = \"bedrock-2023-05-31\",\\nmax_tokens = 512,\\ntemperature = 0.5,\\nmessages = new[]\\n{\\nnew { role = \"user\", content = userMessage }\\n}\\n});\\n// Create a request with the model ID and the model\\'s native request payload.\\nvar request = new InvokeModelRequest()\\n{\\nModelId = modelId,\\nBody = new MemoryStream(System.Text.Encoding.UTF8.GetBytes(nativeRequest)),\\nContentType = \"application/json\"\\n};\\ntry\\n{\\n// Send the request to the Bedrock Runtime and wait for the response.\\nvar response = await client.InvokeModelAsync(request);\\n// Decode the response body.\\nvar modelResponse = await JsonNode.ParseAsync(response.Body);\\nAnthropic Claude 1328\\nAmazon Bedrock User Guide\\n// Extract and print the response text.\\nvar responseText = modelResponse[\"content\"]?[0]?[\"text\"] ?? \"\";\\n Console.WriteLine(responseText);\\n}\\ncatch (AmazonBedrockRuntimeException e)\\n{\\nConsole.WriteLine($\"ERROR: Can\\'t invoke \\'{modelId}\\'. Reason: {e.Message}\");\\nthrow;\\n}\\n• For API details, see InvokeModel in AWS SDK for .NET API Reference.\\nGo\\nSDK for Go V2\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nInvoke the Anthropic Claude 2 foundation model to generate text.\\n// Each model provider has their own individual request and response formats.\\n// For the format, ranges, and default values for Anthropic Claude, refer to:\\n// https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-\\nclaude.html\\ntype ClaudeRequest struct {\\nPrompt string `json:\"prompt\"`\\nMaxTokensToSample int `json:\"max_tokens_to_sample\"`\\nTemperature float64 `json:\"temperature,omitempty\"`\\nStopSequences []string `json:\"stop_sequences,omitempty\"`\\n}\\ntype ClaudeResponse struct {\\nCompletion string `json:\"completion\"`\\n}\\nAnthropic Claude 1329\\n Amazon Bedrock User Guide\\n// Invokes Anthropic Claude on Amazon Bedrock to run an inference using the input\\n// provided in the request body.\\nfunc (wrapper InvokeModelWrapper) InvokeClaude(prompt string) (string, error) {\\nmodelId := \"anthropic.claude-v2\"\\n// Anthropic Claude requires enclosing the prompt as follows:\\nenclosedPrompt := \"Human: \" + prompt + \"\\\\n\\\\nAssistant:\"\\nbody, err := json.Marshal(ClaudeRequest{\\nPrompt: enclosedPrompt,\\nMaxTokensToSample: 200,\\nTemperature: 0.5,\\nStopSequences: []string{\"\\\\n\\\\nHuman:\"},\\n})\\nif err != nil {\\nlog.Fatal(\"failed to marshal\", err)\\n}\\noutput, err := wrapper.BedrockRuntimeClient.InvokeModel(context.TODO(),\\n&bedrockruntime.InvokeModelInput{\\nModelId: aws.String(modelId),\\nContentType: aws.String(\"application/json\"),\\nBody: body,\\n})\\nif err != nil {\\nProcessError(err, modelId)\\n}\\nvar response ClaudeResponse\\nif err := json.Unmarshal(output.Body, &response); err != nil {\\nlog.Fatal(\"failed to unmarshal\", err)\\n}\\nreturn response.Completion, nil\\n}\\n'},\n",
       " {'question': 'How can you retrieve information about an Amazon Bedrock Agent using Python?',\n",
       "  'ground_truth': 'You can use the get_agent function from the AWS SDK for Python (Boto3). This function takes an agent_id as a parameter and returns information about the specified Amazon Bedrock Agent.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'response = self.client.delete_agent_alias(\\nagentId=agent_id, agentAliasId=agent_alias_id\\n)\\nexcept ClientError as e:\\nlogger.error(f\"Couldn\\'t delete agent alias. {e}\")\\nraise\\nelse:\\nreturn response\\n• For API details, see DeleteAgentAlias in AWS SDK for Python (Boto3) API Reference.\\nFor a complete list of AWS SDK developer guides and code examples, see Using this service with\\nan AWS SDK. This topic also includes information about getting started and details about previous\\nSDK versions.\\nBasics 1526\\nAmazon Bedrock User Guide\\nUse GetAgent with an AWS SDK or CLI\\nThe following code examples show how to use GetAgent.\\nAction examples are code excerpts from larger programs and must be run in context. You can see\\nthis action in context in the following code example:\\n• Create and invoke an agent\\nJavaScript\\nSDK for JavaScript (v3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nGet an agent.\\n // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport { checkForPlaceholders } from \"../lib/utils.js\";\\nimport {\\nBedrockAgentClient,\\nGetAgentCommand,\\n} from \"@aws-sdk/client-bedrock-agent\";\\n/**\\n* Retrieves the details of an Amazon Bedrock Agent.\\n*\\n* @param {string} agentId - The unique identifier of the agent.\\n* @param {string} [region=\\'us-east-1\\'] - The AWS region in use.\\n* @returns {Promise<import(\"@aws-sdk/client-bedrock-agent\").Agent>} An object\\ncontaining the agent details.\\n*/\\nexport const getAgent = async (agentId, region = \"us-east-1\") => {\\nBasics 1527\\nAmazon Bedrock User Guide\\nconst client = new BedrockAgentClient({ region });\\nconst command = new GetAgentCommand({ agentId });\\nconst response = await client.send(command);\\nreturn response.agent;\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\n // Replace the placeholders for agentId with an existing agent\\'s id.\\n// Ensure to remove the brackets \\'[]\\' before adding your data.\\n// The agentId must be an alphanumeric string with exactly 10 characters.\\nconst agentId = \"[ABC123DE45]\";\\n// Check for unresolved placeholders in agentId.\\ncheckForPlaceholders([agentId]);\\nconsole.log(`Retrieving agent with ID ${agentId}...`);\\nconst agent = await getAgent(agentId);\\nconsole.log(agent);\\n}\\n• For API details, see GetAgent in AWS SDK for JavaScript API Reference.\\nPython\\nSDK for Python (Boto3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nGet an agent.\\ndef get_agent(self, agent_id, log_error=True):\\n\"\"\"\\nGets information about an agent.\\nBasics 1528\\nAmazon Bedrock User Guide\\n:param agent_id: The unique identifier of the agent.\\n:param log_error: Whether to log any errors that occur when getting the\\nagent.\\nIf True, errors will be logged to the logger. If False,\\nerrors\\n'},\n",
       " {'question': 'How does the AWS SDK for .NET handle potential errors when invoking Meta Llama 2 on Amazon Bedrock, and what specific information does it provide in case of a failure?',\n",
       "  'ground_truth': 'The AWS SDK for .NET handles potential errors when invoking Meta Llama 2 on Amazon Bedrock by using a try-catch block. If an error occurs during the invocation, it catches the AmazonBedrockRuntimeException. In case of a failure, it provides specific information including the model ID that failed to invoke and the reason for the failure. The error message is printed to the console, showing \"ERROR: Can\\'t invoke \\'[modelId]\\'. Reason: [e.Message]\", where [modelId] is the ID of the model being invoked (in this case, \"meta.llama2-13b-chat-v1\"), and [e.Message] contains the specific error message from the exception. After logging this information, the exception is re-thrown to allow for further error handling up the call stack.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'an AWS SDK. This topic also includes information about getting started and details about previous\\nSDK versions.\\nInvoke Meta Llama 2 on Amazon Bedrock using the Invoke Model API\\nThe following code examples show how to send a text message to Meta Llama 2, using the Invoke\\nModel API.\\n.NET\\nAWS SDK for .NET\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Use the native inference API to send a text message to Meta Llama 2.\\nusing System;\\nusing System.IO;\\nusing System.Text.Json;\\nusing System.Text.Json.Nodes;\\nusing Amazon;\\nusing Amazon.BedrockRuntime;\\nusing Amazon.BedrockRuntime.Model;\\n// Create a Bedrock Runtime client in the AWS Region you want to use.\\nvar client = new AmazonBedrockRuntimeClient(RegionEndpoint.USEast1);\\n// Set the model ID, e.g., Llama 2 Chat 13B.\\nvar modelId = \"meta.llama2-13b-chat-v1\";\\n// Define the prompt for the model.\\n var prompt = \"Describe the purpose of a \\'hello world\\' program in one line.\";\\nMeta Llama 1437\\nAmazon Bedrock User Guide\\n// Embed the prompt in Llama 2\\'s instruction format.\\nvar formattedPrompt = $\"<s>[INST] {prompt} [/INST]\";\\n//Format the request payload using the model\\'s native structure.\\nvar nativeRequest = JsonSerializer.Serialize(new\\n{\\nprompt = formattedPrompt,\\nmax_gen_len = 512,\\ntemperature = 0.5\\n});\\n// Create a request with the model ID and the model\\'s native request payload.\\nvar request = new InvokeModelRequest()\\n{\\nModelId = modelId,\\nBody = new MemoryStream(System.Text.Encoding.UTF8.GetBytes(nativeRequest)),\\nContentType = \"application/json\"\\n};\\ntry\\n{\\n// Send the request to the Bedrock Runtime and wait for the response.\\nvar response = await client.InvokeModelAsync(request);\\n// Decode the response body.\\nvar modelResponse = await JsonNode.ParseAsync(response.Body);\\n// Extract and print the response text.\\nvar responseText = modelResponse[\"generation\"] ?? \"\";\\n Console.WriteLine(responseText);\\n}\\ncatch (AmazonBedrockRuntimeException e)\\n{\\nConsole.WriteLine($\"ERROR: Can\\'t invoke \\'{modelId}\\'. Reason: {e.Message}\");\\nthrow;\\n}\\n• For API details, see InvokeModel in AWS SDK for .NET API Reference.\\nMeta Llama 1438\\nAmazon Bedrock User Guide\\nGo\\nSDK for Go V2\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Each model provider has their own individual request and response formats.\\n// For the format, ranges, and default values for Meta Llama 2 Chat, refer to:\\n// https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-\\nmeta.html\\ntype Llama2Request struct {\\nPrompt string `json:\"prompt\"`\\nMaxGenLength int `json:\"max_gen_len,omitempty\"`\\nTemperature float64 `json:\"temperature,omitempty\"`\\n}\\ntype Llama2Response struct {\\nGeneration string `json:\"generation\"`\\n}\\n'},\n",
       " {'question': 'What tool can validate IAM policies for security and functionality?',\n",
       "  'ground_truth': 'IAM Access Analyzer can validate IAM policies to ensure secure and functional permissions. It provides more than 100 policy checks and actionable recommendations to help author secure and functional policies.',\n",
       "  'question_type': 'simple',\n",
       "  'context': '• Use conditions in IAM policies to further restrict access – You can add a condition to your\\npolicies to limit access to actions and resources. For example, you can write a policy condition to\\nspecify that all requests must be sent using SSL. You can also use conditions to grant access to\\nservice actions if they are used through a specific AWS service, such as AWS CloudFormation. For\\nmore information, see IAM JSON policy elements: Condition in the IAM User Guide.\\n• Use IAM Access Analyzer to validate your IAM policies to ensure secure and functional\\npermissions – IAM Access Analyzer validates new and existing policies so that the policies\\nadhere to the IAM policy language (JSON) and IAM best practices. IAM Access Analyzer provides\\nmore than 100 policy checks and actionable recommendations to help you author secure and\\nfunctional policies. For more information, see IAM Access Analyzer policy validation in the IAM\\nUser Guide.\\n • Require multi-factor authentication (MFA) – If you have a scenario that requires IAM users\\nor a root user in your AWS account, turn on MFA for additional security. To require MFA when\\nAPI operations are called, add MFA conditions to your policies. For more information, see\\nConfiguring MFA-protected API access in the IAM User Guide.\\nFor more information about best practices in IAM, see Security best practices in IAM in the IAM User\\nGuide.\\nUse the Amazon Bedrock console\\nTo access the Amazon Bedrock console, you must have a minimum set of permissions. These\\npermissions must allow you to list and view details about the Amazon Bedrock resources in your\\nAWS account. If you create an identity-based policy that is more restrictive than the minimum\\nrequired permissions, the console won\\'t function as intended for entities (users or roles) with that\\npolicy.\\nIdentity-based policy examples 1076\\nAmazon Bedrock User Guide\\n You don\\'t need to allow minimum console permissions for users that are making calls only to the\\nAWS CLI or the AWS API. Instead, allow access to only the actions that match the API operation\\nthat they\\'re trying to perform.\\nTo ensure that users and roles can still use the Amazon Bedrock console, also attach the Amazon\\nBedrock AmazonBedrockFullAccess or AmazonBedrockReadOnly AWS managed policy to the\\nentities. For more information, see Adding permissions to a user in the IAM User Guide.\\nAllow users to view their own permissions\\nThis example shows how you might create a policy that allows IAM users to view the inline and\\nmanaged policies that are attached to their user identity. This policy includes permissions to\\ncomplete this action on the console or programmatically using the AWS CLI or AWS API.\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"ViewOwnUserInfo\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"iam:GetUserPolicy\",\\n\"iam:ListGroupsForUser\",\\n\"iam:ListAttachedUserPolicies\",\\n'},\n",
       " {'question': 'How does versioning a guardrail in Amazon Bedrock differ from deleting it, and what precautions should be taken when performing either action?',\n",
       "  'ground_truth': \"Versioning a guardrail in Amazon Bedrock creates a snapshot of the guardrail's configuration at a specific point in time, allowing you to iterate on the working draft while maintaining stable versions for production use. You can create multiple versions, compare their performance, and easily switch between them in your application. In contrast, deleting a guardrail permanently removes it and all its versions. When deleting a guardrail, you must first disassociate it from all resources and applications to avoid potential errors. It's important to note that while guardrail versions don't have individual ARNs and aren't considered separate resources, deleting the main guardrail will delete all its versions. Therefore, versioning provides flexibility and safety in development, while deletion requires careful consideration of dependencies and is irreversible.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': '\"guardrailArn\": \"string\",\\n\"guardrailId\": \"string\",\\n\"updatedAt\": \"string\",\\n\"version\": \"string\"\\n}\\nEdit a guardrail 402\\nAmazon Bedrock User Guide\\nDelete a guardrail\\nYou can delete a guardrail when you no longer need to use it. Be sure to disassociate the guardrail\\nfrom all the resources or applications that use it before you delete the guardrail in order to avoid\\npotential errors.\\nConsole\\nTo delete a guardrail\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\\nbedrock/.\\n2. Choose Guardrails from the left navigation pane. Then, select a guardrail in the Guardrails\\nsection.\\n3. In the Guardrails section, select a guardrail that you want to delete and then choose\\nDelete.\\n4. Enter delete in the user input field and choose Delete to delete the guardrail.\\nAPI\\nTo delete a guardrail, send a DeleteGuardrail request and only specify the ARN of the guardrail\\n in the guardrailIdentifier field. Don\\'t specify the guardrailVersion\\nThe following is the request format:\\nDELETE /guardrails/guardrailIdentifier?guardrailVersion=guardrailVersion HTTP/1.1\\nWarning\\nIf you delete a guardrail, all of its versions will be deleted.\\nIf the deletion is successful, the response returns an HTTP 200 status code.\\nDelete a guardrail 403\\nAmazon Bedrock User Guide\\nDeploy a guardrail\\nWhen you\\'re ready to deploy your guardrail to production, you create a version of it and invoke\\nthe version of the guardrail in your application. A version is a snapshot of your guardrail that\\nyou create at a point in time when you are iterating on the working draft of the guardrail. Create\\nversions of your guardrail when you are satisfied with a set of configurations. You can use the\\ntest window (for more information, see Test a guardrail) to compare how different versions of\\nyour guardrail perform in in evaluating the input prompts and model responses and generating\\n controlled responses for the final output. Versions allow you to easily switch between different\\nconfigurations for your guardrail and update your application with the most appropriate version\\nfor your use-case.\\nTopics\\n• Create and manage a version of a guardrail\\nCreate and manage a version of a guardrail\\nThe following topics discuss how to create a version of your guardrail when it\\'s ready for\\ndeployment, view information about it, and delete it when you no longer need it.\\nNote\\nGuardrail versions aren\\'t considered resources and therefore do not have an ARN. IAM\\nPolicies that apply to a guardrail apply to all of its versions.\\nTopics\\n• Create a version of a guardrail\\n• View information about guardrail versions\\n• Delete a version of a guardrail\\nCreate a version of a guardrail\\nTo learn how to create a version of a guardrail, select the tab corresponding to your method of\\nchoice and follow the steps.\\nDeploy a guardrail 404\\nAmazon Bedrock User Guide\\nConsole\\nTo create a version\\n'},\n",
       " {'question': 'How long might it take for new data to be available after ingestion?',\n",
       "  'ground_truth': 'It could take a few minutes for the vector embeddings of newly ingested data to be available in the vector store, especially if using Amazon OpenSearch Serverless.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"4. When data ingestion completes, a green success banner appears if it is successful.\\nNote\\nAfter data ingestion completes, it could take few minutes for the vector\\nembeddings of the newly ingested data to be available in the vector store if you use\\nAmazon OpenSearch Serverless.\\nSync your data source 601\\nAmazon Bedrock User Guide\\n5. You can choose a data source to view its Sync history. Select View warnings to see why a\\ndata ingestion job failed.\\nAPI\\nTo ingest a data source into the vector store you configured for your knowledge base, send a\\nStartIngestionJob request with a Agents for Amazon Bedrock build-time endpoint. Specify the\\nknowledgeBaseId and dataSourceId.\\nUse the ingestionJobId returned in the response in a GetIngestionJob request with a Agents\\nfor Amazon Bedrock build-time endpoint to track the status of the ingestion job. In addition,\\nspecify the knowledgeBaseId and dataSourceId.\\n• When the ingestion job finishes, the status in the response is COMPLETE.\\nNote\\n After data syncing completes, it could take a few minutes for the vector embeddings\\nof the newly synced data to reflect in your knowledge base if you use Amazon\\nOpenSearch Serverless.\\n• The statistics object in the response returns information about whether ingestion was\\nsuccessful or not for documents in the data source.\\nYou can also see information for all ingestion jobs for a data source by sending a\\nListIngestionJobs request with a Agents for Amazon Bedrock build-time endpoint. Specify the\\ndataSourceId and the knowledgeBaseId of the knowledge base that the data is being\\ningested to.\\n• Filter for results by specifying a status to search for in the filters object.\\n• Sort by the time that the job was started or the status of a job by specifying the sortBy\\nobject. You can sort in ascending or descending order.\\n• Set the maximum number of results to return in a response in the maxResults field. If there\\n are more results than the number you set, the response returns a nextToken that you can\\nsend in another ListIngestionJobs request to see the next batch of jobs.\\nSync your data source 602\\nAmazon Bedrock User Guide\\nTest a knowledge base in Amazon Bedrock\\nAfter you set up your knowledge base, you can test its behavior by sending queries and seeing the\\nresponses. You can also set query configurations to customize information retrieval. When you are\\nsatisfied with your knowledge base's behavior, you can then set up your application to query the\\nknowledge base or attach the knowledge base to an agent.\\nSelect a topic to learn more about it.\\nTopics\\n• Query the knowledge base and return results or generate responses\\n• Query configurations\\nQuery the knowledge base and return results or generate responses\\nTo learn how to query your knowledge base, select the tab corresponding to your method of choice\\nand follow the steps.\\nConsole\\nTo test your knowledge base\\n\"},\n",
       " {'question': 'How does the invokeModel function handle different model configurations, and what are the key parameters that can be customized when invoking the Amazon Titan Text generation model?',\n",
       "  'ground_truth': 'The invokeModel function in the provided JavaScript code allows for flexible configuration of the Amazon Titan Text generation model. It accepts two parameters: \\'prompt\\' (required) and \\'modelId\\' (optional, defaulting to \"amazon.titan-text-express-v1\"). The function prepares a payload object that includes the input text and textGenerationConfig. Key customizable parameters in the textGenerationConfig include:\\n\\n1. maxTokenCount: Set to 4096, determining the maximum number of tokens in the generated output.\\n2. stopSequences: An empty array, which can be populated to specify sequences that stop text generation.\\n3. temperature: Set to 0, controlling the randomness of the output (lower values make it more deterministic).\\n4. topP: Set to 1, influencing the diversity of the generated text.\\n\\nThese parameters allow users to fine-tune the model\\'s behavior for different use cases. The function then uses the AWS SDK to send an InvokeModelCommand with the prepared payload, processes the response, and returns the generated text.',\n",
       "  'question_type': 'complex',\n",
       "  'context': '} catch (SdkClientException e) {\\nSystem.err.printf(\"ERROR: Can\\'t invoke \\'%s\\'. Reason: %s\", modelId,\\ne.getMessage());\\nthrow new RuntimeException(e);\\n}\\n}\\npublic static void main(String[] args) {\\ninvokeModel();\\n}\\n}\\n• For API details, see InvokeModel in AWS SDK for Java 2.x API Reference.\\nAmazon Titan Text 1290\\nAmazon Bedrock User Guide\\nJavaScript\\nSDK for JavaScript (v3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport { FoundationModels } from \"../../config/foundation_models.js\";\\nimport {\\nBedrockRuntimeClient,\\nInvokeModelCommand,\\n} from \"@aws-sdk/client-bedrock-runtime\";\\n/**\\n* @typedef {Object} ResponseBody\\n* @property {Object[]} results\\n*/\\n/**\\n* Invokes an Amazon Titan Text generation model.\\n*\\n * @param {string} prompt - The input text prompt for the model to complete.\\n* @param {string} [modelId] - The ID of the model to use. Defaults to\\n\"amazon.titan-text-express-v1\".\\n*/\\nexport const invokeModel = async (\\nprompt,\\nmodelId = \"amazon.titan-text-express-v1\",\\n) => {\\n// Create a new Bedrock Runtime client instance.\\nconst client = new BedrockRuntimeClient({ region: \"us-east-1\" });\\n// Prepare the payload for the model.\\nAmazon Titan Text 1291\\nAmazon Bedrock User Guide\\nconst payload = {\\ninputText: prompt,\\ntextGenerationConfig: {\\nmaxTokenCount: 4096,\\nstopSequences: [],\\ntemperature: 0,\\ntopP: 1,\\n},\\n};\\n// Invoke the model with the payload and wait for the response.\\nconst command = new InvokeModelCommand({\\ncontentType: \"application/json\",\\nbody: JSON.stringify(payload),\\nmodelId,\\n});\\nconst apiResponse = await client.send(command);\\n// Decode and return the response.\\nconst decodedResponseBody = new TextDecoder().decode(apiResponse.body);\\n/** @type {ResponseBody} */\\n const responseBody = JSON.parse(decodedResponseBody);\\nreturn responseBody.results[0].outputText;\\n};\\n// Invoke the function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nconst prompt =\\n\\'Complete the following in one sentence: \"Once upon a time...\"\\';\\nconst modelId = FoundationModels.TITAN_TEXT_G1_EXPRESS.modelId;\\nconsole.log(`Prompt: ${prompt}`);\\nconsole.log(`Model ID: ${modelId}`);\\ntry {\\nconsole.log(\"-\".repeat(53));\\nconst response = await invokeModel(prompt, modelId);\\nconsole.log(response);\\n} catch (err) {\\nconsole.log(err);\\n}\\n}\\n• For API details, see InvokeModel in AWS SDK for JavaScript API Reference.\\nAmazon Titan Text 1292\\nAmazon Bedrock User Guide\\nPython\\nSDK for Python (Boto3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n# Use the native inference API to send a text message to Amazon Titan Text.\\nimport boto3\\n'},\n",
       " {'question': 'How can you list the available Amazon Bedrock foundation models?',\n",
       "  'ground_truth': 'You can list the available Amazon Bedrock foundation models using the ListFoundationModels API call, which is available in various AWS SDKs such as PHP, Python (Boto3), and Kotlin.',\n",
       "  'question_type': 'simple',\n",
       "  'context': '• For API details, see ListFoundationModels in AWS SDK for Kotlin API reference.\\nPHP\\nSDK for PHP\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nList the available Amazon Bedrock foundation models.\\npublic function listFoundationModels()\\n{\\n$result = $this->bedrockClient->listFoundationModels();\\nreturn $result;\\n}\\n• For API details, see ListFoundationModels in AWS SDK for PHP API Reference.\\nPython\\nSDK for Python (Boto3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nList the available Amazon Bedrock foundation models.\\ndef list_foundation_models(self):\\nBasics 1215\\nAmazon Bedrock User Guide\\n\"\"\"\\nList the available Amazon Bedrock foundation models.\\n:return: The list of available bedrock foundation models.\\n\"\"\"\\ntry:\\nresponse = self.bedrock_client.list_foundation_models()\\nmodels = response[\"modelSummaries\"]\\n logger.info(\"Got %s foundation models.\", len(models))\\nreturn models\\nexcept ClientError:\\nlogger.error(\"Couldn\\'t list foundation models.\")\\nraise\\n• For API details, see ListFoundationModels in AWS SDK for Python (Boto3) API Reference.\\nFor a complete list of AWS SDK developer guides and code examples, see Using this service with\\nan AWS SDK. This topic also includes information about getting started and details about previous\\nSDK versions.\\nScenarios for Amazon Bedrock using AWS SDKs\\nThe following code examples show you how to implement common scenarios in Amazon Bedrock\\nwith AWS SDKs. These scenarios show you how to accomplish specific tasks by calling multiple\\nfunctions within Amazon Bedrock or combined with other AWS services. Each scenario includes\\na link to the complete source code, where you can find instructions on how to set up and run the\\ncode.\\nScenarios target an intermediate level of experience to help you understand service actions in\\ncontext.\\nExamples\\n • Build and orchestrate generative AI applications with Amazon Bedrock and Step Functions\\nScenarios 1216\\nAmazon Bedrock User Guide\\nBuild and orchestrate generative AI applications with Amazon Bedrock and Step\\nFunctions\\nThe following code example shows how to build and orchestrate generative AI applications with\\nAmazon Bedrock and Step Functions.\\nPython\\nSDK for Python (Boto3)\\nThe Amazon Bedrock Serverless Prompt Chaining scenario demonstrates how AWS Step\\nFunctions, Amazon Bedrock, and Agents for Amazon Bedrock can be used to build and\\norchestrate complex, serverless, and highly scalable generative AI applications. It contains\\nthe following working examples:\\n• Write an analysis of a given novel for a literature blog. This example illustrates a simple,\\nsequential chain of prompts.\\n• Generate a short story about a given topic. This example illustrates how the AI can\\niteratively process a list of items that it previously generated.\\n'},\n",
       " {'question': 'In the Amazon Bedrock Prompt flow builder, how does the configuration of the MakePlaylist node enable dynamic playlist generation, and what steps ensure the flow can handle different user inputs?',\n",
       "  'ground_truth': 'The MakePlaylist node in the Amazon Bedrock Prompt flow builder enables dynamic playlist generation through several key configurations:\\n\\n1. It uses a parameterized prompt: \"Make me a {{genre}} playlist consisting of the following number of songs: {{number}}.\" This allows for flexible input.\\n\\n2. Two inputs are defined: \\'genre\\' (String type) and \\'number\\' (Number type), both mapped to a JSON object structure ($.data.genre and $.data.number respectively).\\n\\n3. The Flow input node is set to expect a JSON object, allowing structured user input.\\n\\n4. Connections are established from the Flow input to both \\'genre\\' and \\'number\\' inputs in the MakePlaylist node, ensuring data flow.\\n\\n5. The output of the MakePlaylist node is connected to the Flow output node.\\n\\nTo handle different user inputs, the flow is tested with a JSON object: {\"genre\": \"pop\", \"number\": 3}. This structure allows users to specify any genre and number of songs, making the flow adaptable to various playlist requests. The use of a model for inference on the prompt further enhances the system\\'s ability to generate diverse playlists based on user input.',\n",
       "  'question_type': 'complex',\n",
       "  'context': '1. Follow the steps under To create a flow in the Console tab at Create a flow in Amazon\\nBedrock. Enter the Prompt flow builder.\\n2. Set up the prompt node by doing the following:\\na. From the Prompt flow builder left pane, select the Nodes tab.\\nb. Drag a Prompt node into your flow in the center pane.\\nc. Select the Configure tab in the Prompt flow builder pane.\\nd. Enter MakePlaylist as the Node name.\\ne. Choose Define in node.\\nf. Set up the following configurations for the prompt:\\ni. Under Select model, select a model to run inference on the prompt.\\nii. In the Message text box, enter Make me a {{genre}} playlist consisting\\nof the following number of songs: {{number}}.. This creates two\\nvariables that will appear as inputs into the node.\\niii. (Optional) Modify the Inference configurations.\\ng. Expand the Inputs section. The names for the inputs are prefilled by the variables in the\\nprompt message. Configure the inputs as follows:\\nExample prompt flows 855\\nAmazon Bedrock User Guide\\n Name Type Expression\\ngenre String $.data.genre\\nnumber Number $.data.number\\nThis configuration means that the prompt node expects a JSON object containing a field\\ncalled genre that will be mapped to the genre input and a field called number that will\\nbe mapped to the number input.\\nh. You can\\'t modify the Output. It will be the response from the model, returned as a string.\\n3. Choose the Flow input node and select the Configure tab. Select Object as the Type. This\\nmeans that flow invocation will expect to receive a JSON object.\\n4. Connect your nodes to complete the flow by doing the following:\\na. Drag a connection from the output node of the Flow input node to the genre input in the\\nMakePlaylist prompt node.\\nb. Drag a connection from the output node of the Flow input node to the number input in\\nthe MakePlaylist prompt node.\\nc. Drag a connection from the output node of the modelCompletion output in the\\nMakePlaylist prompt node to the document input in the Flow output node.\\n 5. Choose Save to save your flow. Your flow should now be prepared for testing.\\n6. Test your flow by entering the following JSON object is the Test prompt flow pane on the\\nright. Choose Run and the flow should return a model response.\\n{\\n\"genre\": \"pop\",\\n\"number\": 3\\n}\\nCreate a flow with a condition node\\nThe following image shows a flow with one condition node returns one of three possible values\\nbased on the condition that is fulfilled:\\nExample prompt flows 856\\nAmazon Bedrock User Guide\\nTo build and test this flow in the console:\\n1. Follow the steps under To create a flow in the Console tab at Create a flow in Amazon\\nBedrock. Enter the Prompt flow builder.\\n2. Set up the condition node by doing the following:\\na. From the Prompt flow builder left pane, select the Nodes tab.\\nb. Drag a Condition node into your flow in the center pane.\\nc. Select the Configure tab in the Prompt flow builder pane.\\nd. Expand the Inputs section. Configure the inputs as follows:\\nName Type Expression\\n'},\n",
       " {'question': 'How do you create an alias for a prompt flow in Amazon Bedrock?',\n",
       "  'ground_truth': 'To create an alias for a prompt flow in Amazon Bedrock, sign in to the AWS Management Console, open the Amazon Bedrock console, select Prompt flows, choose a flow, click Create alias, enter a unique name and optional description, choose to create a new version or use an existing one, and select Create alias.',\n",
       "  'question_type': 'simple',\n",
       "  'context': '1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at Getting Started with the AWS\\nManagement Console.\\n2. Select Prompt flows from the left navigation pane. Then, choose a prompt flow in the\\nFlows section.\\n3. In the Aliases section, choose Create alias.\\n4. Enter a unique name for the alias and provide an optional description.\\nDeploy a prompt flow 871\\nAmazon Bedrock User Guide\\n5. Choose one of the following options:\\n• To create a new version, choose Create a new version and to associate it to this alias.\\n• To use an existing version, choose Use an existing version to associate this alias. From\\nthe dropdown menu, choose the version that you want to associate the alias to.\\n6. Select Create alias. A success banner appears at the top.\\nAPI\\nTo create a version of your prompt flow, send a CreateFlowVersion request (see link for\\n request and response formats and field details) with an Agents for Amazon Bedrock build-time\\nendpoint and specify the ARN or ID of the prompt flow as the flowIdentifier.\\nThe response returns an ID and ARN for the version. Versions are created incrementally, starting\\nfrom 1.\\nTo create an alias to point to a version of your prompt flow, send a CreateFlowAlias request (see\\nlink for request and response formats and field details) with an Agents for Amazon Bedrock\\nbuild-time endpoint.\\nThe following fields are required:\\nField Basic description\\nflowIdentifier The ARN or ID of the prompt flow for which\\nto create an alias.\\nname A name for the alias.\\nroutingConfiguration Specify the version to map the alias to in the\\nflowVersion field.\\nThe following fields are optional:\\nField Use-case\\ndescription To provide a description for the alias.\\nclientToken To prevent reduplication of the request.\\nDeploy a prompt flow 872\\nAmazon Bedrock User Guide\\n To learn how to manage versions and aliases of prompt flows, select from the following topics.\\nTopics\\n• Manage versions of prompt flows in Amazon Bedrock\\n• Manage aliases of prompt flows in Amazon Bedrock\\nManage versions of prompt flows in Amazon Bedrock\\nAfter you create a version of your prompt flow, you can view information about it or delete it.\\nTopics\\n• View information about versions of prompt flows in Amazon Bedrock\\n• Delete a version of a prompt flow in Amazon Bedrock\\nView information about versions of prompt flows in Amazon Bedrock\\nTo learn how to view information about the versions of a prompt flow, select the tab corresponding\\nto your method of choice and follow the steps.\\nConsole\\nTo view information about a version of a prompt flow\\n1. Open the AWS Management Console and sign in to your account. Navigate to Amazon\\nBedrock.\\n2. Select Flows from the left navigation pane. Then, in the Flows section, select a prompt\\nflow you want to view.\\n'},\n",
       " {'question': 'What are the key differences in the setup process for Pinecone and Redis Enterprise Cloud as vector stores for Amazon Bedrock, and how do their security configurations compare?',\n",
       "  'ground_truth': \"The setup processes for Pinecone and Redis Enterprise Cloud as vector stores for Amazon Bedrock have several key differences:\\n\\n1. API Key vs. Username/Password: Pinecone requires an API key, while Redis Enterprise Cloud uses a username and password combination.\\n\\n2. Secret Configuration: For Pinecone, you create a secret with a single key-value pair (apiKey), whereas Redis Enterprise Cloud requires multiple keys in the secret (username and potentially others).\\n\\n3. TLS Requirement: Redis Enterprise Cloud explicitly requires enabling TLS, which is not mentioned for Pinecone.\\n\\n4. Vector Index Setup: Redis Enterprise Cloud requires specifying a vector index name, vector field, and text field, which are not mentioned in the Pinecone setup.\\n\\n5. Dimensions Specification: The Redis setup includes a table of vector dimensions for different models, which is not provided for Pinecone.\\n\\nSecurity configurations also differ:\\n- Pinecone focuses on API key management and KMS key permissions.\\n- Redis Enterprise Cloud emphasizes TLS enablement and more detailed database access credentials.\\n\\nBoth services use AWS Secrets Manager for secure credential storage, but the content and structure of the secrets differ based on each service's authentication requirements.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': \"To access your Pinecone index, you must provide your Pinecone API key to Amazon Bedrock\\nthrough the AWS Secrets Manager.\\nTo set up a secret for your Pinecone configuration\\n1. Follow the steps at Create an AWS Secrets Manager secret, setting the key as apiKey and\\nthe value as the API key to access your Pinecone index.\\n2. To find your API key, open your Pinecone console and select API Keys.\\n3. After you create the secret, take note of the ARN of the KMS key.\\n4. Attach permissions to your service role to decrypt the ARN of the KMS key by following\\nthe steps in Permissions to decrypt an AWS Secrets Manager secret for the vector store\\ncontaining your knowledge base.\\nSet up a vector index 541\\nAmazon Bedrock User Guide\\n5. Later, when you create your knowledge base, enter the ARN in the Credentials secret ARN\\nfield.\\nRedis Enterprise Cloud\\nNote\\nIf you use Redis Enterprise Cloud, you agree to authorize AWS to access the designated\\n third-party source on your behalf in order to provide vector store services to you. You're\\nresponsible for complying with any third-party terms applicable to use and transfer of\\ndata from the third-party service.\\nFor detailed documentation on setting up a vector store in Redis Enterprise Cloud, see\\nIntegrating Redis Enterprise Cloud with Amazon Bedrock.\\nWhile you set up the vector store, take note of the following information, which you will fill out\\nwhen you create a knowledge base:\\n• Endpoint URL – The public endpoint URL for your database.\\n• Vector index name – The name of the vector index for your database.\\n• Vector field – The name of the field where the vector embeddings will be stored. Refer to the\\nfollowing table to determine how many dimensions the vector should contain.\\nModel Dimensions\\nTitan G1 Embeddings - Text 1,536\\nTitan V2 Embeddings - Text 1,024\\nCohere Embed English 1,024\\nCohere Embed Multilingual 1,024\\n • Text field – The name of the field where the Amazon Bedrock stores the chunks of raw text.\\n• Bedrock-managed metadata field – The name of the field where Amazon Bedrock stores\\nmetadata related to your knowledge base.\\nSet up a vector index 542\\nAmazon Bedrock User Guide\\nTo access your Redis Enterprise Cloud cluster, you must provide your Redis Enterprise Cloud\\nsecurity configuration to Amazon Bedrock through the AWS Secrets Manager.\\nTo set up a secret for your Redis Enterprise Cloud configuration\\n1. Enable TLS to use your database with Amazon Bedrock by following the steps at Transport\\nLayer Security (TLS).\\n2. Follow the steps at Create an AWS Secrets Manager secret. Set up the following keys with\\nthe appropriate values from your Redis Enterprise Cloud configuration in the secret:\\n• username – The username to access your Redis Enterprise Cloud database. To find your\\nusername, look under the Security section of your database in the Redis Console.\\n\"},\n",
       " {'question': 'How can you list available foundation models in Amazon Bedrock using Go?',\n",
       "  'ground_truth': 'You can list available foundation models in Amazon Bedrock using Go by creating a Bedrock client and calling the ListFoundationModels method. The code example shows how to create a Bedrock client using the AWS SDK for Go v2, and then use the ListFoundationModels method to retrieve and print the list of available foundation models.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'in the AWS Code Examples Repository.\\npackage main\\nimport (\\n\"context\"\\n\"fmt\"\\n\"github.com/aws/aws-sdk-go-v2/config\"\\n\"github.com/aws/aws-sdk-go-v2/service/bedrock\"\\n)\\nconst region = \"us-east-1\"\\n// main uses the AWS SDK for Go (v2) to create an Amazon Bedrock client and\\n// list the available foundation models in your account and the chosen region.\\n// This example uses the default settings specified in your shared credentials\\n// and config files.\\nfunc main() {\\nsdkConfig, err := config.LoadDefaultConfig(context.TODO(),\\nconfig.WithRegion(region))\\nif err != nil {\\nfmt.Println(\"Couldn\\'t load default configuration. Have you set up your\\nAWS account?\")\\nfmt.Println(err)\\nreturn\\nAmazon Bedrock 1194\\nAmazon Bedrock User Guide\\n}\\nbedrockClient := bedrock.NewFromConfig(sdkConfig)\\nresult, err := bedrockClient.ListFoundationModels(context.TODO(),\\n&bedrock.ListFoundationModelsInput{})\\nif err != nil {\\nfmt.Printf(\"Couldn\\'t list foundation models. Here\\'s why: %v\\\\n\", err)\\nreturn\\n}\\n if len(result.ModelSummaries) == 0 {\\nfmt.Println(\"There are no foundation models.\")}\\nfor _, modelSummary := range result.ModelSummaries {\\nfmt.Println(*modelSummary.ModelId)\\n}\\n}\\n• For API details, see ListFoundationModels in AWS SDK for Go API Reference.\\nJavaScript\\nSDK for JavaScript (v3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport {\\nBedrockClient,\\nListFoundationModelsCommand,\\n} from \"@aws-sdk/client-bedrock\";\\nconst REGION = \"us-east-1\";\\nconst client = new BedrockClient({ region: REGION });\\nAmazon Bedrock 1195\\nAmazon Bedrock User Guide\\nexport const main = async () => {\\nconst command = new ListFoundationModelsCommand({});\\nconst response = await client.send(command);\\nconst models = response.modelSummaries;\\n console.log(\"Listing the available Bedrock foundation models:\");\\nfor (let model of models) {\\nconsole.log(\"=\".repeat(42));\\nconsole.log(` Model: ${model.modelId}`);\\nconsole.log(\"-\".repeat(42));\\nconsole.log(` Name: ${model.modelName}`);\\nconsole.log(` Provider: ${model.providerName}`);\\nconsole.log(` Model ARN: ${model.modelArn}`);\\nconsole.log(` Input modalities: ${model.inputModalities}`);\\nconsole.log(` Output modalities: ${model.outputModalities}`);\\nconsole.log(` Supported customizations: ${model.customizationsSupported}`);\\nconsole.log(` Supported inference types: ${model.inferenceTypesSupported}`);\\nconsole.log(` Lifecycle status: ${model.modelLifecycle.status}`);\\nconsole.log(\"=\".repeat(42) + \"\\\\n\");\\n}\\nconst active = models.filter(\\n(m) => m.modelLifecycle.status === \"ACTIVE\",\\n).length;\\nconst legacy = models.filter(\\n(m) => m.modelLifecycle.status === \"LEGACY\",\\n).length;\\nconsole.log(\\n`There are ${active} active and ${legacy} legacy foundation models in\\n${REGION}.`,\\n);\\nreturn response;\\n};\\n'},\n",
       " {'question': \"How does Amazon Bedrock's versioning and aliasing system for prompt flows facilitate efficient deployment and management of different iterations?\",\n",
       "  'ground_truth': \"Amazon Bedrock's versioning and aliasing system for prompt flows facilitates efficient deployment and management by allowing users to create immutable snapshots (versions) of their prompt flows and use aliases to point to specific versions. When a prompt flow is created, it starts with a working draft (DRAFT) and a test alias (TSTALIASID). Users can iterate on the working draft and create numbered versions (starting from 1) when satisfied. Aliases can be created to point to specific versions, enabling quick switching between different iterations without tracking version numbers. This system allows for easy rollbacks by changing an alias to point to a previous version, and supports smooth updates to production by creating new versions from the working draft and updating the alias accordingly. This approach provides flexibility in managing different stages of prompt flow development and deployment.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': 'Provide the input in the document field, provide a name for the input in the nodeName field,\\nand provide a name for the input in the nodeOutputName field.\\nThe response is returned in a stream. Each event returned contains output from a node in the\\ndocument field, the node that was processed in the nodeName field, and the type of node in\\nthe nodeType field. These events are of the following format:\\n{\\n\"flowOutputEvent\": {\\n\"content\": {\\n\"document\": \"JSON-formatted string\"\\n},\\n\"nodeName\": \"string\",\\n\"nodeType\": \"string\"\\n}\\n}\\nIf the prompt flow finishes, a flowCompletionEvent field with the completionReason is\\nalso returned. If there\\'s an error, the corresponding error field is returned.\\nDeploy a prompt flow in Amazon Bedrock\\nNote\\nPrompt flows is in preview and is subject to change.\\nWhen you first create a prompt flow, a working draft version (DRAFT) and a test alias\\n(TSTALIASID) that points to the working draft version are created. When you make changes to\\n your prompt flow, the changes apply to the working draft, and so it is the latest version of your\\nprompt flow. You iterate on your working draft until you\\'re satisfied with the behavior of your\\nprompt flow. Then, you can set up your prompt flow for deployment by creating versions of your\\nprompt flow.\\nA version is a snapshot that preserves the resource as it exists at the time it was created. You\\ncan continue to modify the working draft and create versions of your prompt flow as necessary.\\nDeploy a prompt flow 870\\nAmazon Bedrock User Guide\\nAmazon Bedrock creates versions in numerical order, starting from 1. Versions are immutable\\nbecause they act as a snapshot of your prompt flow at the time you created it. To make updates\\nto a prompt flow that you\\'ve deployed to production, you must create a new version from the\\nworking draft and make calls to the alias that points to that version.\\nTo deploy your prompt flow, you must create an alias that points to a version of your prompt\\n flow. Then, you make InvokeFlow requests to that alias. With aliases, you can switch efficiently\\nbetween different versions of your prompt flow without keeping track of the version. For example,\\nyou can change an alias to point to a previous version of your prompt flow if there are changes that\\nyou need to revert quickly.\\nTo deploy your prompt flow\\nCreate an alias and version of your prompt flow. Select the tab corresponding to your method of\\nchoice and follow the steps.\\nConsole\\nTo create a version of your Prompt flows\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at Getting Started with the AWS\\nManagement Console.\\n2. Select Prompt flows from the left navigation pane. Then, choose a prompt flow in the\\nPrompt flows section.\\n3. In the Versions section, choose Publish version.\\n4. After the version is published, a success banner appears at the top.\\nTo create an alias for your Prompt flows\\n'},\n",
       " {'question': 'What are the three rating methods available for model evaluation in Amazon Bedrock?',\n",
       "  'ground_truth': 'The three rating methods available for model evaluation in Amazon Bedrock are: Likert scale comparison of multiple model outputs, Choice buttons (radio button), and Ordinal rank.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'Amazon Bedrock User Guide\\nTo help workers complete their assigned tasks, you can provide instructions in two places.\\nProvide a good description for each evaluation and rating method\\nThe descriptions should provide a succinct explanation of the metrics selected. The description\\nshould expand on the metric, and make clear how you want workers to evaluate the selected rating\\nmethod. To see examples of how each rating method is shown in the worker UI, see Summary of\\navailable rating methods .\\nProvide your workers overall evaluation instructions\\nThese instructions are shown on the same webpage where workers complete a task. You can use\\nthis space to provide high level direction for the model evaluation job, and to describe the ground\\ntruth responses if you\\'ve included them in your prompt dataset.\\nSummary of available rating methods\\nIn each of the following sections, you can see an example of the rating methods your work team\\n saw in the evaluation UI, and also how those results are saved in Amazon S3.\\nLikert scale, comparison of multiple model outputs\\nHuman evaluators indicate their preference between the two responses from the model on a 5\\npoint Likert scale according to your instructions. The results in the final report will be shown as a\\nhistogram of preference strength ratings from the evaluators over your whole dataset.\\nMake sure you define the important points of the 5 point scale in your instructions, so your\\nevaluators know how to rate responses based on your expectations.\\nRating methods 473\\nAmazon Bedrock User Guide\\nJSON output\\nThe first child-key under evaluationResults is where the selected rating method is returned.\\nIn the output file saved to your Amazon S3 bucket, the results from each worker are saved to the\\n\"evaluationResults\": \"comparisonLikertScale\" key value pair.\\nChoice buttons (radio button)\\nChoice buttons allow a human evaluator to indicate their one preferred response over another\\n response. Evaluators indicate their preference between two responses according to your\\ninstructions with radio buttons. The results in the final report will be shown as a percentage of\\nresponses that workers preferred for each model. Be sure to explain your evaluation method clearly\\nin the instructions.\\nRating methods 474\\nAmazon Bedrock User Guide\\nJSON output\\nThe first child-key under evaluationResults is where the selected rating method is returned.\\nIn the output file saved to your Amazon S3 bucket, the results from each worker are saved to the\\n\"evaluationResults\": \"comparisonChoice\" key value pair.\\nOrdinal rank\\nOrdinal rank allows a human evaluator to rank their preferred responses to a prompt in order\\nstarting at 1 according to your instructions. The results in the final report will be shown as a\\nhistogram of the rankings from the evaluators over the whole dataset. Be sure to define what a\\nrank of 1 means in your instructions.\\nRating methods 475\\nAmazon Bedrock User Guide\\n'},\n",
       " {'question': 'How might adjusting the learning rate and batch size for Amazon Titan Text Premier model customization affect performance, and what precautions should be taken when modifying these parameters?',\n",
       "  'ground_truth': \"When adjusting the learning rate and batch size for Amazon Titan Text Premier model customization, it's important to consider their interdependence and potential impacts. The recommended learning rate range is between 1.00E-07 and 1.00E-05, with a default of 1.00E-06. A larger learning rate may speed up convergence but could negatively impact core model capabilities. For batch size changes, it's crucial to adjust the learning rate accordingly using the formula: newLearningRate = oldLearningRate x newBatchSize / oldBatchSize. However, it's important to note that Titan Text Premier currently only supports a mini-batch size of 1 for customer fine-tuning. To ensure optimal results, it's recommended to validate the training data quality using a small sub-sample (around 100 samples) and monitor validation metrics before submitting a larger training job. This approach helps in assessing the impact of hyperparameter adjustments on model performance while minimizing the risk of degrading the model's capabilities across various tasks.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': 'about the hyperparameters that you can set, see Amazon Titan text model customization\\nhyperparameters.\\nImpact on other tasks types\\nIn general, the larger the training dataset, the better the performance for a specific task. However,\\ntraining for a specific task might make the model perform worse on different tasks, especially if\\nyou use a lot of examples. For example, if the training dataset for a summarization task contains\\n100,000 samples, the model might perform worse on a classification task).\\nModel size\\nIn general, the larger the model, the better the task performs given limited training data.\\nIf you are using the model for a classification task, you might see relatively small gains for few-shot\\nfine-tuning (less than 100 samples), especially if the number of classes is relatively small (less than\\n100).\\nEpochs\\nWe recommend using the following metrics to determine the number of epochs to set:\\n 1. Validation output accuracy – Set the number of epochs to one that yields a high accuracy.\\n2. Training and validation loss – Determine the number of epochs after which the training and\\nvalidation loss becomes stable. This corresponds to when the model converges. Find the training\\nloss values in the step_wise_training_metrics.csv and validation_metrics.csv files.\\nAmazon Titan Text Premier 959\\nAmazon Bedrock User Guide\\nBatch size\\nWhen you change the batch size, we recommend that you change the learning rate using the\\nfollowing formula:\\nnewLearningRate = oldLearningRate x newBatchSize / oldBatchSize\\nTitan Text Premier model currently only supports mini-batch size of 1 for customer finetuning.\\nLearning rate\\nTo get the best results from finetuning capabilities, we recommend using a learning rate between\\n1.00E-07 and 1.00E-05. A good starting point is the recommended default value of 1.00E-06.\\nA larger learning rate may help training converge faster, however, it may adversely impact core\\n model capabilities.\\nValidate your training data with small sub-sample - To validate the quality of your training data,\\nwe recommend experimenting with a smaller dataset (~100s of samples) and monitoring the\\nvalidation metrics, before submitting the training job with larger training dataset.\\nLearning warmup steps\\nWe recommend the default value of 5.\\nTroubleshooting\\nThis section summarizes errors that you might encounter and what to check if you do.\\nPermissions issues\\nIf you encounter an issue with permissions to access an Amazon S3 bucket, check that the\\nfollowing are true:\\n1. If the Amazon S3 bucket uses a CM-KMS key for Server Side encryption, ensure that the IAM role\\npassed to Amazon Bedrock has kms:Decrypt permissions for the AWS KMS key. For example,\\nsee Allow a user to enccrypt and decrypt with any AWS KMS key in a specific AWS account.\\n2. The Amazon S3 bucket is in the same region as the Amazon Bedrock model customization job.\\n'},\n",
       " {'question': 'What is the rate limit set for the web crawler?',\n",
       "  'ground_truth': 'The rate limit for the web crawler is set to 50.',\n",
       "  'question_type': 'simple',\n",
       "  'context': '\"sourceConfiguration\": {\\n\"urlConfiguration\": {\\n\"seedUrls\": [{\\nWeb Crawler 599\\nAmazon Bedrock User Guide\\n\"url\": \"https://www.examplesite.com\"\\n}]\\n}\\n},\\n\"crawlerConfiguration\": {\\n\"crawlerLimits\": {\\n\"rateLimit\": 50\\n},\\n\"scope\": \"HOST_ONLY\",\\n\"inclusionFilters\": [\\n\"https://www\\\\.examplesite\\\\.com/.*\\\\.html\"\\n],\\n\"exclusionFilters\": [\\n\"https://www\\\\.examplesite\\\\.com/contact-us\\\\.html\"\\n]\\n}\\n},\\n\"type\": \"WEB\"\\n}\\nSync your data source with your Amazon Bedrock knowledge\\nbase\\nAfter you create your knowledge base, you ingest your data source/sources into your knowledge\\nbase so that they\\'re indexed and are able to be queried. Ingestion converts the raw data in your\\ndata source into vector embeddings. Before you begin ingestion, check that your data source fulfills\\nthe following conditions:\\n• You have configured the connection information for your data source. To configure a data source\\nconnector to crawl your data from your data source repository, see Supported data source\\nconnectors.\\n • The files are in supported formats. For more information, see Support document formats.\\n• The files don\\'t exceed the maximum file size of 50 MB. For more information, see Knowledge\\nbase quotas.\\n• If your data source contains metadata files, check the following conditions to ensure that the\\nmetadata files aren\\'t ignored:\\n• Each .metadata.json file shares the same name as the source file that it\\'s associated with.\\nSync your data source 600\\nAmazon Bedrock User Guide\\n• If the vector index for your knowledge base is in an Amazon OpenSearch Serverless vector\\nstore, check that the vector index is configured with the faiss engine. If the vector index is\\nconfigured with the nmslib engine, you\\'ll have to do one of the following:\\n• Create a new knowledge base in the console and let Amazon Bedrock automatically create a\\nvector index in Amazon OpenSearch Serverless for you.\\n• Create another vector index in the vector store and select faiss as the Engine. Then create\\n a new knowledge base and specify the new vector index.\\n• If the vector index for your knowledge base is in an Amazon Aurora database cluster, check\\nthat the table for your index contains a column for each metadata property in your metadata\\nfiles before starting ingestion.\\nNote\\nEach time you add, modify, or remove files from your data source, you must sync the data\\nsource so that it is re-indexed to the knowledge base. Syncing is incremental, so Amazon\\nBedrock only processes added, modified, or deleted documents since the last sync.\\nTo learn how to ingest your data sources into your knowledge base, Select the tab corresponding\\nto your method of choice and follow the steps.\\nConsole\\nTo ingest your data sources\\n1. Open the Amazon Bedrock console at https://console.aws.amazon.com/bedrock/.\\n2. From the left navigation pane, select Knowledge base and choose your knowledge base.\\n3. In the Data source section, select Sync to begin data ingestion.\\n'},\n",
       " {'question': 'How does the AmazonDataZoneBedrockPermissionsBoundary policy balance security and functionality for Amazon Bedrock Studio users, and what potential limitations might this create for advanced users?',\n",
       "  'ground_truth': 'The AmazonDataZoneBedrockPermissionsBoundary policy balances security and functionality by granting specific, limited permissions to Amazon Bedrock Studio users. It allows read and write access to essential AWS services like S3, Bedrock, OpenSearch Serverless, and Lambda, but only for resources managed by Amazon Bedrock Studio. This approach enhances security by preventing unauthorized access to other resources while still enabling core functionalities.\\n\\nHowever, this policy might create limitations for advanced users. For instance, they can only access S3 buckets with names starting with \"br-studio-\" and belonging to their account. Similarly, they can invoke Bedrock models but can\\'t create or modify them. The policy also restricts actions to specific services, potentially limiting users who need to integrate with other AWS services not covered by this policy.\\n\\nWhile this approach is secure and suitable for most use cases, advanced users might find it restrictive if they need to perform actions outside the predefined scope, such as accessing non-Bedrock Studio managed resources or using AWS services not explicitly allowed by the policy.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'When you create Amazon Bedrock Studio projects, apps, and components, Amazon Bedrock Studio\\napplies this permissions boundary to the IAM roles produced when creating those resources.\\nAmazon Bedrock Studio uses the AmazonDataZoneBedrockPermissionsBoundary managed\\npolicy to limit permissions of the provisioned IAM principal it is attached to. Principals might\\ntake the form of the user roles that Amazon DataZone can assume on behalf of Amazon Bedrock\\nStudio users, and then conduct actions such as reading and writing Amazon S3 objects or invoking\\nAmazon Bedrock agents.\\nThe AmazonDataZoneBedrockPermissionsBoundary policy grants read and write access for\\nAmazon Bedrock Studio to services such as Amazon S3, Amazon Bedrock, Amazon OpenSearch\\nServerless, and AWS Lambda. The policy also gives read and write permissions to some\\ninfrastructure resources that are required to use these services such as AWS Secrets Manager\\nsecrets, Amazon CloudWatch log groups, and AWS KMS keys.\\n This policy consists of the following sets of permissions.\\n• s3 – Allows read and write access to objects in Amazon S3 buckets that are managed by Amazon\\nBedrock Studio.\\n• bedrock – Grants the ability to use Amazon Bedrock agents, knowledge bases, and guardrails\\nthat are managed by Amazon Bedrock Studio.\\n• aoss – Allows API access to Amazon OpenSearch Serverless collections that are managed by\\nAmazon Bedrock Studio.\\n• lambda – Grants the ability to invoke AWS Lambda functions that are managed by Amazon\\nBedrock Studio.\\n• secretsmanager – Allows read and write access to AWS Secrets Manager secrets that are\\nmanaged by Amazon Bedrock Studio.\\n• logs – Provides write access to Amazon CloudWatch Logs that are managed by Amazon Bedrock\\nStudio.\\nIdentity-based policy examples 1087\\nAmazon Bedrock User Guide\\n• kms – Grants access to use AWS KMS keys for encrypting Amazon Bedrock Studio data.\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AccessS3Buckets\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n \"s3:ListBucket\",\\n\"s3:ListBucketVersions\",\\n\"s3:GetObject\",\\n\"s3:PutObject\",\\n\"s3:DeleteObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:DeleteObjectVersion\"\\n],\\n\"Resource\": \"arn:aws:s3:::br-studio-${aws:PrincipalAccount}-*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceAccount\": \"${aws:PrincipalAccount}\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"AccessOpenSearchCollections\",\\n\"Effect\": \"Allow\",\\n\"Action\": \"aoss:APIAccessAll\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceAccount\": \"${aws:PrincipalAccount}\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"InvokeBedrockModels\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"bedrock:InvokeModel\",\\n\"bedrock:InvokeModelWithResponseStream\"\\n],\\nIdentity-based policy examples 1088\\nAmazon Bedrock User Guide\\n\"Resource\": \"arn:aws:bedrock:*::foundation-model/*\"\\n},\\n{\\n\"Sid\": \"AccessBedrockResources\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"bedrock:InvokeAgent\",\\n\"bedrock:Retrieve\",\\n\"bedrock:StartIngestionJob\",\\n\"bedrock:GetIngestionJob\",\\n\"bedrock:ListIngestionJobs\",\\n\"bedrock:ApplyGuardrail\",\\n\"bedrock:ListPrompts\",\\n'},\n",
       " {'question': 'How can you invoke Cohere Command R on Amazon Bedrock?',\n",
       "  'ground_truth': 'You can invoke Cohere Command R on Amazon Bedrock using the Invoke Model API.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'an AWS SDK. This topic also includes information about getting started and details about previous\\nSDK versions.\\nInvoke Cohere Command R and R+ on Amazon Bedrock using the Invoke Model\\nAPI\\nThe following code examples show how to send a text message to Cohere Command R and R+,\\nusing the Invoke Model API.\\n.NET\\nAWS SDK for .NET\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Use the native inference API to send a text message to Cohere Command R.\\nusing System;\\nusing System.IO;\\nusing System.Text.Json;\\nusing System.Text.Json.Nodes;\\nusing Amazon;\\nusing Amazon.BedrockRuntime;\\nusing Amazon.BedrockRuntime.Model;\\n// Create a Bedrock Runtime client in the AWS Region you want to use.\\nvar client = new AmazonBedrockRuntimeClient(RegionEndpoint.USEast1);\\n// Set the model ID, e.g., Command R.\\nvar modelId = \"cohere.command-r-v1:0\";\\n// Define the user message.\\n var userMessage = \"Describe the purpose of a \\'hello world\\' program in one line.\";\\nCohere Command 1390\\nAmazon Bedrock User Guide\\n//Format the request payload using the model\\'s native structure.\\nvar nativeRequest = JsonSerializer.Serialize(new\\n{\\nmessage = userMessage,\\nmax_tokens = 512,\\ntemperature = 0.5\\n});\\n// Create a request with the model ID and the model\\'s native request payload.\\nvar request = new InvokeModelRequest()\\n{\\nModelId = modelId,\\nBody = new MemoryStream(System.Text.Encoding.UTF8.GetBytes(nativeRequest)),\\nContentType = \"application/json\"\\n};\\ntry\\n{\\n// Send the request to the Bedrock Runtime and wait for the response.\\nvar response = await client.InvokeModelAsync(request);\\n// Decode the response body.\\nvar modelResponse = await JsonNode.ParseAsync(response.Body);\\n// Extract and print the response text.\\nvar responseText = modelResponse[\"text\"] ?? \"\";\\nConsole.WriteLine(responseText);\\n}\\ncatch (AmazonBedrockRuntimeException e)\\n{\\n Console.WriteLine($\"ERROR: Can\\'t invoke \\'{modelId}\\'. Reason: {e.Message}\");\\nthrow;\\n}\\n• For API details, see InvokeModel in AWS SDK for .NET API Reference.\\nCohere Command 1391\\nAmazon Bedrock User Guide\\nJava\\nSDK for Java 2.x\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nUse the Invoke Model API to send a text message.\\n// Use the native inference API to send a text message to Cohere Command R.\\nimport org.json.JSONObject;\\nimport org.json.JSONPointer;\\nimport software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;\\nimport software.amazon.awssdk.core.SdkBytes;\\nimport software.amazon.awssdk.core.exception.SdkClientException;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.bedrockruntime.BedrockRuntimeClient;\\npublic class Command_R_InvokeModel {\\npublic static String invokeModel() {\\n// Create a Bedrock Runtime client in the AWS Region you want to use.\\n'},\n",
       " {'question': \"In an OpenAPI schema for an action group, how do the 'parameters' and 'requestBody' sections differ in their structure and purpose, and what key information should be included in each to guide an AI agent's interaction with users?\",\n",
       "  'ground_truth': \"The 'parameters' and 'requestBody' sections in an OpenAPI schema for an action group serve different purposes and have distinct structures:\\n\\n1. Parameters:\\n- Structure: An array of objects, each representing a parameter.\\n- Purpose: Define information needed from the user that's typically part of the URL or query string.\\n- Key information:\\n  a) 'name': The parameter's identifier (required)\\n  b) 'description': Explains the parameter's purpose (required)\\n  c) 'required': Boolean indicating if it's mandatory (optional)\\n  d) 'schema': Defines the data type and format (optional)\\n\\n2. RequestBody:\\n- Structure: An object with 'required', 'content', and nested 'schema' fields.\\n- Purpose: Specifies the structure of data sent in the request body, typically for POST or PUT requests.\\n- Key information:\\n  a) 'required': Boolean indicating if the request body is mandatory (optional)\\n  b) 'content': Specifies the media type and schema of the body\\n  c) 'schema': Defines properties of the request body, including:\\n     - 'type': Data type of each property\\n     - 'description': Explains each property's purpose\\n\\nBoth sections guide the AI agent in gathering necessary information from users. The 'parameters' section helps the agent understand what to ask for URL or query parameters, while the 'requestBody' section informs the agent about the structure of data needed in the request body. This allows the agent to effectively elicit all required information from users to make a successful API call.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': 'order to fulfill a task. Each property contains the following fields:\\n• type – (Required for each property) The data type of the response field.\\n• description – (Optional) Describes the property. The agent can use this information to\\ndetermine the information that it needs to return to the end user.\\nparameters\\n\"parameters\": [\\n{\\n\"name\": \"string\",\\n\"description\": \"string\",\\n\"required\": boolean,\\n\"schema\": {\\n...\\n}\\n},\\n...\\n]\\nYour agent uses the following fields to determine the information it must get from the end user\\nto perform the action group\\'s requirements.\\n• name – (Required) The name of the parameter.\\n• description – (Required) A description of the parameter. Use this field to help the agent\\nunderstand how to elicit this parameter from the agent user or determine that it already has\\nthat parameter value from prior actions or from the user’s request to the agent.\\n• required – (Optional) Whether the parameter is required for the API request. Use this\\n field to indicate to the agent whether this parameter is needed for every invocation or if it\\'s\\noptional.\\n• schema – (Optional) The definition of input and output data types. For more information, see\\nData Models (Schemas) on the Swagger website.\\nDefining actions in the action group 663\\nAmazon Bedrock User Guide\\nrequestBody\\nFollowing is the general structure of a requestBody field:\\n\"requestBody\": {\\n\"required\": boolean,\\n\"content\": {\\n\"<media type>\": {\\n\"schema\": {\\n\"properties\": {\\n\"<property>\": {\\n\"type\": \"string\",\\n\"description\": \"string\"\\n},\\n...\\n}\\n}\\n}\\n}\\n}\\nThe following list describes each field:\\n• required – (Optional) Whether the request body is required for the API request.\\n• content – (Required) The content of the request body.\\n• <media type> – (Optional) The format of the request body. For more information, see\\nMedia types on the Swagger website.\\n• schema – (Optional) Defines the data type of the request body and its fields.\\n • properties – (Optional) Your agent uses properties that you define in the schema to\\ndetermine the information it must get from the end user to make the API request. Each\\nproperty contains the following fields:\\n• type – (Optional) The data type of the request field.\\n• description – (Optional) Describes the property. The agent can use this information to\\ndetermine the information it needs to return to the end user.\\nTo learn how to add the OpenAPI schema you created while creating the action group, see Add an\\naction group to your agent in Amazon Bedrock.\\nDefining actions in the action group 664\\nAmazon Bedrock User Guide\\nExample API schemas\\nThe following example provides a simple OpenAPI schema in YAML format that gets the weather\\nfor a given location in Celsius.\\nopenapi: 3.0.0\\ninfo:\\ntitle: GetWeather API\\nversion: 1.0.0\\ndescription: gets weather\\npaths:\\n/getWeather/{location}/:\\nget:\\nsummary: gets weather in Celsius\\ndescription: gets weather in Celsius\\noperationId: getWeather\\nparameters:\\n'},\n",
       " {'question': 'What permissions does Amazon Bedrock need for agent memory encryption?',\n",
       "  'ground_truth': 'Amazon Bedrock needs permissions to generate encrypted data keys, use them to encrypt agent memory, and re-encrypt the generated data key with different encryption contexts. Specifically, it requires \"kms:GenerateDataKeyWithoutPlainText\" and \"kms:ReEncrypt*\" actions.',\n",
       "  'question_type': 'simple',\n",
       "  'context': '\"Resource\": \"arn:aws:kms:${region}:${account-id}:key/${key-id}\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"kms:EncryptionContext:aws:bedrock:arn\":\\n\"arn:aws:bedrock:${region}:${account-id}:agent/${agent-id}\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"Allow the service role to use the key to encrypt and decrypt\\nAgent resources\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::${account-id}:role/${role}\"\\n},\\n\"Action\": [\\n\"kms:GenerateDataKey*\",\\n\"kms:Decrypt\",\\n],\\n\"Resource\": \"arn:aws:kms:${region}:${account-id}:key/${key-id}\"\\n},\\n{\\n\"Sid\": \"Allow the attachment of persistent resources\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"bedrock.amazonaws.com\"\\n},\\n\"Action\": [\\n\"kms:CreateGrant\",\\n\"kms:ListGrants\",\\n\"kms:RevokeGrant\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"Bool\": {\\n\"kms:GrantIsForAWSResource\": \"true\"\\n}\\n}\\n}\\n]\\nData encryption 1041\\nAmazon Bedrock User Guide\\n}\\nPermissions for agent memory\\nIf you\\'ve enabled memory for your agent and if you encrypt agent sessions with a customer\\n managed key, you must configure the following key policy and the calling identity IAM permissions\\nto configure your customer managed key.\\nCustomer managed key policy\\nAmazon Bedrock uses these permissions to generate encrypted data keys and then use the\\ngenerated keys to encrypt agent memory. Amazon Bedrock also needs permissions to re-encrypt\\nthe the generated data key with different encryption contexts. Re-encrypt permissions are also\\nused when customer managed key transitions between another customer managed key or service\\nowned key. For more information, see Hierarchical Keyring.\\nReplace the $region, account-id, and ${caller-identity-role} with appropriate values.\\n{\\n\"Version\": \"2012-10-17\",\\n{\\n\"Sid\": \"Allow access for bedrock to enable long term memory\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": [\\n\"bedrock.amazonaws.com\",\\n],\\n},\\n\"Action\": [\\n\"kms:GenerateDataKeyWithoutPlainText\",\\n\"kms:ReEncrypt*\"\\n],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"$account-id\"\\n},\\n\"ArnLike\": {\\n \"aws:SourceArn\": \"arn:aws:bedrock:$region:$account-id:agent-alias/*\"\\n}\\n}\\n\"Resource\": \"*\"\\n},\\nData encryption 1042\\nAmazon Bedrock User Guide\\n{\\n\"Sid\": \"Allow the caller identity control plane permissions for long term\\nmemory\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::${account-id}:role/${caller-identity-role}\"\\n},\\n\"Action\": [\\n\"kms:GenerateDataKeyWithoutPlainText\",\\n\"kms:ReEncrypt*\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"kms:EncryptionContext:aws-crypto-ec:aws:bedrock:arn\":\\n\"arn:aws:bedrock:${region}:${account-id}:agent-alias/*\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"Allow the caller identity data plane permissions to decrypt long term\\nmemory\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::${account-id}:role/${caller-identity-role}\"\\n},\\n\"Action\": [\\n\"kms:Decrypt\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"kms:EncryptionContext:aws-crypto-ec:aws:bedrock:arn\":\\n\"arn:aws:bedrock:${region}:${account-id}:agent-alias/*\",\\n\"kms:ViaService\": \"bedrock.$region.amazonaws.com\"\\n}\\n}\\n}\\n'},\n",
       " {'question': \"How can a developer selectively apply guardrails to specific parts of a conversation when using Amazon Bedrock's Converse API, and what are the potential benefits of this approach?\",\n",
       "  'ground_truth': 'Developers can selectively apply guardrails to specific parts of a conversation in Amazon Bedrock\\'s Converse API by using the guardContent field within a Message object. This field allows for the guardrail to assess only the content specified in guardContent, rather than the entire message. \\n\\nThe approach offers several benefits:\\n\\n1. Targeted assessment: Developers can focus the guardrail on the most recent or relevant message in a conversation, ignoring previous context that may not need evaluation.\\n\\n2. Efficiency: By limiting the scope of guardrail assessment, it potentially reduces processing time and resource usage.\\n\\n3. Contextual flexibility: Additional context can be provided in the message without subjecting it to guardrail assessment, allowing for more nuanced conversations.\\n\\n4. Fine-grained control: Developers can apply guardrails selectively to different parts of complex messages or multi-turn conversations.\\n\\nTo implement this, developers would structure their Message object with a guardContent field containing a GuardrailConverseContentBlock. For example:\\n\\n{\\n  \"role\": \"user\",\\n  \"content\": [\\n    {\\n      \"guardContent\": {\\n        \"text\": {\\n          \"text\": \"Create a playlist of 2 heavy metal songs.\"\\n        }\\n      }\\n    }\\n  ]\\n}\\n\\nThis approach allows for more precise and efficient use of guardrails in conversational AI applications built with Amazon Bedrock.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'in the message that you want the guardrail to assess. For information about the models that you\\ncan use with guardrails and the Converse API, see Supported models and model features.\\nTopics\\n• Configuring the guardrail\\n• Guarding a message\\n• Guarding a system prompt\\n• Message and system prompt guardrail behavior\\nConfiguring the guardrail\\nYou specify configuration information for the guardrail in the guardrailConfig input parameter.\\nThe configuration includes the ID and the version of the guardrail that you want to use. You can\\nalso enable tracing for the guardrail, which provides information about the content that the\\nguardrail blocked.\\nWith the Converse operation, guardrailConfig is a GuardrailConfiguration object, as shown in\\nthe following example.\\n{\\n\"guardrailIdentifier\": \"Guardrail ID\",\\n\"guardrailVersion\": \"Guardrail version\",\\n\"trace\": \"enabled\"\\n}\\nUse the base inference operations 415\\nAmazon Bedrock User Guide\\n If you use ConverseStream, you pass a GuardrailStreamConfiguration object. Optionally, you\\ncan use the streamProcessingMode field to specify that you want the model to complete the\\nguardrail assessment, before returning streaming response chunks. Or, you can have the model\\nasynchronously respond whilst the guardrail continues its assessment in the background. For more\\ninformation, see Configure streaming response behavior.\\nGuarding a message\\nWhen you pass a message (Message) to a model, the guardrail assesses the content in the message.\\nOptionally, you can guard selected content in the message by specifying the guardContent\\n(GuardrailConverseContentBlock) field. The guardrail evaluates only the content in the\\nguardContent field and not the rest of the message. This is useful for having the guardrail assess\\nonly the most message in a conversation, as shown in the following example.\\n[\\n{\\n\"role\": \"user\",\\n\"content\": [\\n{\\n\"text\": \"Create a playlist of 2 pop songs.\"\\n}\\n]\\n},\\n{\\n \"role\": \"assistant\",\\n\"content\": [\\n{\\n\"text\": \" Sure! Here are two pop songs:\\\\n1. \\\\\"Bad Habits\\\\\" by Ed\\nSheeran\\\\n2. \\\\\"All Of The Lights\\\\\" by Kanye West\\\\n\\\\nWould you like to add any more\\nsongs to this playlist? \"\\n}\\n]\\n},\\n{\\n\"role\": \"user\",\\n\"content\": [\\n{\\n\"guardContent\": {\\n\"text\": {\\n\"text\": \"Create a playlist of 2 heavy metal songs.\"\\n}\\n}\\nUse the base inference operations 416\\nAmazon Bedrock User Guide\\n}\\n]\\n}\\n]\\nAnother use is providing additional context for a message, without having the guardrail assess that\\nadditional context.\\n[\\n{\\n\"role\": \"user\",\\n\"content\": [\\n{\\n\"text\": \"Only answer with a list of songs.\"\\n},\\n{\\n\"guardContent\": {\\n\"text\": {\\n\"text\": \"Create a playlist of heavy metal songs.\"\\n}\\n}\\n}\\n]\\n}\\n]\\nNote\\nUsing the guardContent field is analogous to using input tags with InvokeModel and\\nInvokeModelWithResponseStream. For more information, see the section called “Input\\ntags”.\\nGuarding a system prompt\\nYou can use guardrails with system prompts that you send to the Converse API. To guard a system\\n'},\n",
       " {'question': 'What is the recommended learning rate range for Titan Text Premier model finetuning?',\n",
       "  'ground_truth': 'The recommended learning rate range for Titan Text Premier model finetuning is between 1.00E-07 and 1.00E-05, with a default value of 1.00E-06.',\n",
       "  'question_type': 'simple',\n",
       "  'context': '1. Validation output accuracy – Set the number of epochs to one that yields a high accuracy.\\n2. Training and validation loss – Determine the number of epochs after which the training and\\nvalidation loss becomes stable. This corresponds to when the model converges. Find the training\\nloss values in the step_wise_training_metrics.csv and validation_metrics.csv files.\\nAmazon Titan Text Premier 959\\nAmazon Bedrock User Guide\\nBatch size\\nWhen you change the batch size, we recommend that you change the learning rate using the\\nfollowing formula:\\nnewLearningRate = oldLearningRate x newBatchSize / oldBatchSize\\nTitan Text Premier model currently only supports mini-batch size of 1 for customer finetuning.\\nLearning rate\\nTo get the best results from finetuning capabilities, we recommend using a learning rate between\\n1.00E-07 and 1.00E-05. A good starting point is the recommended default value of 1.00E-06.\\nA larger learning rate may help training converge faster, however, it may adversely impact core\\n model capabilities.\\nValidate your training data with small sub-sample - To validate the quality of your training data,\\nwe recommend experimenting with a smaller dataset (~100s of samples) and monitoring the\\nvalidation metrics, before submitting the training job with larger training dataset.\\nLearning warmup steps\\nWe recommend the default value of 5.\\nTroubleshooting\\nThis section summarizes errors that you might encounter and what to check if you do.\\nPermissions issues\\nIf you encounter an issue with permissions to access an Amazon S3 bucket, check that the\\nfollowing are true:\\n1. If the Amazon S3 bucket uses a CM-KMS key for Server Side encryption, ensure that the IAM role\\npassed to Amazon Bedrock has kms:Decrypt permissions for the AWS KMS key. For example,\\nsee Allow a user to enccrypt and decrypt with any AWS KMS key in a specific AWS account.\\n2. The Amazon S3 bucket is in the same region as the Amazon Bedrock model customization job.\\n 3. The IAM role trust policy includes the service SP (bedrock.amazonaws.com).\\nTroubleshooting 960\\nAmazon Bedrock User Guide\\nThe following messages indicate issues with permissions to access training or validation data in an\\nAmazon S3 bucket:\\nCould not validate GetObject permissions to access Amazon S3 bucket: training-data-\\nbucket at key train.jsonl\\nCould not validate GetObject permissions to access Amazon S3 bucket: validation-data-\\nbucket at key validation.jsonl\\nIf you encounter one of the above errors, check that the IAM role passed to the service has\\ns3:GetObject and s3:ListBucket permissions for the training and validation dataset Amazon\\nS3 URIs.\\nThe following message indicates issues with permissions to write the output data in an Amazon S3\\nbucket:\\nAmazon S3 perms missing (PutObject): Could not validate PutObject permissions to access\\nS3 bucket: bedrock-output-bucket at key output/.write_access_check_file.tmp\\n'},\n",
       " {'question': 'How does Forward Access Sessions (FAS) differ from using a service role, and what potential security implications should be considered when choosing between these two approaches for cross-service actions in AWS?',\n",
       "  'ground_truth': \"Forward Access Sessions (FAS) and service roles are two different approaches for handling cross-service actions in AWS, each with distinct characteristics and security implications:\\n\\n1. FAS uses the permissions of the principal (user or role) calling an AWS service, combined with the requesting service's permissions, to make requests to downstream services. This means the original caller's permissions are propagated, which can provide more granular control but also requires the caller to have permissions for both the initial and subsequent actions.\\n\\n2. A service role, on the other hand, is an IAM role that a service assumes to perform actions on your behalf. It has its own set of permissions defined, independent of the calling principal.\\n\\nSecurity implications to consider:\\n\\n- With FAS, you need to ensure that the calling principal has appropriate permissions for all potential downstream actions, which can lead to broader permission sets and potential security risks if not managed carefully.\\n- Service roles allow for more isolation of permissions, as the role's permissions are separate from the caller's. This can enhance security by limiting the scope of actions a service can perform.\\n- FAS may provide better auditability as actions are tied directly to the original caller, while service roles might obscure the original requester in logs.\\n- Service roles can be easier to manage centrally and can be reused across multiple calls or services, potentially simplifying permission management.\\n\\nThe choice between FAS and service roles depends on the specific use case, security requirements, and the desired balance between granular control and ease of management.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': \"you make a call in a service, it's common for that service to run applications in Amazon EC2 or\\nstore objects in Amazon S3. A service might do this using the calling principal's permissions,\\nusing a service role, or using a service-linked role.\\n• Forward access sessions (FAS) – When you use an IAM user or role to perform actions in\\nAWS, you are considered a principal. When you use some services, you might perform an\\naction that then initiates another action in a different service. FAS uses the permissions of the\\nprincipal calling an AWS service, combined with the requesting AWS service to make requests\\nto downstream services. FAS requests are only made when a service receives a request that\\nrequires interactions with other AWS services or resources to complete. In this case, you must\\nhave permissions to perform both actions. For policy details when making FAS requests, see\\nForward access sessions.\\n • Service role – A service role is an IAM role that a service assumes to perform actions on your\\nbehalf. An IAM administrator can create, modify, and delete a service role from within IAM. For\\nmore information, see Creating a role to delegate permissions to an AWS service in the IAM\\nUser Guide.\\n• Service-linked role – A service-linked role is a type of service role that is linked to an AWS\\nservice. The service can assume the role to perform an action on your behalf. Service-linked\\nroles appear in your AWS account and are owned by the service. An IAM administrator can\\nview, but not edit the permissions for service-linked roles.\\n• Applications running on Amazon EC2 – You can use an IAM role to manage temporary\\ncredentials for applications that are running on an EC2 instance and making AWS CLI or AWS API\\nrequests. This is preferable to storing access keys within the EC2 instance. To assign an AWS role\\n to an EC2 instance and make it available to all of its applications, you create an instance profile\\nthat is attached to the instance. An instance profile contains the role and enables programs that\\nare running on the EC2 instance to get temporary credentials. For more information, see Using\\nan IAM role to grant permissions to applications running on Amazon EC2 instances in the IAM\\nUser Guide.\\nAuthenticating with identities 1065\\nAmazon Bedrock User Guide\\nTo learn whether to use IAM roles or IAM users, see When to create an IAM role (instead of a user)\\nin the IAM User Guide.\\nManaging access using policies\\nYou control access in AWS by creating policies and attaching them to AWS identities or resources.\\nA policy is an object in AWS that, when associated with an identity or resource, defines their\\npermissions. AWS evaluates these policies when a principal (user, root user, or role session) makes\\na request. Permissions in the policies determine whether the request is allowed or denied. Most\\n\"},\n",
       " {'question': 'Which AWS regions support all Amazon Bedrock models except Claude 3 Opus, Titan Text Premier, and Mistral Small?',\n",
       "  'ground_truth': 'US East (N. Virginia, us-east-1) and US West (Oregon, us-west-2) regions support all Amazon Bedrock models except Claude 3 Opus, Titan Text Premier, and Mistral Small.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"compability.\\n• To return information about a specific foundation model, send a GetFoundationModel request,\\nspecifying the model ID.\\nSelect a tab to see code examples in an interface or language.\\nGet model information 38\\nAmazon Bedrock User Guide\\nAWS CLI\\nList the Amazon Bedrock foundation models.\\naws bedrock list-foundation-models\\nGet information about Anthropic Claude v2.\\naws bedrock get-foundation-model --model-identifier anthropic.claude-v2\\nPython\\nList the Amazon Bedrock foundation models.\\nimport boto3\\nbedrock = boto3.client(service_name='bedrock')\\nbedrock.list_foundation_models()\\nGet information about Anthropic Claude v2.\\nimport boto3\\nbedrock = boto3.client(service_name='bedrock')\\nbedrock.get_foundation_model(modelIdentifier='anthropic.claude-v2')\\nModel support by AWS Region\\nNote\\nAll models, except Anthropic Claude 3 Opus, Amazon Titan Text Premier, and Mistral Small\\nare supported in both the US East (N. Virginia, us-east-1) and the US West (Oregon, us-\\nwest-2) Regions.\\n Amazon Titan Text Premier, Mistral Small, and AI21 Jamba-Instruct models are only\\navailable in the US East (N. Virginia, us-east-1) Region.\\nAnthropic Claude 3 Opus, Meta Llama 3.1 Instruct, and Mistral Large 2 (24.07) models are\\nonly available in the US West (Oregon, us-west-2) Region.\\nModel support by AWS Region 39\\nAmazon Bedrock User Guide\\nNote\\nAccess to models in Europe (Ireland) and Asia Pacific (Singapore) Regions are currently\\ngated. Please contact your account manager to request model access in these Regions.\\nThe following table shows the FMs that are available in other Regions and whether they're\\nsupported in each Region.\\nModel Asia Asia Asia Asia Canada Europe Europe Europe Europe South AWS\\nPacific Pacific Pacific Pacific (Central()Frankfu(rIr eland)( London()Paris) AmericaG ovCloud\\n(Mumba(iS)ingapo(Sr ydney()Tokyo) t) NOTE: (São (US-\\ne) Gated Paulo) West)\\nNOTE: access\\nGated only\\naccess\\nonly\\nAmazon Yes No Yes Yes Yes Yes Gated Yes Yes Yes Yes\\nTitan\\nText\\nG1 -\\nExpress\\n Amazon Yes No Yes No Yes Yes Gated Yes Yes Yes No\\nTitan\\nText\\nG1 -\\nLite\\nAmazon No No No No No No No No No No No\\nTitan\\nText\\nPremier\\nAmazon No No No Yes No Yes No No No No No\\nTitan\\nEmbedding\\nModel support by AWS Region 40\\nAmazon Bedrock User Guide\\nModel Asia Asia Asia Asia Canada Europe Europe Europe Europe South AWS\\nPacific Pacific Pacific Pacific (Central()Frankfu(rIr eland)( London()Paris) AmericaG ovCloud\\n(Mumba(iS)ingapo(Sr ydney()Tokyo) t) NOTE: (São (US-\\ne) Gated Paulo) West)\\nNOTE: access\\nGated only\\naccess\\nonly\\ns\\nG1 -\\nText\\nAmazon No No No No Yes Yes No Yes No Yes Yes\\nTitan\\nText\\nEmbedding\\ns V2\\nAmazon Yes No Yes No Yes Yes Gated Yes Yes Yes No\\nTitan\\nMultimoda\\nl\\nEmbedding\\ns G1\\nAmazon Yes No No No No No Gated Yes No No No\\nTitan\\nImage\\nGenerator\\nG1\\nV1\\nModel support by AWS Region 41\\nAmazon Bedrock User Guide\\nModel Asia Asia Asia Asia Canada Europe Europe Europe Europe South AWS\\nPacific Pacific Pacific Pacific (Central()Frankfu(rIr eland)( London()Paris) AmericaG ovCloud\\n\"},\n",
       " {'question': \"How does the ModelInvocationInput object in Amazon Bedrock's trace events contribute to understanding and potentially modifying an agent's behavior across different processing steps?\",\n",
       "  'ground_truth': \"The ModelInvocationInput object in Amazon Bedrock's trace events provides crucial insights into an agent's behavior and allows for potential modifications across different processing steps. It appears in various trace types (PreProcessingTrace, OrchestrationTrace, PostProcessingTrace, and GuardrailTrace) and contains several key elements:\\n\\n1. Prompt information: The 'text' field shows the exact prompt given to the agent at each step, allowing developers to understand what input the agent is working with.\\n\\n2. Processing stage identification: The 'type' field indicates which step of the agent's process is being traced (e.g., PRE_PROCESSING, ORCHESTRATION), enabling a clear view of the agent's workflow.\\n\\n3. Inference configuration: The 'inferenceConfiguration' object includes parameters like maximumLength, stopSequences, temperature, topK, and topP, which influence response generation. By analyzing and adjusting these, developers can fine-tune the agent's output.\\n\\n4. Customization flags: 'promptCreationMode' and 'parserMode' indicate whether default templates or parsers have been overridden, while 'overrideLambda' provides the ARN of any custom parser function used.\\n\\nThis comprehensive information allows developers to:\\n- Trace the agent's decision-making process across different stages\\n- Identify areas for optimization or customization\\n- Understand how prompt engineering and inference parameters affect the agent's performance\\n- Implement and track the impact of advanced prompts or custom parsers\\n\\nBy leveraging this data, developers can iteratively refine the agent's behavior, improving its effectiveness and tailoring it to specific use cases.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': '• PreProcessingTrace – Traces the input and output of the pre-processing step, in which the agent\\ncontextualizes and categorizes user input and determines if it is valid.\\n• OrchestrationTrace – Traces the input and output of the orchestration step, in which the agent\\ninterprets the input, invokes action groups, and queries knowledge bases. Then the agent returns\\noutput to either continue orchestration or to respond to the user.\\nTrace events 711\\nAmazon Bedrock User Guide\\n• PostProcessingTrace – Traces the input and output of the post-processing step, in which the\\nagent handles the final output of the orchestration and determines how to return the response\\nto the user.\\n• FailureTrace – Traces the reason that a step failed.\\n• GuardrailTrace – Traces the actions of the Guardrail.\\nEach of the traces (except FailureTrace) contains a ModelInvocationInput object. The\\nModelInvocationInput object contains configurations set in the prompt template for the step,\\n alongside the prompt provided to the agent at this step. For more information about how to\\nmodify prompt templates, see Advanced prompts in Amazon Bedrock. The structure of the\\nModelInvocationInput object is as follows:\\n{\\n\"traceId\": \"string\",\\n\"text\": \"string\",\\n\"type\": \"PRE_PROCESSING | ORCHESTRATION | KNOWLEDGE_BASE_RESPONSE_GENERATION |\\nPOST_PROCESSING\",\\n\"inferenceConfiguration\": {\\n\"maximumLength\": number,\\n\"stopSequences\": [\"string\"],\\n\"temperature\": float,\\n\"topK\": float,\\n\"topP\": float\\n},\\n\"promptCreationMode\": \"DEFAULT | OVERRIDDEN\",\\n\"parserMode\": \"DEFAULT | OVERRIDDEN\",\\n\"overrideLambda\": \"string\"\\n}\\nThe following list describes the fields of the ModelInvocationInput object:\\n• traceId – The unique identifier of the trace.\\n• text – The text from the prompt provided to the agent at this step.\\n• type – The current step in the agent\\'s process.\\n• inferenceConfiguration – Inference parameters that influence response generation. For\\nmore information, see Inference parameters.\\n • promptCreationMode – Whether the agent\\'s default base prompt template was overridden for\\nthis step. For more information, see Advanced prompts in Amazon Bedrock.\\nTrace events 712\\nAmazon Bedrock User Guide\\n• parserMode – Whether the agent\\'s default response parser was overridden for this step. For\\nmore information, see Advanced prompts in Amazon Bedrock.\\n• overrideLambda – The Amazon Resource Name (ARN) of the parser Lambda function used to\\nparse the response, if the default parser was overridden. For more information, see Advanced\\nprompts in Amazon Bedrock.\\nFor more information about each trace type, see the following sections:\\nPreProcessingTrace\\n{\\n\"modelInvocationInput\": { // see above for details }\\n\"modelInvocationOutput\": {\\n\"parsedResponse\": {\\n\"isValid\": boolean,\\n\"rationale\": \"string\"\\n},\\n\"traceId\": \"string\"\\n}\\n}\\nThe PreProcessingTrace consists of a ModelInvocationInput object and a\\nPreProcessingModelInvocationOutput object. The PreProcessingModelInvocationOutput contains\\n'},\n",
       " {'question': 'How many digits are in a SWIFT code?',\n",
       "  'ground_truth': \"SWIFT codes consist of either 8 or 11 characters. The 11-digit codes refer to specific branches, while 8-digit codes (or 11-digit codes ending in 'XXX') refer to the head or primary office.\",\n",
       "  'question_type': 'simple',\n",
       "  'context': 'credit and debit cards. For American Express credit or debit cards, the CVV is a four-digit\\nnumeric code.\\n• CREDIT_DEBIT_CARD_EXPIRY\\nThe expiration date for a credit or debit card. This number is usually four digits long and\\nis often formatted as month/year or MM/YY. Guardrails for Amazon Bedrock recognizes\\nexpiration dates such as 01/21, 01/2021, and Jan 2021.\\n• CREDIT_DEBIT_CARD_NUMBER\\nThe number for a credit or debit card. These numbers can vary from 13 to 16 digits in length.\\nHowever, Amazon Comprehend also recognizes credit or debit card numbers when only the\\nlast four digits are present.\\n• PIN\\nA four-digit personal identification number (PIN) with which you can access your bank account.\\n• INTERNATIONAL_BANK_ACCOUNT_NUMBER\\nAn International Bank Account Number has specific formats in each country. For more\\ninformation, see www.iban.com/structure.\\n• SWIFT_CODE\\nA SWIFT code is a standard format of Bank Identifier Code (BIC) used to specify a particular\\n bank or branch. Banks use these codes for money transfers such as international wire transfers.\\nSWIFT codes consist of eight or 11 characters. The 11-digit codes refer to specific branches,\\nwhile eight-digit codes (or 11-digit codes ending in \\'XXX\\') refer to the head or primary office.\\n• IT\\nSensitive information filters 369\\nAmazon Bedrock User Guide\\n• IP_ADDRESS\\nAn IPv4 address, such as 198.51.100.0.\\n• MAC_ADDRESS\\nA media access control (MAC) address is a unique identifier assigned to a network interface\\ncontroller (NIC).\\n• URL\\nA web address, such as www.example.com.\\n• AWS_ACCESS_KEY\\nA unique identifier that\\'s associated with a secret access key; you use the access key ID and\\nsecret access key to sign programmatic AWS requests cryptographically.\\n• AWS_SECRET_KEY\\nA unique identifier that\\'s associated with an access key. You use the access key ID and secret\\naccess key to sign programmatic AWS requests cryptographically.\\n• USA specific\\n• US_BANK_ACCOUNT_NUMBER\\n A US bank account number, which is typically 10 to 12 digits long.\\n• US_BANK_ROUTING_NUMBER\\nA US bank account routing number. These are typically nine digits long,\\n• US_INDIVIDUAL_TAX_IDENTIFICATION_NUMBER\\nA US Individual Taxpayer Identification Number (ITIN) is a nine-digit number that starts with a\\n\"9\" and contain a \"7\" or \"8\" as the fourth digit. An ITIN can be formatted with a space or a dash\\nafter the third and forth digits.\\n• US_PASSPORT_NUMBER\\nA US passport number. Passport numbers range from six to nine alphanumeric characters.\\n• US_SOCIAL_SECURITY_NUMBER\\nA US Social Security Number (SSN) is a nine-digit number that is issued to US citizens,\\npermanent residents, and temporary working residents.\\nSensitive information filters 370\\nAmazon Bedrock User Guide\\n• Canada specific\\n• CA_HEALTH_NUMBER\\nA Canadian Health Service Number is a 10-digit unique identifier, required for individuals to\\naccess healthcare benefits.\\n• CA_SOCIAL_INSURANCE_NUMBER\\n'},\n",
       " {'question': 'In the described prompt flow, how does the system determine which action to take for an item with a retail price of $15, a market price of $13, and a type of \"produce\", and why is this specific action chosen?',\n",
       "  'ground_truth': 'The system would choose the action \"buy\" for this item. Here\\'s why:\\n\\n1. The flow has two conditions and a default output:\\n   - Condition 1: (retailPrice > 10) and (type == \"produce\")\\n   - Condition 2: (retailPrice > marketPrice)\\n   - Default (if no conditions are met)\\n\\n2. For the given item:\\n   - retailPrice ($15) is greater than 10\\n   - type is \"produce\"\\n   - retailPrice ($15) is greater than marketPrice ($13)\\n\\n3. The item satisfies both conditions. However, the flow is configured to check conditions in order and use the first matching condition.\\n\\n4. Condition 1 is met first, so the system will use the first flow output node.\\n\\n5. The first flow output node is configured to return $.data.action[0], which is the first value in the action array.\\n\\n6. Based on the example provided in the context, the action array is typically structured as [\"don\\'t buy\", \"buy\", \"undecided\"].\\n\\n7. Therefore, $.data.action[1] would return \"buy\", which is the second element in the array.\\n\\nThis demonstrates how the flow uses conditional logic to determine actions based on item attributes, and how the order of conditions and the structure of the output array affect the final decision.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'retailPrice Number $.data.re\\ntailPrice\\nExample prompt flows 857\\nAmazon Bedrock User Guide\\nName Type Expression\\nmarketPrice Number $.data.ma\\nrketPrice\\ntype String $.data.type\\nThis configuration means that the condition node expects a JSON object that contains the\\nfields retailPrice, marketPrice, and type.\\ne. Configure the conditions by doing the following:\\ni. In the Conditions section, optionally change the name of the condition. Then add the\\nfollowing condition in the Condition text box: (retailPrice > 10) and (type\\n== \"produce\").\\nii. Add a second condition by choosing Add condition. Optionally change the name of\\nthe second condition. Then add the following condition in the Condition text box:\\n(retailPrice > marketPrice).\\n3. Choose the Flow input node and select the Configure tab. Select Object as the Type. This\\nmeans that flow invocation will expect to receive a JSON object.\\n4. Add flow output nodes so that you have three in total. Configure them as follows in the\\n Configure tab of the Prompt flow builder pane of each flow output node:\\na. Set the input type of the first flow output node as String and the expression as\\n$.data.action[0] to return the first value in the array in the action field of the\\nincoming object.\\nb. Set the input type of the second flow output node as Number and the expression as\\n$.data.action[1] to return the second value in the array in the action field of the\\nincoming object.\\nc. Set the input type of the third flow output node as Number and the expression as\\n$.data.action[2] to return the third value in the array in the action field of the\\nincoming object.\\n5. Connect the first condition to the first flow output node, the second condition to the second\\nflow output node, and the default condition to the third flow output node.\\n6. Connect the inputs and outputs in all the nodes to complete the flow by doing the following:\\nExample prompt flows 858\\nAmazon Bedrock User Guide\\n a. Drag a connection from the output node of the Flow input node to the genre input in the\\nMakePlaylist prompt node.\\nb. Drag a connection from the output node of the Flow input node to the number input in\\nthe MakePlaylist prompt node.\\nc. Drag a connection from the output node of the modelCompletion output in the\\nMakePlaylist prompt node to the document input in the Flow output node.\\nd. Drag a connection from the output of the Flow input node to the document input in each\\nof the three output nodes.\\n7. Choose Save to save your flow. Your flow should now be prepared for testing.\\n8. Test your flow by entering the following JSON objects is the Test prompt flow pane on the\\nright. Choose Run for each input:\\n1. The following object fulfills the first condition (the retailPrice is more than 10 and the\\ntype is \"produce\") and returns the first value in action (\"don\\'t buy\"):\\n{\\n\"retailPrice\": 11,\\n\"marketPrice\": 12,\\n\"type\": \"produce\",\\n\"action\": [\"don\\'t buy\", \"buy\", \"undecided\"]\\n}\\nNote\\n'},\n",
       " {'question': 'What are the minimum components needed to prepare an agent for testing or deployment?',\n",
       "  'ground_truth': \"To prepare an agent for testing or deployment, you must minimally configure the agent resource role (ARN of the service role with permissions), foundation model (FM) for the agent to invoke, and instructions (natural language describing the agent's purpose and interactions). Additionally, at least one action group or knowledge base should be configured.\",\n",
       "  'question_type': 'simple',\n",
       "  'context': \"Management Console to automatically create a service role for you.\\n3. (Optional) Create a guardrail to implement safeguards for your agent and to prevent\\nunwanted behavior from model responses and user messages. You can then associate it with\\nyour agent.\\nPrerequisites 651\\nAmazon Bedrock User Guide\\n4. (Optional) Purchase Provisioned Throughput to increase the number and rate of tokens that\\nyour agent can process in a given time frame. You can then associate it with an alias of your\\nagent when you create a version of your agent and associate an alias with it.\\nCreate an agent in Amazon Bedrock\\nTo create an agent with Amazon Bedrock, you set up the following components:\\n• The configuration of the agent, which defines the purpose of the agent and indicates the\\nfoundation model (FM) that it uses to generate prompts and responses.\\n• At least one of the following:\\n• Action groups that define what actions the agent is designed to perform.\\n • A knowledge base of data sources to augment the generative capabilities of the agent by\\nallowing search and query.\\nYou can minimally create an agent that only has a name. To Prepare an agent so that you can test\\nor deploy it, you must minimally configure the following components:\\nConfiguration Description\\nAgent resource role The ARN of the service role with permissions\\nto call API operations on the agent\\nFoundation model (FM) An FM for the agent to invoke to perform\\norchestration\\nInstructions Natural language describing what the agent\\nshould do and how it should interact with\\nusers\\nYou should also configure at least one action group or knowledge base for the agent. If you\\nprepare an agent with no action groups or knowledge bases, it will return responses based only on\\nthe FM and instructions and base prompt templates.\\nTo learn how to create an agent, select the tab corresponding to your method of choice and follow\\nthe steps.\\nCreate an agent 652\\nAmazon Bedrock User Guide\\nConsole\\n To create an agent\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\\nbedrock/.\\n2. Select Agents from the left navigation pane.\\n3. In the Agents section, choose Create Agent.\\n4. (Optional) Change the automatically generated Name for the agent and provide an\\noptional Description for it.\\n5. Choose Create. Your agent is created and you will be taken to the Agent builder for your\\nnewly created agent, where you can configure your agent.\\n6. You can continue to the following procedure to configure your agent or return to the Agent\\nbuilder later.\\nTo configure your agent\\n1. If you're not already in the agent builder, do the following:\\na. Sign in to the AWS Management Console using an IAM role with Amazon\\nBedrock permissions, and open the Amazon Bedrock console at https://\\nconsole.aws.amazon.com/bedrock/.\\n\"},\n",
       " {'question': 'How does the lambda_handler function determine which parsing method to use, and what potential issue might arise if a new prompt type is introduced without updating the function?',\n",
       "  'ground_truth': 'The lambda_handler function determines which parsing method to use based on the \"promptType\" field in the input event. It checks if the promptType is equal to PREPROCESSING_PROMPT_TYPE, and if so, it calls the parse_pre_processing function. However, this approach has a potential issue: if a new prompt type is introduced, the lambda_handler function would need to be updated to handle it. Currently, it only explicitly handles the PREPROCESSING_PROMPT_TYPE, and there\\'s no else clause or default case to handle other prompt types. This means that if a new prompt type is added without updating the lambda_handler, it would not be processed correctly, potentially leading to errors or unexpected behavior in the system.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'PREPROCESSING_PROMPT_TYPE = \"PRE_PROCESSING\"\\nPRE_PROCESSING_RATIONALE_PATTERN = re.compile(PRE_PROCESSING_RATIONALE_REGEX,\\nre.DOTALL)\\nPREPROCESSING_CATEGORY_PATTERN = re.compile(PREPROCESSING_CATEGORY_REGEX, re.DOTALL)\\nlogger = logging.getLogger()\\n# This parser lambda is an example of how to parse the LLM output for the default\\nPreProcessing prompt\\ndef lambda_handler(event, context):\\nprint(\"Lambda input: \" + str(event))\\nlogger.info(\"Lambda input: \" + str(event))\\nprompt_type = event[\"promptType\"]\\n# Sanitize LLM response\\nmodel_response = sanitize_response(event[\\'invokeModelRawResponse\\'])\\nif event[\"promptType\"] == PREPROCESSING_PROMPT_TYPE:\\nreturn parse_pre_processing(model_response)\\ndef parse_pre_processing(model_response):\\ncategory_matches = re.finditer(PREPROCESSING_CATEGORY_PATTERN, model_response)\\nrationale_matches = re.finditer(PRE_PROCESSING_RATIONALE_PATTERN, model_response)\\ncategory = next((match.group(1) for match in category_matches), None)\\n rationale = next((match.group(1) for match in rationale_matches), None)\\nreturn {\\nAdvanced prompts 765\\nAmazon Bedrock User Guide\\n\"promptType\": \"PRE_PROCESSING\",\\n\"preProcessingParsedResponse\": {\\n\"rationale\": rationale,\\n\"isValidInput\": get_is_valid_input(category)\\n}\\n}\\ndef sanitize_response(text):\\npattern = r\"(\\\\\\\\n*)\"\\ntext = re.sub(pattern, r\"\\\\n\", text)\\nreturn text\\ndef get_is_valid_input(category):\\nif category is not None and category.strip().upper() == \"D\" or\\ncategory.strip().upper() == \"E\":\\nreturn True\\nreturn False\\nOrchestration\\nThe following examples shows an orchestration parser Lambda function written in Python.\\nThe example code differs depending on whether your action group was defined with an OpenAPI\\nschema or with function details:\\n1. To see examples for an action group defined with an OpenAPI schema, select the tab\\ncorresponding to the model that you want to see examples for.\\nAnthropic Claude 2.0\\nimport json\\nimport re\\nimport logging\\nRATIONALE_REGEX_LIST = [\\n \"(.*?)(<function_call>)\",\\n\"(.*?)(<answer>)\"\\n]\\nRATIONALE_PATTERNS = [re.compile(regex, re.DOTALL) for regex in\\nRATIONALE_REGEX_LIST]\\nRATIONALE_VALUE_REGEX_LIST = [\\n\"<scratchpad>(.*?)(</scratchpad>)\",\\nAdvanced prompts 766\\nAmazon Bedrock User Guide\\n\"(.*?)(</scratchpad>)\",\\n\"(<scratchpad>)(.*?)\"\\n]\\nRATIONALE_VALUE_PATTERNS = [re.compile(regex, re.DOTALL) for regex in\\nRATIONALE_VALUE_REGEX_LIST]\\nANSWER_REGEX = r\"(?<=<answer>)(.*)\"\\nANSWER_PATTERN = re.compile(ANSWER_REGEX, re.DOTALL)\\nANSWER_TAG = \"<answer>\"\\nFUNCTION_CALL_TAG = \"<function_call>\"\\nASK_USER_FUNCTION_CALL_REGEX = r\"(<function_call>user::askuser)(.*)\\\\)\"\\nASK_USER_FUNCTION_CALL_PATTERN = re.compile(ASK_USER_FUNCTION_CALL_REGEX,\\nre.DOTALL)\\nASK_USER_FUNCTION_PARAMETER_REGEX = r\"(?<=askuser=\\\\\")(.*?)\\\\\"\"\\nASK_USER_FUNCTION_PARAMETER_PATTERN =\\nre.compile(ASK_USER_FUNCTION_PARAMETER_REGEX, re.DOTALL)\\nKNOWLEDGE_STORE_SEARCH_ACTION_PREFIX = \"x_amz_knowledgebase_\"\\nFUNCTION_CALL_REGEX = r\"<function_call>(\\\\w+)::(\\\\w+)::(.+)\\\\((.+)\\\\)\"\\n'},\n",
       " {'question': 'What is the purpose of the AmazonBedrockStudioPermissionsBoundary policy?',\n",
       "  'ground_truth': 'The AmazonBedrockStudioPermissionsBoundary policy limits permissions of IAM principals created by Amazon Bedrock Studio when creating projects, apps, and components. It grants read and write access to services like Amazon S3, Amazon Bedrock, Amazon OpenSearch Serverless, and AWS Lambda, as well as necessary infrastructure resources.',\n",
       "  'question_type': 'simple',\n",
       "  'context': '\"Effect\": \"Allow\",\\n\"Action\": [\\n\"bedrock:GetFoundationModel\",\\n\"bedrock:ListFoundationModels\",\\n\"bedrock:GetModelInvocationLoggingConfiguration\",\\n\"bedrock:GetProvisionedModelThroughput\",\\n\"bedrock:ListProvisionedModelThroughputs\",\\n\"bedrock:GetModelCustomizationJob\",\\n\"bedrock:ListModelCustomizationJobs\",\\n\"bedrock:ListCustomModels\",\\n\"bedrock:GetCustomModel\",\\n\"bedrock:GetModelInvocationJob\",\\n\"bedrock:ListModelInvocationJobs\",\\n\"bedrock:ListTagsForResource\",\\n\"bedrock:GetFoundationModelAvailability\",\\n\"bedrock:GetGuardrail\",\\n\"bedrock:ListGuardrails\",\\n\"bedrock:GetEvaluationJob\",\\n\"bedrock:ListEvaluationJobs\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}\\nAWS managed policies 1095\\nAmazon Bedrock User Guide\\nAWS managed policy: AmazonBedrockStudioPermissionsBoundary\\nNote\\n• This policy is a permissions boundary. A permissions boundary sets the maximum\\npermissions that an identity-based policy can grant to an IAM principal. You should\\nnot use and attach Amazon Bedrock Studio permissions boundary policies on your\\n own. Amazon Bedrock Studio permissions boundary policies should only be attached\\nto Amazon Bedrock Studio managed roles. For more information on permissions\\nboundaries, see Permissions boundaries for IAM entities in the IAM User Guide.\\n• The current version of Amazon Bedrock Studio continues to expect a similar policy\\nnamed AmazonDataZoneBedrockPermissionsBoundary to exist in your AWS\\naccount. For more information, see Step 2: Create permissions boundary, service role, and\\nprovisioning role.\\nWhen you create Amazon Bedrock Studio projects, apps, and components, Amazon Bedrock Studio\\napplies this permissions boundary to the IAM roles produced when creating those resources.\\nAmazon Bedrock Studio uses the AmazonBedrockStudioPermissionsBoundary managed\\npolicy to limit permissions of the provisioned IAM principal it is attached to. Principals might\\ntake the form of the user roles that Amazon DataZone can assume on behalf of Amazon Bedrock\\n Studio users, and then conduct actions such as reading and writing Amazon S3 objects or invoking\\nAmazon Bedrock agents.\\nThe AmazonBedrockStudioPermissionsBoundary policy grants read and write access for\\nAmazon Bedrock Studio to services such as Amazon S3, Amazon Bedrock, Amazon OpenSearch\\nServerless, and AWS Lambda. The policy also gives read and write permissions to some\\ninfrastructure resources that are required to use these services such as AWS Secrets Manager\\nsecrets, Amazon CloudWatch log groups, and AWS KMS keys.\\nThis policy consists of the following sets of permissions.\\n• s3 – Allows read and write access to objects in Amazon S3 buckets that are managed by Amazon\\nBedrock Studio.\\n• bedrock – Grants the ability to use Amazon Bedrock agents, knowledge bases, and guardrails\\nthat are managed by Amazon Bedrock Studio.\\nAWS managed policies 1096\\nAmazon Bedrock User Guide\\n• aoss – Allows API access to Amazon OpenSearch Serverless collections that are managed by\\nAmazon Bedrock Studio.\\n'},\n",
       " {'question': 'How does the ConverseStream API handle real-time response processing for Meta Llama models, and what parameters can be adjusted to control the output?',\n",
       "  'ground_truth': \"The ConverseStream API handles real-time response processing for Meta Llama models by returning a stream of response chunks. Developers can iterate through this stream to process the response as it's generated. The API allows adjustment of several parameters to control the output:\\n\\n1. maxTokens: Limits the maximum number of tokens in the response (e.g., 512).\\n2. temperature: Controls the randomness of the output (e.g., 0.5 for balanced creativity).\\n3. topP: Influences the diversity of word choices (e.g., 0.9 for a good balance).\\n\\nThese parameters are set in the inferenceConfig object when creating the ConverseStream command. The code examples demonstrate how to set up the client, create the command with these parameters, and then process the streamed response in real-time using a loop that writes each chunk of text to the output as it's received.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': ').build();\\ntry {\\n// Send the message with a basic inference configuration and attach\\nthe handler.\\nclient.converseStream(request -> request\\n.modelId(modelId)\\n.messages(message)\\n.inferenceConfig(config -> config\\n.maxTokens(512)\\n.temperature(0.5F)\\n.topP(0.9F)\\n), responseStreamHandler).get();\\nMeta Llama 1433\\nAmazon Bedrock User Guide\\n} catch (ExecutionException | InterruptedException e) {\\nSystem.err.printf(\"Can\\'t invoke \\'%s\\': %s\", modelId,\\ne.getCause().getMessage());\\n}\\n}\\n}\\n• For API details, see ConverseStream in AWS SDK for Java 2.x API Reference.\\nJavaScript\\nSDK for JavaScript (v3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nSend a text message to Meta Llama, using Bedrock\\'s Converse API and process the response\\nstream in real-time.\\n// Use the Conversation API to send a text message to Meta Llama.\\nimport {\\nBedrockRuntimeClient,\\nConverseStreamCommand,\\n} from \"@aws-sdk/client-bedrock-runtime\";\\n // Create a Bedrock Runtime client in the AWS Region you want to use.\\nconst client = new BedrockRuntimeClient({ region: \"us-east-1\" });\\n// Set the model ID, e.g., Llama 3 8b Instruct.\\nconst modelId = \"meta.llama3-8b-instruct-v1:0\";\\n// Start a conversation with the user message.\\nconst userMessage =\\n\"Describe the purpose of a \\'hello world\\' program in one line.\";\\nconst conversation = [\\nMeta Llama 1434\\nAmazon Bedrock User Guide\\n{\\nrole: \"user\",\\ncontent: [{ text: userMessage }],\\n},\\n];\\n// Create a command with the model ID, the message, and a basic configuration.\\nconst command = new ConverseStreamCommand({\\nmodelId,\\nmessages: conversation,\\ninferenceConfig: { maxTokens: 512, temperature: 0.5, topP: 0.9 },\\n});\\ntry {\\n// Send the command to the model and wait for the response\\nconst response = await client.send(command);\\n// Extract and print the streamed response text in real-time.\\nfor await (const item of response.stream) {\\nif (item.contentBlockDelta) {\\n process.stdout.write(item.contentBlockDelta.delta?.text);\\n}\\n}\\n} catch (err) {\\nconsole.log(`ERROR: Can\\'t invoke \\'${modelId}\\'. Reason: ${err}`);\\nprocess.exit(1);\\n}\\n• For API details, see ConverseStream in AWS SDK for JavaScript API Reference.\\nPython\\nSDK for Python (Boto3)\\nNote\\nThere\\'s more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\\nMeta Llama 1435\\nAmazon Bedrock User Guide\\nSend a text message to Meta Llama, using Bedrock\\'s Converse API and process the response\\nstream in real-time.\\n# Use the Conversation API to send a text message to Meta Llama\\n# and print the response stream.\\nimport boto3\\nfrom botocore.exceptions import ClientError\\n# Create a Bedrock Runtime client in the AWS Region you want to use.\\nclient = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\\n# Set the model ID, e.g., Llama 3 8b Instruct.\\nmodel_id = \"meta.llama3-8b-instruct-v1:0\"\\n# Start a conversation with the user message.\\n'},\n",
       " {'question': 'What parameter is required when making inference requests to Cohere Command models?',\n",
       "  'ground_truth': 'The \"message\" parameter, which is the text input for the model to respond to, is required when making inference requests to Cohere Command models.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'for i, embedding in enumerate(response_body.get(\\'embeddings\\')):\\nprint(f\"\\\\tEmbedding {i}\")\\nprint(*embedding)\\nprint(\"Texts\")\\nfor i, text in enumerate(response_body.get(\\'texts\\')):\\nprint(f\"\\\\tText {i}: {text}\")\\nexcept ClientError as err:\\nmessage = err.response[\"Error\"][\"Message\"]\\nlogger.error(\"A client error occurred: %s\", message)\\nprint(\"A client error occured: \" +\\nformat(message))\\nelse:\\nprint(\\nf\"Finished generating text embeddings with Cohere model {model_id}.\")\\nif __name__ == \"__main__\":\\nmain()\\nCohere Command R and Command R+ models\\nYou make inference requests to Cohere Command R and Cohere Command R+ models with\\nInvokeModel or InvokeModelWithResponseStream (streaming). You need the model ID for the\\nmodel that you want to use. To get the model ID, see Amazon Bedrock model IDs.\\nTip\\nFor conversational applications, we recommend that you use the Converse API. The\\nConverse API provides a unified set of parameters that work across all models that support\\n messages. For more information, see Use the Converse API.\\nTopics\\nCohere models 166\\nAmazon Bedrock User Guide\\n• Request and Response\\n• Code example\\nRequest and Response\\nRequest\\nThe Cohere Command models have the following inference parameters.\\n{\\n\"message\": string,\\n\"chat_history\": [\\n{\\n\"role\":\"USER or CHATBOT\",\\n\"message\": string\\n}\\n],\\n\"documents\": [\\n{\"title\": string, \"snippet\": string},\\n],\\n\"search_queries_only\" : boolean,\\n\"preamble\" : string,\\n\"max_tokens\": int,\\n\"temperature\": float,\\n\"p\": float,\\n\"k\": float,\\n\"prompt_truncation\" : string,\\n\"frequency_penalty\" : float,\\n\"presence_penalty\" : float,\\n\"seed\" : int,\\n\"return_prompt\" : boolean,\\n\"tools\" : [\\n{\\n\"name\": string,\\n\"description\": string,\\n\"parameter_definitions\": {\\n\"parameter name\": {\\n\"description\": string,\\n\"type\": string,\\n\"required\": boolean\\n}\\nCohere models 167\\nAmazon Bedrock User Guide\\n}\\n}\\n],\\n\"tool_results\" : [\\n{\\n\"call\": {\\n\"name\": string,\\n\"parameters\": {\\n\"parameter name\": string\\n}\\n},\\n\"outputs\": [\\n{\\n\"text\": string\\n}\\n]\\n}\\n],\\n \"stop_sequences\": [string],\\n\"raw_prompting\" : boolean\\n}\\nThe following are required parameters.\\n• message – (Required) Text input for the model to respond to.\\nThe following are optional parameters.\\n• chat_history – A list of previous messages between the user and the model, meant to give\\nthe model conversational context for responding to the user\\'s message.\\nThe following are required fields.\\n• role – The role for the message. Valid values are USER or CHATBOT. tokens.\\n• message – Text contents of the message.\\nThe following is example JSON for the chat_history field\\n\"chat_history\": [\\n{\"role\": \"USER\", \"message\": \"Who discovered gravity?\"},\\nCohere models 168\\nAmazon Bedrock User Guide\\n{\"role\": \"CHATBOT\", \"message\": \"The man who is widely credited with discovering\\ngravity is Sir Isaac Newton\"}\\n]\\n• documents – A list of texts that the model can cite to generate a more accurate reply. Each\\ndocument is a string-string dictionary. The resulting generation includes citations that\\n'},\n",
       " {'question': 'How do the InvokeModel and Converse operations differ in their functionality and usage when interacting with Amazon Bedrock foundation models, and why might one be preferred over the other?',\n",
       "  'ground_truth': \"The InvokeModel and Converse operations in Amazon Bedrock differ in several key aspects:\\n\\n1. Functionality:\\n   - InvokeModel is a more basic operation that allows you to submit a prompt and generate a model response.\\n   - Converse is a more advanced operation that unifies the inference request across Amazon Bedrock models and simplifies the management of multi-turn conversations.\\n\\n2. Usage:\\n   - InvokeModel requires specifying more parameters in the request, such as textGenerationConfig with maxTokenCount, temperature, and topP.\\n   - Converse simplifies the request structure, potentially making it easier to use for developers.\\n\\n3. Output handling:\\n   - InvokeModel writes the response to a specified output file (e.g., invoke-model-output-text.txt).\\n   - Converse's output handling is not explicitly mentioned in the context, but it likely provides a more streamlined response format.\\n\\n4. Conversation management:\\n   - InvokeModel doesn't inherently support multi-turn conversations.\\n   - Converse is designed to simplify the management of multi-turn conversations, making it more suitable for interactive applications.\\n\\n5. Model compatibility:\\n   - While both can be used with Amazon Bedrock models, Converse is recommended when supported, suggesting it may not be available for all models.\\n\\nConverse is generally preferred over InvokeModel when supported because it offers a more unified approach across different Amazon Bedrock models and provides better support for conversational applications. However, InvokeModel might still be necessary for certain models or specific use cases that require fine-grained control over generation parameters.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': 'steps at Request access to an Amazon Bedrock foundation model.\\n• You\\'ve received access keys for your IAM user and configured a profile with them. Otherwise,\\nfollow the steps that are applicable to your use case at Get credentials to grant programmatic\\naccess to a user.\\nTest that your permissions and access keys are set up properly for Amazon Bedrock, using the\\nAmazon Bedrock role that you created. These examples assume that you have configured a default\\nprofile with your access keys. Note the following:\\n• Minimally, you must configure a profile containing an AWS access key ID and an AWS secret\\naccess key.\\n• If you\\'re using temporary credentials, you must also include an AWS session token.\\nTopics\\n• List the foundation models that Amazon Bedrock has to offer\\n• Submit a text prompt to a model and generate a text response with InvokeModel\\n• Submit a text prompt to a model and generate a text response with Converse\\nList the foundation models that Amazon Bedrock has to offer\\n The following example runs the ListFoundationModels operation using an Amazon Bedrock\\nendpoint. ListFoundationModels lists the foundation models (FMs) that are available in\\nAmazon Bedrock in your region. In a terminal, run the following command:\\nRun examples with the AWS CLI 17\\nAmazon Bedrock User Guide\\naws bedrock list-foundation-models --region us-east-1\\nIf the command is successful, the response returns a list of foundation models that are available in\\nAmazon Bedrock.\\nSubmit a text prompt to a model and generate a text response with InvokeModel\\nThe following example runs the InvokeModel operation using an Amazon Bedrock runtime\\nendpoint. InvokeModel lets you submit a prompt to generate a model response. In a terminal, run\\nthe following command:\\naws bedrock-runtime invoke-model \\\\\\n--model-id amazon.titan-text-express-v1 \\\\\\n--body \\'{\"inputText\": \"Describe the purpose of a \\\\\"hello world\\\\\" program in one line.\",\\n \"textGenerationConfig\" : {\"maxTokenCount\": 512, \"temperature\": 0.5, \"topP\": 0.9}}\\' \\\\\\n--cli-binary-format raw-in-base64-out \\\\\\n--outfile invoke-model-output-text.txt\\nIf the command is successful, the response generated by the model is written to the invoke-\\nmodel-output-text.txt file. The text response is returned in the outputText field, alongside\\naccompanying information.\\nSubmit a text prompt to a model and generate a text response with Converse\\nThe following example runs the Converse operation using an Amazon Bedrock runtime endpoint.\\nConverse lets you submit a prompt to generate a model response. We recommend using\\nConverse operation over InvokeModel when supported, because it unifies the inference request\\nacross Amazon Bedrock models and simplifies the management of multi-turn conversations. In a\\nterminal, run the following command:\\naws bedrock-runtime converse \\\\\\n--model-id amazon.titan-text-express-v1 \\\\\\n'},\n",
       " {'question': 'What are two main versions of Amazon Titan Image Generator G1?',\n",
       "  'ground_truth': 'The two main versions of Amazon Titan Image Generator G1 are v1 and v2.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'Amazon Bedrock to access the data by attaching an IAM policy to your Amazon Bedrock service\\nrole. For more information on granting an IAM policies for training data, see Grant custom jobs\\naccess to your training data.\\nHyperparameters\\nThese values can be adjusted for the Multimodal Embeddings model hyperparameters. The default\\nvalues will work well for most use cases.\\n• Learning rate - (min/max learning rate) – default: 5.00E-05, min: 5.00E-08, max: 1\\n• Batch size - Effective batch size – default: 576, min: 256, max: 9,216\\n• Max epochs – default: \"auto\", min: 1, max: 100\\nAmazon Titan Image Generator G1 models\\nAmazon Titan Image Generator G1 is an image generation model. It comes in two versions v1 and\\nv2.\\nAmazon Titan Image Generator v1 enables users to generate and edit images in versatile ways.\\nUsers can create images that match their text-based descriptions by simply inputting natural\\nlanguage prompts. Furthermore, they can upload and edit existing images, including applying\\n text-based prompts without the need for a mask, or editing specific parts of an image using an\\nimage mask. The model also supports outpainting, which extends the boundaries of an image, and\\ninpainting, which fills in missing areas. It offers the ability to generate variations of an image based\\non an optional text prompt, as well as instant customization options that allow users to transfer\\nstyles using reference images or combine styles from multiple references, all without requiring any\\nfine-tuning.\\nTitan Image Generator v2 supports all the existing features of Titan Image Generator v1 and adds\\nseveral new capabilities. It allows users to leverage reference images to guide image generation,\\nwhere the output image aligns with the layout and composition of the reference image while still\\nHyperparameters 993\\nAmazon Bedrock User Guide\\nfollowing the textual prompt. It also includes an automatic background removal feature, which can\\n remove backgrounds from images containing multiple objects without any user input. The model\\nprovides precise control over the color palette of generated images, allowing users to preserve a\\nbrand\\'s visual identity without the requirement for additional fine-tuning. Additionally, the subject\\nconsistency feature enables users to fine-tune the model with reference images to preserve the\\nchosen subject (e.g., pet, shoe or handbag) in generated images. This comprehensive suite of\\nfeatures empowers users to unleash their creative potential and bring their imaginative visions to\\nlife.\\nFor more information on Amazon Titan Image Generator G1 models prompt engineering\\nguidelines, see Amazon Titan Image Generator Prompt Engineering Best Practices.\\nTo continue supporting best practices in the responsible use of AI, Titan Foundation Models\\n(FMs) are built to detect and remove harmful content in the data, reject inappropriate content\\n'},\n",
       " {'question': 'How can an Amazon Bedrock agent be customized to enhance its functionality, and what are the key considerations when configuring these advanced features?',\n",
       "  'ground_truth': \"An Amazon Bedrock agent can be customized through several advanced features to enhance its functionality:\\n\\n1. Knowledge bases: You can associate knowledge groups with the agent to expand its information sources.\\n\\n2. Guardrails: You can add a guardrail to block and filter out harmful content, selecting a specific guardrail and version.\\n\\n3. Advanced prompts: You can customize the prompts sent to the foundation model (FM) at each orchestration step.\\n\\n4. Agent resource role: You must specify an ARN for a service role with permissions to call API operations on the agent.\\n\\n5. Foundation model selection: You need to specify which FM the agent will orchestrate with.\\n\\n6. Instructions: You provide instructions to guide the agent's behavior, used in the $instructions$ placeholder of the orchestration prompt template.\\n\\n7. Idle session timeout: You can set a duration after which the agent ends the session and deletes stored information.\\n\\n8. Encryption: You have the option to specify a KMS key ARN for encrypting agent resources.\\n\\nWhen configuring these features, key considerations include:\\n- Ensuring the selected knowledge bases are relevant and up-to-date\\n- Choosing appropriate guardrails to maintain safety and compliance\\n- Crafting effective custom prompts that align with the agent's purpose\\n- Granting the correct permissions in the agent's resource role\\n- Selecting a suitable foundation model for the agent's tasks\\n- Providing clear and comprehensive instructions\\n- Setting an appropriate idle session timeout to balance resource usage and user experience\\n- Implementing proper encryption for sensitive data\\n\\nThese customizations allow for a highly tailored agent that can better meet specific use cases and requirements.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': '4. In the Knowledge bases section, you can choose Add to associate knowledge groups with\\nyour agent. For more information on setting up knowledge bases, see Knowledge bases for\\nAmazon Bedrock. To learn how to associate knowledge bases with your agent, see Associate\\na knowledge base with an Amazon Bedrock agent.\\n5. In the Guardrails details section, you can choose Edit to associate a guardrail with\\nyour agent to block and filter out harmful content. Select a guardrail you want to use\\nfrom the drop down menu under Select guardrail and then choose the version to use\\nunder Guardrail version. You can select View to see your Guardrail settings. For more\\ninformation, see Guardrails for Amazon Bedrock.\\n6. In the Advanced prompts section, you can choose Edit to customize the prompts that\\nare sent to the FM by your agent in each step of orchestration. For more information\\nabout the prompt templates that you can use for customization, see Advanced prompts in\\n Amazon Bedrock. To learn how to configure advanced prompts, see Configure the prompt\\ntemplates.\\n7. When you finish configuring your agent, select one of the following options:\\nCreate an agent 655\\nAmazon Bedrock User Guide\\n• To stay in the Agent builder, choose Save. You can then Prepare the agent in order to\\ntest it with your updated configurations in the test window. To learn how to test your\\nagent, see Test an Amazon Bedrock agent.\\n• To return to the Agent Details page, choose Save and exit.\\nAPI\\nTo create an agent, send a CreateAgent request (see link for request and response formats and\\nfield details) with an Agents for Amazon Bedrock build-time endpoint.\\nSee code examples\\nTo prepare your agent and test or deploy it, so that you can test or deploy it, you must\\nminimally include the following fields (if you prefer, you can skip these configurations and\\nconfigure them later by sending an UpdateAgent request):\\nField Use case\\n agentResourceRoleArn To specify an ARN of the service role with\\npermissions to call API operations on the\\nagent\\nfoundationModel To specify a foundation model (FM) for the\\nagent to orchestrate with\\ninstruction To provide instructions to tell the agent\\nwhat to do. Used in the $instructions$\\nplaceholder of the orchestration prompt\\ntemplate.\\nThe following fields are optional:\\nField Use case\\ndescription Describes what the agent does\\nCreate an agent 656\\nAmazon Bedrock User Guide\\nField Use case\\nidleSessionTTLInSeconds Duration after which the agent ends the\\nsession and deletes any stored information.\\ncustomerEncryptionKeyArn ARN of a KMS key to encrypt agent resources\\ntags To associate tags with your agent.\\npromptOverrideConfiguration To customize the prompts sent to the FM at\\neach step of orchestration.\\nguardrailConfiguration To add a guardrail to the agent. Specify the\\nID or ARN of the guardrail and the version to\\nuse.\\nclientToken Identifier to ensure the API request\\ncompletes only once.\\n'},\n",
       " {'question': \"How can you view details of an agent's orchestration process?\",\n",
       "  'ground_truth': 'To view details for each step of the agent\\'s orchestration process, select \"Show trace\" in the Test window. This displays the prompt, inference configurations, and agent\\'s reasoning process for each step, including usage of action groups and knowledge bases.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"generate or after it is generated, you have the following options:\\n• To view details for each step of the agent's orchestration process, including the prompt,\\ninference configurations, and agent's reasoning process for each step and usage of its\\naction groups and knowledge bases, select Show trace. The trace is updated in real-time\\nso you can view it before the response is returned. To expand or collapse the trace for a\\nstep, select an arrow next to a step. For more information about the Trace window and\\ndetails that appear, see Trace events in Amazon Bedrock.\\n• If the agent invokes a knowledge base, the response contains footnotes. To view the\\nlink to the S3 object containing the cited information for a specific part of the response,\\nselect the relevant footnote.\\nTest an agent 705\\nAmazon Bedrock User Guide\\n• If you set your agent to return control rather than using a Lambda function to handle the\\naction group, the response contains the predicted action and its parameters. Provide an\\n example output value from the API or function for the action and then choose Submit to\\ngenerate an agent response. See the following image for an example:\\nTest an agent 706\\nAmazon Bedrock User Guide\\nYou can perform the following actions in the Test window:\\nTest an agent 707\\nAmazon Bedrock User Guide\\n• To start a new conversation with the agent, select the refresh icon.\\n• To view the Trace window, select the expand icon. To close the Trace window, select the\\nshrink icon.\\n• To close the Test window, select the right arrow icon.\\nYou can enable or disable action groups and knowledge bases. Use this feature to troubleshoot\\nyour agent by isolating which action groups or knowledge bases need to be updated by\\nassessing its behavior with different settings.\\nTo enable an action group or knowledge base\\n1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock\\npermissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/\\nbedrock/.\\n 2. Select Agents from the left navigation pane. Then, choose an agent in the Agents section.\\n3. In the Agents section. select the link for the agent that you want to test from the list of\\nagents.\\n4. On the agent's details page, in the Working draft section, select the link for the Working\\ndraft.\\n5. In the Action groups or Knowledge bases section, hover over the State of the action group\\nor knowledge base whose state you want to change.\\n6. An edit button appears. Select the edit icon and then choose from the dropdown menu\\nwhether the action group or knowledge base is Enabled or Disabled.\\n7. If an action group is Disabled, the agent doesn't use the action group. If a knowledge base\\nis Disabled, the agent doesn't use the knowledge base. Enable or disable action groups or\\nknowledge bases and then use the Test window to troubleshoot your agent.\\n8. Choose Prepare to apply the changes that you have made to the agent before testing it.\\nAPI\\n\"},\n",
       " {'question': 'How does the structure of the action response differ when an action group is defined with an API schema versus function details, and what unique fields are available in each case?',\n",
       "  'ground_truth': \"The structure of the action response differs significantly between action groups defined with an API schema and those defined with function details:\\n\\n1. API schema-defined action groups:\\n   - Include fields like 'apiPath', 'httpMethod', 'httpStatusCode', and 'responseBody'\\n   - 'responseBody' contains the response as defined in the OpenAPI schema\\n   - Focuses on RESTful API-style interactions\\n\\n2. Function details-defined action groups:\\n   - Include fields like 'responseState' (optional) and 'responseBody'\\n   - 'responseState' can be set to 'FAILURE' or 'REPROMPT' to control agent behavior\\n   - 'responseBody' contains an object with a content type (currently only TEXT) and the response body\\n\\nBoth types can optionally include 'sessionAttributes', 'promptSessionAttributes', and 'knowledgeBasesConfiguration'.\\n\\nThe key difference is that API schema responses are structured around HTTP concepts, while function details responses offer more control over the agent's behavior and are more flexible in terms of response formatting.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': '• If you defined the action group with an API schema, the following fields can be in the\\nresponse:\\n• apiPath – The path to the API operation, as defined in the OpenAPI schema.\\n• httpMethod – The method of the API operation, as defined in the OpenAPI schema.\\n• httpStatusCode – The HTTP status code returned from the API operation.\\n• responseBody – Contains the response body, as defined in the OpenAPI schema.\\nHandling fulfillment of the action 676\\nAmazon Bedrock User Guide\\n• If you defined the action group with function details, the following fields can be in the\\nresponse:\\n• responseState (Optional) – Set to one of the following states to define the agent\\'s\\nbehavior after processing the action:\\n• FAILURE – The agent throws a DependencyFailedException for the current session.\\nApplies when the function execution fails because of a dependency failure.\\n• REPROMPT – The agent passes a response string to the model to reprompt it. Applies\\n when the function execution fails because of invalid input.\\n• responseBody – Contains an object that defines the response from execution of the\\nfunction. The key is the content type (currently only TEXT is supported) and the value is an\\nobject containing the body of the response.\\n• (Optional) sessionAttributes – Contains session attributes and their values. For more\\ninformation, see Session and prompt session attributes.\\n• (Optional) promptSessionAttributes – Contains prompt attributes and their values. For\\nmore information, see Session and prompt session attributes.\\n• (Optional) knowledgeBasesConfiguration – Contains a list of query configurations for\\nknowledge bases attached to the agent. For more information, see Knowledge base retrieval\\nconfigurations.\\nAction group Lambda function example\\nThe following is an minimal example of how the Lambda function can be defined in Python. Select\\nthe tab corresponding to whether you defined the action group with an OpenAPI schema or with\\n function details:\\nOpenAPI schema\\ndef lambda_handler(event, context):\\nagent = event[\\'agent\\']\\nactionGroup = event[\\'actionGroup\\']\\napi_path = event[\\'apiPath\\']\\n# get parameters\\nget_parameters = event.get(\\'parameters\\', [])\\n# post parameters\\npost_parameters = event[\\'requestBody\\'][\\'content\\'][\\'application/json\\']\\n[\\'properties\\']\\nHandling fulfillment of the action 677\\nAmazon Bedrock User Guide\\nresponse_body = {\\n\\'application/json\\': {\\n\\'body\\': \"sample response\"\\n}\\n}\\naction_response = {\\n\\'actionGroup\\': event[\\'actionGroup\\'],\\n\\'apiPath\\': event[\\'apiPath\\'],\\n\\'httpMethod\\': event[\\'httpMethod\\'],\\n\\'httpStatusCode\\': 200,\\n\\'responseBody\\': response_body\\n}\\nsession_attributes = event[\\'sessionAttributes\\']\\nprompt_session_attributes = event[\\'promptSessionAttributes\\']\\napi_response = {\\n\\'messageVersion\\': \\'1.0\\',\\n\\'response\\': action_response,\\n\\'sessionAttributes\\': session_attributes,\\n\\'promptSessionAttributes\\': prompt_session_attributes\\n}\\nreturn api_response\\nFunction details\\ndef lambda_handler(event, context):\\n'},\n",
       " {'question': 'What is the primary purpose of Knowledge bases for Amazon Bedrock?',\n",
       "  'ground_truth': 'Knowledge bases for Amazon Bedrock help you leverage Retrieval Augmented Generation (RAG), allowing you to augment responses generated by Large Language Models with information drawn from your own data store.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"that a response is based on and also check that the response makes sense and is factually correct.\\nYou take the following steps to set up and use your knowledge base.\\n1. Gather source documents to add to your knowledge base.\\n2. Store your source documents in a supported data source and configure the connection\\ninformation to connect to and crawl your data.\\n3. (Optional if using Amazon S3 to store your source documents) Create a metadata file for each\\nsource document to allow for filtering of results during knowledge base query.\\n4. (Optional) Set up a vector index in a supported vector store to index your data. You can use the\\nAmazon Bedrock console to create an Amazon OpenSearch Serverless vector database for you.\\n5. Create and configure your knowledge base. You must enable model access to use a model\\nthat's supported for knowledge bases.\\nIf you use the Amazon Bedrock API, take note of your model Amazon Resource Name (ARN)\\n that's required as part of the configuration for knowledge base retrieval and generation and\\nfor converting your data into vector embeddings. Copy the model ID for your chosen model for\\n512\\nAmazon Bedrock User Guide\\nknowledge bases and construct the model ARN using the model (resource) ID, following the\\nprovided ARN examples for your model resource type.\\nIf you use the Amazon Bedrock console, you are not required to construct a model ARN, as you\\ncan select an available model as part of the steps for creating a knowledge base.\\n6. Ingest your data by letting knowledge bases generate embeddings with an embeddings model\\nand storing them in a supported vector store.\\n7. Set up your application or agent to query the knowledge base and return augmented\\nresponses.\\nTopics\\n• How it works\\n• Supported regions and models for Knowledge bases for Amazon Bedrock\\n• Prerequisites for Knowledge bases for Amazon Bedrock\\n• Create a knowledge base\\n• Chat with your document data using the knowledge base\\n • Data source connectors\\n• Sync your data source with your Amazon Bedrock knowledge base\\n• Test a knowledge base in Amazon Bedrock\\n• Manage a knowledge base\\n• Manage a data source\\n• Deploy a knowledge base\\nHow it works\\nKnowledge bases for Amazon Bedrock help you take advantage of Retrieval Augmented\\nGeneration (RAG), a popular technique that involves drawing information from a data store\\nto augment the responses generated by Large Language Models (LLMs). When you set up a\\nknowledge base with your data sources, your application can query the knowledge base to return\\ninformation to answer the query either with direct quotations from sources or with natural\\nresponses generated from the query results.\\nWith knowledge bases, you can build applications that are enriched by the context that is received\\nfrom querying a knowledge base. It enables a faster time to market by abstracting from the heavy\\nHow it works 513\\nAmazon Bedrock User Guide\\n\"}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_qa_dataset(chunks_with_metadata, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
