{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Test Datasets\n",
    "\n",
    "**Why use Synthetic Test Datasets?**\n",
    "\n",
    "Evaluating the performance of RAG (Retrieval-Augmented Generation) augmented pipelines is crucial.\n",
    "\n",
    "However, manually creating hundreds of QA (Question-Answer-Context) samples from documents can be time-consuming and labor-intensive. Additionally, human-generated questions may struggle to reach the level of complexity needed for thorough evaluation, ultimately affecting the quality of the assessment.\n",
    "\n",
    "Using synthetic data generation can reduce developer time in the data aggregation process **by up to 90%**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class PDFLoader:\n",
    "    def __init__(self, file_path: str, start_page: int = None, end_page: int = None):\n",
    "        self.file_path = file_path\n",
    "        self.start_page = start_page\n",
    "        self.end_page = end_page\n",
    "\n",
    "    def load(self) -> Dict[str, Any]:\n",
    "        combined_text = \"\"\n",
    "        metadata = {}\n",
    "\n",
    "        with pdfplumber.open(self.file_path) as pdf:\n",
    "            total_pages = len(pdf.pages)\n",
    "\n",
    "            start = (self.start_page or 1) - 1\n",
    "            end = min(self.end_page or total_pages, total_pages)\n",
    "\n",
    "            for page_num in range(start, end):\n",
    "                page = pdf.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                combined_text += text + \"\\n\"\n",
    "\n",
    "            metadata = {\n",
    "                \"source\": self.file_path,\n",
    "                \"filename\": self.file_path,\n",
    "                \"total_pages\": total_pages,\n",
    "                \"extracted_pages\": f\"{start + 1}-{end}\"\n",
    "            }\n",
    "\n",
    "            for key, value in pdf.metadata.items():\n",
    "                if isinstance(value, (str, int)):\n",
    "                    metadata[key] = value\n",
    "\n",
    "        return {\n",
    "            \"page_content\": combined_text.strip(),\n",
    "            \"metadata\": metadata\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Used for Practice\n",
    "\n",
    "Amazon Bedrock Manual Documentation (https://docs.aws.amazon.com/bedrock/latest/userguide/)\n",
    "\n",
    "- Link: https://d1jp7kj5nqor8j.cloudfront.net/bedrock-manual.pdf\n",
    "- File name: `bedrock-manual.pdf`\n",
    "\n",
    "_Please copy the downloaded file to the data folder for the practice session_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFLoader(\"data/bedrock-manual.pdf\", start_page=16, end_page=1574)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def split_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100, separators: Optional[List[str]] = None) -> List[str]:\n",
    "    separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "\n",
    "    def _split_text_recursive(text: str, separators: List[str]) -> List[str]:\n",
    "        if not separators:\n",
    "            return [text]\n",
    "\n",
    "        separator = separators[0]\n",
    "        splits = re.split(f\"({re.escape(separator)})\", text)\n",
    "        splits = [\"\".join(splits[i:i+2]) for i in range(0, len(splits), 2)]\n",
    "\n",
    "        final_chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for split in splits:\n",
    "            if len(current_chunk) + len(split) <= chunk_size:\n",
    "                current_chunk += split\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    final_chunks.append(current_chunk)\n",
    "                if len(split) > chunk_size:\n",
    "                    subsplits = _split_text_recursive(split, separators[1:])\n",
    "                    final_chunks.extend(subsplits)\n",
    "                else:\n",
    "                    current_chunk = split\n",
    "\n",
    "        if current_chunk:\n",
    "            final_chunks.append(current_chunk)\n",
    "\n",
    "        return final_chunks\n",
    "\n",
    "    chunks = _split_text_recursive(text, separators)\n",
    "\n",
    "    if chunk_overlap > 0:\n",
    "        overlapped_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                overlapped_chunks.append(chunk)\n",
    "            else:\n",
    "                overlap_text = chunks[i-1][-chunk_overlap:]\n",
    "                overlapped_chunks.append(overlap_text + chunk)\n",
    "        chunks = overlapped_chunks\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(docs['page_content'], 1000, 0)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_with_metadata = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunks_with_metadata.append({\n",
    "        'content': chunk,\n",
    "        'metadata': {\n",
    "            'chunk_id': i,\n",
    "            'filename': docs['metadata'].get('filename', 'unknown')\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_with_metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Q&A Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region = 'us-west-2'\n",
    "retry_config = Config(\n",
    "    region_name=region,\n",
    "    retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "boto3_client = boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "def converse_with_bedrock(model_id, sys_prompt, usr_prompt):\n",
    "    temperature = 0.5\n",
    "    top_p = 0.9\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def create_prompt(sys_template, user_template):\n",
    "    sys_prompt = [{\"text\": sys_template}]\n",
    "    usr_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template}]}]\n",
    "    return sys_prompt, usr_prompt\n",
    "\n",
    "def get_context_chunks(chunks_with_metadata, start_id):\n",
    "    context_chunks = [\n",
    "        chunks_with_metadata[start_id]['content'],\n",
    "        chunks_with_metadata[start_id + 1]['content'],\n",
    "        chunks_with_metadata[start_id + 2]['content']\n",
    "    ]\n",
    "    return \" \".join(context_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Use \n",
    "\n",
    "LLM will generate Q&A dataset that conforms to the schema description in the tooluse config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"QuestionAnswerGenerator\",\n",
    "                \"description\": \"Generates questions and answers based on the given context.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"question\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The generated question\"\n",
    "                            },\n",
    "                            \"answer\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The answer to the generated question\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"question\", \"answer\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse_with_bedrock_tools(sys_prompt, usr_prompt, tool_config):\n",
    "    temperature = 0.0\n",
    "    top_p = 0.1\n",
    "    top_k = 1\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "    response = boto3_client.converse(\n",
    "        modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "        messages=usr_prompt,\n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def parse_tool_use(message):\n",
    "    stop_reason = message['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        tool_requests = message['output']['message']['content']\n",
    "        for tool_request in tool_requests:\n",
    "            if 'toolUse' in tool_request:\n",
    "                tool = tool_request['toolUse']\n",
    "\n",
    "                if tool['name'] == 'QuestionAnswerGenerator':\n",
    "                    return tool['input']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A Dataset Generation Instruction\n",
    "\n",
    "- `simple`: directly answerable questions from the given context\n",
    "- `complex`: reasoning questions and answers.\n",
    "\n",
    "_Modify the system/user prompts tailored to your dataset_\n",
    "\n",
    "Generated Q&A pair will be stored in `data/qa_dataset.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_dataset(chunks, num_pairs=5, output_file=\"data/sample_qa_dataset.jsonl\"):\n",
    "    total_chunks = len(chunks)\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        start_id = random.randint(0, total_chunks - 3)\n",
    "        context = get_context_chunks(chunks_with_metadata, start_id)\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            sys_template = \"\"\"\n",
    "            You are an expert at generating practical questions based on given documentation.\n",
    "            Your task is to generate complex, reasoning questions and answers.\n",
    "\n",
    "            Follow these rules:\n",
    "            1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
    "            2. Ensure questions are relevant, concise, preferably under 25 words, and fully answerable with the provided information\n",
    "            3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
    "            4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
    "            5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
    "            \"\"\"\n",
    "            question_type = \"complex\"\n",
    "        else:\n",
    "            sys_template = \"\"\"\n",
    "            You are an expert at generating practical questions based on given documentation.\n",
    "            Your task is to create simple, directly answerable questions from the given context.\n",
    "\n",
    "            Follow these rules:\n",
    "            1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
    "            2. Ensure questions are relevant, concise, preferably under 10 words, and fully answerable with the provided information\n",
    "            3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
    "            4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
    "            5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
    "            \"\"\"\n",
    "            question_type = \"simple\"\n",
    "\n",
    "        user_template = f\"\"\"\n",
    "        Generate a {question_type} question and its answer based on the following context:\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Use the QuestionAnswerGenerator tool to provide the output.\n",
    "        \"\"\"\n",
    "\n",
    "        sys_prompt, user_prompt = create_prompt(sys_template, user_template)\n",
    "        response = converse_with_bedrock_tools(sys_prompt, user_prompt, tool_config)\n",
    "        qa_data = parse_tool_use(response)\n",
    "\n",
    "        if qa_data:\n",
    "            qa_item = {\n",
    "                \"question\": qa_data[\"question\"],\n",
    "                \"ground_truth\": qa_data[\"answer\"],\n",
    "                \"question_type\": question_type,\n",
    "                \"contexts\": context\n",
    "            }\n",
    "\n",
    "            print(qa_item)\n",
    "\n",
    "            with open(output_file, 'a') as f:\n",
    "                json.dump(qa_item, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "            dataset.append(qa_item)\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_qa_dataset(chunks_with_metadata, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
